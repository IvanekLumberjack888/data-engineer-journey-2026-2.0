# ğŸš€ Introduction

# DP-700 EXAM PREP COURSE | Video 1 of 11 (Ãšvod) â€“ Learn Microsoft Fabric with Will

**Tato lekce zahajuje novou sÃ©rii, kterÃ¡ tÄ› mÃ¡ pÅ™ipravit na DP-700 certifikaci Microsoft Fabric Data Engineer. NajdeÅ¡ zde podrobnÃ½ pÅ™ehled, co tÄ› na tÃ©to cestÄ› ÄekÃ¡ a proÄ mÃ¡ smysl do toho jÃ­t.**

---

## Struktura videa

- Ãšvod do sÃ©rie (0:00)
    
- ProÄ teÄ? Voucher na zkouÅ¡ku (1:05)
    
- ProÄ se snaÅ¾it o DP-700? (2:54)
    
- Co obsahuje DP-700 certifikaÄnÃ­ zkouÅ¡ka? (4:50)
    
- SrovnÃ¡nÃ­ DP-700 vs DP-600 (11:30)
    
- OficiÃ¡lnÃ­ zdroje ke studiu (12:25)
    
- PlÃ¡n celÃ© sÃ©rie na YouTube (14:00)
    
- PlÃ¡n DP-700 Bootcamp ve Fabric Dojo (14:20)
    
- ÄŒastÃ© dotazy k certifikaci (15:15)
    
- ZÃ¡vÄ›reÄnÃ© myÅ¡lenky (17:47)
    

---

## ProÄ jÃ­t do DP-700 a co to pro tebe znamenÃ¡?

- Od 14. ledna 2025 je DP-700 zkouÅ¡ka v ostrÃ©m provozu, vÄetnÄ› **slevovÃ©ho voucheru 50%** (po splnÄ›nÃ­ podmÃ­nek).
    
    - PodmÃ­nky pro voucher: buÄ DP-203 certifikace, nebo dokonÄenÃ­ Fabric learning path, pÅ™Ã­padnÄ› ÃºÄast na Å¾ivÃ©m nebo online DP-700 Å¡kolenÃ­.
        
    - Voucher platÃ­ pouze do 31.3. nebo do rozdÃ¡nÃ­ (20 000 ks).
        

## Co ti dÃ¡ DP-700 certifikace?

- Strukturu pro studium a uÄenÃ­.
    
- VyÅ¡Å¡Ã­ Å¡ance zÃ­skÃ¡nÃ­ prÃ¡ce, ale samotnÃ¡ certifikace nenÃ­ zÃ¡rukou zamÄ›stnÃ¡nÃ­.
    
- UplatnÄ›nÃ­ ve firemnÃ­ch konzultacÃ­ch, lepÅ¡Ã­ finanÄnÃ­ ohodnocenÃ­.
    
- Data engineering je zÃ¡kladem modernÃ­ho BI/AI rozhodovÃ¡nÃ­.
    
- PoÅ¾adavky na data inÅ¾enÃ½ry stoupajÃ­, plat je nadprÅ¯mÄ›rnÃ½.
    

---

## Jak vypadÃ¡ DP-700 zkouÅ¡ka?

- 50â€“60 otÃ¡zek, z toho cca 10 v case study.
    
- TÅ™i vyvÃ¡Å¾enÃ© oblasti (domÃ©ny):
    
    - Implementace a sprÃ¡va analytickÃ©ho Å™eÅ¡enÃ­
        
    - Ingest (naÄÃ­tÃ¡nÃ­) & transformace dat
        
    - Monitoring a optimalizace analytickÃ©ho Å™eÅ¡enÃ­
        

## PÅ™ehled domÃ©n

## 1. Implementace, sprÃ¡va a governance:

- NastavenÃ­ workspace (vÄetnÄ› sprÃ¡vy tenant).
    
- Version control, deployment pipelines.
    
- Access control: workspace, item, detailnÄ› (Å™Ã¡dkovÃ¡, objektovÃ¡, sloupcovÃ¡ bezpeÄnost, dynamic data masking).
    
- SprÃ¡va datovÃ© governance: certifikace, endorsement, sensitivity labels.
    
- Orchestrace: jakÃ© orchestration patterny, aktivity a postupy vyuÅ¾Ã­vat v praxi.
    

## 2. Ingest & Transformace:

- RÅ¯znÃ© typy ingestion (pipeline, data flow, notebook, event stream).
    
- RozhodovÃ¡nÃ­ â€kdy pouÅ¾Ã­t coâ€œ.
    
- PlnÃ© vs inkrementÃ¡lnÃ­ naÄÃ­tÃ¡nÃ­ â€“ znalost Spark SQL, PySpark, Delta package.
    
- PrÃ¡ce s datovÃ½mi ÃºloÅ¾iÅ¡ti: lakehouse, warehouse, eventhouse.
    
- Streaming data: zdroje (Azure Event Hubs, IoT Hub), transformace v eventhouse, analÃ½za pomocÃ­ KQL.
    
- NutnÃ¡ sluÅ¡nÃ¡ znalost TSQL, KQL, PySpark.
    
- DP-700 vyÅ¾aduje hlubÅ¡Ã­ porozumÄ›nÃ­ kÃ³du: nejen syntax, ale i rozhodovÃ¡nÃ­ â€proÄ nÄ›co pouÅ¾Ã­tâ€œ, dopady na vÃ½kon.
    

## 3. Monitoring & Optimalizace:

- Monitoring: lakehouse, warehouse query, event stream, pipeline.
    
- Error handling: debugovÃ¡nÃ­ PySpark jobÅ¯, pipeline runÅ¯.
    
- Optimalizace: napÅ™. tabulka v bronze vrstvÄ›, optimalizace medailon architektury, refaktorace KQL skriptÅ¯ â€” jak zrychlit kÃ³d a nastavenÃ­ v praxi.
    

---

## SrovnÃ¡nÃ­ DP-700 vs DP-600

|Vlastnost|DP-700|DP-600|
|---|---|---|
|HlubÅ¡Ã­ detail|Orchestrace, inkrementÃ¡lnÃ­ loading,|ZÃ¡klady orchestrace|
||eventhouse, streaming, hlubÅ¡Ã­ PySpark||
|Monitoring & optimalizace|DetailnÃ­, Å¡irÅ¡Ã­ rozsah|SpÃ­Å¡ zÃ¡kladnÃ­|
|Error handling|VÃ½raznÄ› pokroÄilejÅ¡Ã­|ZÃ¡klady|
|NovÃ© tÃ©mata|KQL, hlubÅ¡Ã­ PySpark, eventhouse|KQL, PySpark (po zmÄ›nÄ› 11/2024)|

---

## OficiÃ¡lnÃ­ zdroje ke studiu

- DP-700 Study Guide (studijnÃ­ prÅ¯vodce)
    
- Dokumentace Microsoft Fabric
    
- Exam Experience & Sandbox (pro ty, kdo jdou poprvÃ© na Microsoft certifikaci â€“ rÅ¯znÃ© typy otÃ¡zek v testu)
    
- SamostudijnÃ­ moduly (nutnÃ© pro voucher, ale nejsou dostateÄnÃ© na sloÅ¾enÃ­ zkouÅ¡ky â€” pouÅ¾ij jako start, ne hlavnÃ­ obsah)
    
- **YouTube sÃ©rie** a **Fabric Dojo**: mnoho tutoriÃ¡lÅ¯, Å¾ivÃ© sezenÃ­, Q&A, pÅ™Ã­klady â€hands-onâ€œ.
    

---

## DoporuÄenÃ½ zpÅ¯sob pÅ™Ã­pravy

- Kombinovat studijnÃ­ pÅ™Ã­ruÄku, dokumentaci, YouTube sÃ©rii, hands-on tutoriÃ¡ly.
    
- AktivnÄ› se ÃºÄastnit diskuzÃ­, Å¾ivÃ½ch lekcÃ­ ve Fabric Dojo.
    
- OvÄ›Å™ovat si znalosti v praxi, klÃ¡st dÅ¯raz na vzÃ¡jemnÃ© propojovÃ¡nÃ­ sluÅ¾eb Fabric.
    
- ÄŒÃ­m vÃ­ce praktickÃ½ch projektÅ¯ v poslednÃ­m roce, tÃ­m kratÅ¡Ã­ pÅ™Ã­prava (1â€“3 mÄ›sÃ­ce), zaÄÃ¡teÄnÃ­ci 3â€“6+ mÄ›sÃ­cÅ¯.
    

---

## ÄŒastÃ© otÃ¡zky

- **Jak poznÃ¡m, Å¾e jsem pÅ™ipraven/a?**  
    ZatÃ­m nejsou k dispozici oficiÃ¡lnÃ­ practice testy. Sleduj videa, dokumentaci, testuj v praxi. SÃ¡m poznÃ¡Å¡, zda tomu rozumÃ­Å¡ do hloubky.
    
- **StaÄÃ­ tento kurz ke zkouÅ¡ce?**  
    Kurz je â€hyperkondenzovanÃ½â€œâ€“ pouÅ¾Ã­vaj takÃ© dokumentaci, studuj rÅ¯znÃ© pÅ™Ã­stupy a vysvÄ›tlenÃ­ od jinÃ½ch profesionÃ¡lÅ¯.
    

---

## ZÃ¡vÄ›reÄnÃ© tipy

- Sleduj dalÅ¡Ã­ videa v sÃ©rii.
    
- PÅ™ihlas se do Fabric Dojo pro dalÅ¡Ã­ podporu a Q&A.
    
- Kombinuj zdroje a hlavnÄ› prakticky testuj zÃ­skanÃ© znalosti.
    
- SprÃ¡vnÃ¡ pÅ™Ã­prava, time management a zkuÅ¡enost se strukturou Microsoft certifikacÃ­ vÃ½raznÄ› zvyÅ¡ujÃ­ Å¡anci uspÄ›t.
    

---

**Odkazy a dalÅ¡Ã­ obsah**

- Skool.com/FabricDojo â€“ placenÃ¡ komunita, Å¾ivÃ© lekce, hands-on tutoriÃ¡ly
    
- YouTube Channel: Learn Microsoft Fabric with Will
    
- DP-700 study guide, online dokumentace, practice exam sandbox
    

---

_Tento markdown je podrobnÃ½m pÅ™ehledem z ÃºvodnÃ­ho videa DP-700 sÃ©rie. HodÃ­ se uloÅ¾it jako poznÃ¡mky nebo podklad pro vlastnÃ­ studium._[youtube](https://www.youtube.com/watch?v=XECqSfKmtCk)â€‹

1. [https://www.youtube.com/watch?v=XECqSfKmtCk](https://www.youtube.com/watch?v=XECqSfKmtCk)

---
---

# ğŸš€ Workspace settings

# Workspace Settings v Microsoft Fabric | DP-700 EXAM PREP (Video 2/11)

**TÃ©mata videa na zÃ¡kladÄ› timeline:**

- Spark Settings a tvorba vlastnÃ­ch Spark poolÅ¯
    
- Domains
    
- OneLake & OneLake File Explorer
    
- Shortcuts & Shortcut Caching
    
- NastavenÃ­ pro Apache Airflow Job
    

---

## 1. Spark Settings

**Starter Pool (vÃ½chozÃ­ Spark cluster):**

- **RychlÃ½ start:** Spark cluster je dostupnÃ½ neustÃ¡le (start pod 10 sekund), dokud se nespustÃ­ kÃ³d, neplatÃ­Å¡ nic navÃ­c.
    
- **AutoÅ¡kÃ¡lovÃ¡nÃ­:** 1â€“10 uzlÅ¯ podle pouÅ¾itÃ© SKU, dynamickÃ¡ alokace executorÅ¯ (Java VM prochÃ¡zejÃ­cÃ­ data v partition).
    
- **DistribuovanÃ© vÃ½poÄty:** Å kÃ¡lovatelnost pÅ™es vÃ­ce uzlÅ¯, paralelismus pÅ™es vÃ­ce executorÅ¯.
    
- **Limity starter poolu:**
    
    - **Velikost dat:** Nad urÄitou velikost uÅ¾ starter pool nemusÃ­ staÄit, je vhodnÃ© vytvoÅ™it custom pool.
        
    - **Konkurence:** VÄ›tÅ¡Ã­ poÄet uÅ¾ivatelÅ¯/jobÅ¯ znamenÃ¡ nutnost custom poolu.
        
    - **VysokÃ¡ variabilita jobÅ¯:** RÅ¯znÄ› nÃ¡roÄnÃ© Ãºlohy â€“ rozdÄ›lit do vÃ­ce custom poolÅ¯.
        
    - **SpecifickÃ¡ prÃ¡ce:** Data scientist, ML training â€“ vhodnÃ© pouÅ¾Ã­vat custom spark pool.
        

**Custom Spark Pool:**

- V workspace settings lze kromÄ› starter poolu vytvoÅ™it vlastnÃ­ pool (zmÄ›na node size, autoscale, executor range).
    
- VlastnÃ­ pool zpomalÃ­ startup na nÄ›kolik minut (vÃ­ce jako klasickÃ½ Spark).
    
- Po zapnutÃ­ â€manage private endpointsâ€œ ve Fabric nelze pouÅ¾Ã­vat starter pool v tenantovi.
    

**NastavenÃ­ poolÅ¯:**

- Workspace -> Spark settings -> vytvoÅ™it pool, nastavit parametry.
    
- **Environment:** speciÃ¡lnÃ­ Fabric asset, kde lze nastavit Spark konfigurace a instalovat Python knihovny.
    
    - Environment lze pouÅ¾Ã­t jako default workspace environment nebo i pro individuÃ¡lnÃ­ job/notebook.
        
    - VÃ½hoda: git integrace, moÅ¾nost verzovÃ¡nÃ­ v kontrolnÃ­m systÃ©mu.
        
    - NevÃ½hoda: nelze pouÅ¾Ã­t napÅ™Ã­Ä workspace, musÃ­ se duplikovat.
        

**Job-level konfigurace:**

- Lze vytvoÅ™it vÃ­ce environmentÅ¯ a pÅ™iÅ™adit k rÅ¯znÃ½m notebookÅ¯m/jobÅ¯m.
    
- OptimÃ¡lnÃ­ konfigurace pro rÅ¯znÃ© Ãºkoly.
    

---

## 2. DalÅ¡Ã­ Spark settingy workspace:

- **Reserve maximum cores:** MÅ¯Å¾eÅ¡ rezervovat maximÃ¡lnÃ­ poÄet jader pro job (â€pessimistickÃ© plÃ¡novÃ¡nÃ­â€œ â€“ zvyÅ¡uje stabilitu, omezuje dostupnost zdrojÅ¯ pro jinÃ© joby).
    
- **High concurrency:** PovolenÃ­ vÃ­cenÃ¡sobnÃ½ch Spark session pro jednoho uÅ¾ivatele nebo pipeline (tagovÃ¡nÃ­ session).
    
- **Capacity-level settings:** Capacity admin mÅ¯Å¾e povolit Äi zakÃ¡zat tvorbu custom poolÅ¯/workspaces, pÅ™Ã­padnÄ› i starter pool.
    

---

## 3. Domains (DomÃ©ny ve Fabric)

- **LogickÃ© seskupenÃ­ dat:** MapovÃ¡nÃ­ firemnÃ­ organizaÄnÃ­ struktury na Fabric â€” domÃ©na = skupina workspace (napÅ™. region, oddÄ›lenÃ­).
    
- **Role:**
    
    - **Fabric admin:** SprÃ¡va (tvorba/mazÃ¡nÃ­) domÃ©n, jmenovÃ¡nÃ­ domain adminÅ¯, pÅ™ehled vÅ¡ech domÃ©n.
        
    - **Domain admin:** SprÃ¡va a pÅ™iÅ™azovÃ¡nÃ­ workspace do domÃ©ny, jmenovÃ¡nÃ­ domain contributorÅ¯.
        
    - **Domain contributor:** Workspace admin â€” mÅ¯Å¾e workspace pÅ™iÅ™adit do domÃ©ny.
        
- **DelegovanÃ© nastavenÃ­ tenant:** DefaultnÃ­ sensitivity label a endorsement/certifikace lze povolit na Ãºrovni domÃ©ny.
    

---

## 4. OneLake & OneLake File Explorer

- VeÅ¡kerÃ¡ data z Fabric (lakehouse, warehouse) lze spravovat v centrÃ¡lnÃ­m cloudovÃ©m uloÅ¾iÅ¡ti OneLake.
    
- **File Explorer:** PodobnÄ› jako OneDrive klient, umoÅ¾Åˆuje stÃ¡hnout a prohlÃ­Å¾et obsah OneLake (jenom Delta tabulky z lakehouse/warehouse).
    
    - KQL databÃ¡ze a Power BI semantic modely standardnÄ› nejsou viditelnÃ© v File Explorer aÅ¾ po aktivaci â€OneLake integrationâ€œ v nastavenÃ­ (nutnÃ©, aby byly v reÅ¾imu import, ne direct).
        
- **OneLake integration:** Pro novÃ© tabulky v KQL db a Power BI semantic modelu; existujÃ­cÃ­ tabulky se backfillujÃ­ jen s novou funkcÃ­.
    

---

## 5. Shortcuts & Shortcut Caching

- **Shortcuts:** ReferencovÃ¡nÃ­ externÃ­ch dat (S3, Google Cloud Storage, ADLS Gen2).
    
- **Shortcut caching:** SnÃ­Å¾enÃ­ nÃ¡kladÅ¯ na egress u cloud providerÅ¯ uklÃ¡dÃ¡nÃ­m dat do Azure (caching).
    
    - Pravidla: max 1 GB soubor, retention 1â€“28 dnÅ¯ (po kaÅ¾dÃ©m pÅ™Ã­stupu se prodlouÅ¾Ã­).
        
    - Po 24+ hodinÃ¡ch (nebo nastavenÃ© dobÄ›) neaktivace je cache odstranÄ›na.
        

---

## 6. Apache Airflow Pools

- **OrchestrÃ¡tor:** SlouÅ¾Ã­ ke spouÅ¡tÄ›nÃ­ DAGÅ¯ (workflow, pipeline) â€” Fabric umoÅ¾Åˆuje Å™Ã­dit joby pÅ™es pool (nastavenÃ­ node size, autoscale).
    
- **NENÃ hlavnÃ­m tÃ©matem DP-700**, ale nastavenÃ­ je nutnÃ¡ znÃ¡t.
    

---

## DoporuÄenÃ­

- Pro praktickÃ½ trÃ©nink pouÅ¾Ã­vat hands-on tutoriÃ¡ly a pÅ™Ã­padnÄ› bootcampy.
    
- Kombinovat zdroje: video, dokumentace, Å¾ivÃ© sessions.
    
- SprÃ¡vnÄ› konfigurovat Spark pooly podle reÃ¡lnÃ½ch datovÃ½ch potÅ™eb, nejen podle defaultÅ¯.
    

---

_Tento markdown ti poskytne detailnÃ­ zÃ¡pis k Workspace Settings, Spark poolÅ¯m, domÃ©nÃ¡m, OneLake, shortcutÅ¯m a Airflow pools v Microsoft Fabric pro pÅ™Ã­pravu na DP-700 zkouÅ¡ku._[youtube](https://www.youtube.com/watch?v=-64AAqSavfo)â€‹

1. [https://www.youtube.com/watch?v=-64AAqSavfo](https://www.youtube.com/watch?v=-64AAqSavfo)

---
---

# ğŸš€ CI/CD

# CI/CD v Microsoft Fabric | DP-700 EXAM PREP (Video 3/11)

**TÃ©mata videa dle timeline:**

- Version Control (Git integrace)
    
- Deployment Pipelines
    
- Database Projects
    

---

## 1. Version Control (Git integrace)

- **MoÅ¾nosti:** Lze propojit workspace s Azure DevOps nebo GitHub repozitÃ¡Å™em.
    
- **NastavenÃ­:**
    
    - Na Ãºrovni tenant admina je nutnÃ© povolit integraci s Gitem v admin portÃ¡lu Fabric (napÅ™Ã­klad povolit synchronizaci workspace items s Gitem, mezinÃ¡rodnÃ­ repozitÃ¡Å™e, integrace s GitHubem).
        
    - Na stranÄ› Azure DevOps: je tÅ™eba nastavit organizaci, projekt, repozitÃ¡Å™ a pÅ™idat sprÃ¡vnÃ© uÅ¾ivatele (musÃ­ mÃ­t oprÃ¡vnÄ›nÃ­).
        
    - Konektivita workspace: v nastavenÃ­ workspace se definuje organizace, projekt, repozitÃ¡Å™, branch (pÅ™Ã­padnÄ› sloÅ¾ka/multirepo strategie v Gitu).
        
    - GitHub pÅ™ipojenÃ­ vyÅ¾aduje osobnÃ­ access token.
        
- **Role a oprÃ¡vnÄ›nÃ­:**
    
    - _Admin_: mÅ¯Å¾e pÅ™ipojit workspace, mÄ›nit branch pro vÅ¡echny uÅ¾ivatele, spravovat spojenÃ­.
        
    - _Member/Contributor_: mohou commitovat a pullovat zmÄ›ny, mohou vytvÃ¡Å™et novÃ© workspaces (pokud majÃ­ povoleno).
        
    - _Viewer_: pouze ÄtenÃ­, nemÃ¡ prÃ¡va na git operace.
        
- **Co se verzionuje?**
    
    - **NE verzionuje se:** Å¾Ã¡dnÃ¡ zdrojovÃ¡ data (tabulky, soubory v lakehouse/warehouse, schÃ©mata dat), plÃ¡novaÄe refreshÅ¯.
        
    - **Ano:** struktura data warehouse (create table/view), kÃ³d notebookÅ¯, pipelines atd.
        
    - Pokud nenÃ­ nÄ›co ve version control, nenÃ­ to ani deployovÃ¡no (napÅ™Ã­klad refresh schedule je tÅ™eba nastavit manuÃ¡lnÄ› nebo pÅ™es API).
        

---

## 2. Deployment Pipelines

- **Princip:** UmoÅ¾Åˆuje posouvat poloÅ¾ky mezi rÅ¯znÃ½mi prostÅ™edÃ­mi (bÄ›Å¾nÄ› Dev â†’ Test â†’ Prod, lze nastavit aÅ¾ 10 stages).
    
- **PraktickÃ© pouÅ¾itÃ­:** KopÃ­rovÃ¡nÃ­ notebookÅ¯, pipelines, semantic models atd. mezi workspaces â€“ testovÃ¡nÃ­ a nasazenÃ­.
    
- **Proces mÅ¯Å¾e bÃ½t manuÃ¡lnÃ­ nebo automatizovanÃ½** (API, PowerShell, Azure Pipelines).
    
- **DÅ¯leÅ¾itÃ© koncepty:**
    
    - **Deployment rules:** Lze mÄ›nit propojenÃ¡ data mezi prostÅ™edÃ­mi (napÅ™Ã­klad zmÄ›nit cÃ­lovÃ½ Lakehouse ve vyÅ¡Å¡Ã­m prostÅ™edÃ­).
        
    - **Item pairing:** PÅ™i nasazenÃ­ se propojÃ­ poloÅ¾ky mezi prostÅ™edÃ­mi (dokonce i pÅ™i zmÄ›nÄ› nÃ¡zvu zÅ¯stÃ¡vÃ¡ logickÃ© propojenÃ­).
        
    - **Coverage nenÃ­ 100%:** NejlepÅ¡Ã­ pokrytÃ­ je pro semantic models, pipelines, notebooks.
        

---

## 3. Database Projects

- **Co to je:** SQL Database Project pÅ™edstavuje zpÅ¯sob, jak verzovat a automatizovanÄ› nasazovat strukturu Fabric Data Warehouse.
    
- **Proces:**
    
    - _Workspace s git integracÃ­_: Po commitu database warehouse do Gitu vznikne SQL project (soubor .sql s CREATE TABLE atd.).
        
    - _Pro prÃ¡ci lokÃ¡lnÄ›_: KlonovÃ¡nÃ­ repozitÃ¡Å™e, otevÅ™enÃ­ v VS Code (rozÅ¡Ã­Å™enÃ­ SQL Database Projects).
        
    - _Editace schÃ©matu_: PÅ™idÃ¡vÃ¡nÃ­ tabulek, zmÄ›ny struktury.
        
    - _Build_: VznikÃ¡ DACPAC artifact (reprezentace schÃ©matu, bez dat).
        
    - _Deploy_: PomocÃ­ connection stringu lze nasadit DACPAC zpÄ›t do Fabric (a to i plnÄ› automatizovanÄ› â€“ napÅ™. pomocÃ­ Azure Pipelines).
        
- **VÃ½hoda:** Lze automatizovat celÃ½ CI/CD proces schÃ©matu data warehouse, vÄetnÄ› buildÅ¯, testÅ¯, release steps.
    

---

## ShrnutÃ­: KlÃ­ÄovÃ© poznatky

- SprÃ¡vnÄ› nastavenÃ¡ CI/CD pipeline umoÅ¾Åˆuje:
    
    - verzovÃ¡nÃ­ kÃ³du a struktury datovÃ½ch objektÅ¯ v Gitu (Azure DevOps, GitHub)
        
    - automatizovanÃ©/manaulnÃ­ nasazovÃ¡nÃ­ mezi prostÅ™edÃ­mi pÅ™es deployment pipelines
        
    - deklarativnÃ­ sprÃ¡vu datovÃ½ch warehouse pÅ™es SQL projects (infrastruktura jako kÃ³d)
        
- VÅ¾dy si dej pozor, co je a nenÃ­ ve version control â€“ pouze to, co je verzovÃ¡no, je opravdu automatizovatelnÄ› nasaditelnÃ©!
    

---

_DoporuÄenÃ­: VyzkouÅ¡ej si prakticky Git integraci, deployment pipelines i SQL database projects ve vlastnÃ­m test workspace s Azure DevOps nebo GitHubem._

1. [https://www.youtube.com/watch?v=JhTl_fDZsE0](https://www.youtube.com/watch?v=JhTl_fDZsE0)

---
---

# ğŸš€ Security & Governance

# Konfigurace Å™Ã­zenÃ­ pÅ™Ã­stupu v Microsoft Fabric (DP-700)

**Video:** [Configuring Access Control in Microsoft Fabric | DP-700 EXAM PREP (Video 4 of 11)]  
**Autor:** Learn Microsoft Fabric with Will  
**DÃ©lka:** 28 minut  
**Obsah videa:** BezpeÄnost, sprÃ¡va pÅ™Ã­stupu a governance v Microsoft Fabric

---

## 1. ÃšrovnÄ› Å™Ã­zenÃ­ pÅ™Ã­stupu

Microsoft Fabric umoÅ¾Åˆuje nastavovat pÅ™Ã­stup na nÄ›kolika ÃºrovnÃ­ch:

- **ÃšroveÅˆ workspace** â€“ pÅ™Ã­stup ke vÅ¡em objektÅ¯m v workspace.
    
- **ÃšroveÅˆ poloÅ¾ky (item)** â€“ pÅ™Ã­stup pouze ke konkrÃ©tnÃ­ poloÅ¾ce (napÅ™. datovÃ½ sklad nebo lakehouse).
    
- **ÃšroveÅˆ objektu nebo souboru** â€“ pÅ™Ã­stup ke konkrÃ©tnÃ­mu objektu nebo souboru.
    
- **GranulÃ¡rnÃ­ ÃºroveÅˆ** â€“ Å™Ã¡dek, sloupec, tabulka, soubor.
    

Princip **nejmenÅ¡Ã­ch prÃ¡v ("least privilege")**: dÃ¡vejte vÅ¾dy jen minimÃ¡lnÃ­ nutnÃ¡ prÃ¡va.

---

## 2. Workspace-level Access Control

Role ve workspace:

|Role|PrÃ¡va|
|---|---|
|Admin|VÅ¡echna prÃ¡va.|
|Member|VÄ›tÅ¡ina prÃ¡v, nemÅ¯Å¾e pÅ™idÃ¡vat adminy ani mazat workspace.|
|Contributor|ZÃ¡pis na Ãºrovni poloÅ¾ek, nemÅ¯Å¾e sdÃ­let ani mÄ›nit nÄ›kterÃ© poloÅ¾ky.|
|Viewer|Pouze prohlÃ­Å¾enÃ­ dat (pÅ™es SQL endpoint), nemÅ¯Å¾e spouÅ¡tÄ›t pipeliny.|

Workspace sdÃ­lÃ­te pÅ™es **Manage Access** â†’ definujete uÅ¾ivatele/skupinu a pÅ™iÅ™adÃ­te roli.

---

## 3. Item-level Access Control

- UÅ¾ivatel nebo skupina dostane pÅ™Ã­stup jen ke konkrÃ©tnÃ­ poloÅ¾ce (napÅ™. jen k Data Warehousu).
    
- UÅ¾ivatel nevidÃ­ workspace samotnÃ½, jen item pÅ™es **OneLake Catalog**.
    
- Bez dalÅ¡Ã­ch zaÅ¡krtnutÃ½ch prÃ¡v = pouze â€readâ€œ â€“ tedy pÅ™Ã­stup k samotnÃ© skladbÄ›, ne datÅ¯m v tabulkÃ¡ch.
    

**DalÅ¡Ã­ volby pÅ™i sdÃ­lenÃ­:**

- Read all data using SQL â€“ ÄtenÃ­ vÅ¡ech dat pÅ™es T-SQL.
    
- Read all OneLake data â€“ ÄtenÃ­ pÅ™es Spark, pipeliny, externÃ­ nÃ¡stroje.
    
- Build report on semantic model â€“ tvorba reportÅ¯.
    

---

## 4. GranulÃ¡rnÃ­ Å™Ã­zenÃ­ pÅ™Ã­stupu

KaÅ¾dÃ½ typ engine (Lakehouse, Data Warehouse) mÃ¡ vlastnÃ­ moÅ¾nosti:

|Typ|Metoda|Endpoint|
|---|---|---|
|ObjektovÃ©|OneLake Data Access / Grant Select|Lakehouse/Spark/T-SQL|
|Å˜Ã¡dkovÃ© (RLS)|Security Policy + TVF|T-SQL (DW/Lakehouse)|
|SloupcovÃ©|Grant Select (columns)|T-SQL|

**Pozor:** VyÅ¡Å¡Ã­ ÃºroveÅˆ pÅ™Ã­stupu (napÅ™. workspace Viewer) pÅ™eplnÃ­ niÅ¾Å¡Ã­ nastavenÃ­.

---

## 5. Implementace Row Level Security (RLS)

## Postup (T-SQL):

1. VytvoÅ™te schema `security` (volitelnÃ©)
    
2. VytvoÅ™te Table Valued Function (TVF), kterÃ¡ filtruje Å™Ã¡dky podle uÅ¾ivatele
    
3. Definujte Security Policy, kterÃ¡ tuto funkci aplikuje na konkrÃ©tnÃ­ tabulku
    

sql

`CREATE FUNCTION security.FilterRowForUser(@salesRep NVARCHAR) RETURNS TABLE AS RETURN SELECT * FROM Sales.Orders WHERE SalesRep = USER_NAME();`

sql

`CREATE SECURITY POLICY SalesOrderFilter ADD FILTER PREDICATE security.FilterRowForUser(SalesRep) ON Sales.Orders WITH (STATE = ON);`

---

## 6. ObjektovÃ¡/sloupcovÃ¡ bezpeÄnost

- **Grant SELECT na konkrÃ©tnÃ­ tabulku nebo sloupce**
    
- SdÃ­lÃ­te s SQL rolÃ­ nebo konkrÃ©tnÃ­m uÅ¾ivatelem (entra ID)
    
- OpÄ›t â€“ bez workspace prÃ¡v
    

sql

`GRANT SELECT ON Sales.Orders TO SalesReps; GRANT SELECT ON Sales.CustomerDetails (CustomerID, Name) TO SalesReps;`

---

## 7. OneLake Data Access (Lakehouse)

- Role-based pÅ™Ã­stup k tabulkÃ¡m/souborÅ¯m
    
- V Lakehouse vytvoÅ™Ã­te roli, pÅ™iÅ™adÃ­te tabulky/soubory, pÅ™idÃ¡te Äleny (ruÄnÄ› nebo podle existujÃ­cÃ­ch prÃ¡v)
    
- Po aktivaci funkce uÅ¾ ji nelze vrÃ¡tit zpÄ›t
    

---

## 8. DynamickÃ© maskovÃ¡nÃ­ dat (Dynamic Data Masking)

ZakrytÃ­ vybranÃ½ch dat pro vybranÃ© uÅ¾ivatele â€“ data zÅ¯stÃ¡vajÃ­ nemÄ›nÄ›na v pozadÃ­.

PouÅ¾itÃ­ v T-SQL (pÅ™i vytvÃ¡Å™enÃ­ Äi ÃºpravÄ› tabulky):

sql

`CREATE TABLE EmployeeData (     FirstName NVARCHAR(100) MASKED WITH (FUNCTION = 'default()') ); ALTER TABLE EmployeeData ALTER COLUMN Email ADD MASKED WITH (FUNCTION = 'email()'); ALTER TABLE EmployeeData ALTER COLUMN Salary ADD MASKED WITH (FUNCTION = 'random(50000, 100000)'); ALTER TABLE EmployeeData ALTER COLUMN Phone ADD MASKED WITH (FUNCTION = 'partial(4,"X",0)');`

Typy masek:

- **default** â€“ univerzÃ¡lnÃ­ maska
    
- **email** â€“ maskuje vÅ¡e kromÄ› prvnÃ­ho znaku a ÄÃ¡sti po @
    
- **random** â€“ generuje nÃ¡hodnÃ© ÄÃ­slo (napÅ™. pro plat)
    
- **partial** â€“ viditelnÃ½ prefix/padding/suffix
    

**Pozor:** MaskovÃ¡nÃ­ nenÃ­ bezpeÄnostnÃ­ opatÅ™enÃ­ â€“ lze obejÃ­t chytrÃ½mi dotazy.

---

## 9. Data Governance (oznaÄovÃ¡nÃ­ a endorsement)

- **Sensitivity label:** Ochrana informacÃ­, spravovÃ¡no v Purview, nastavitelnÃ© na poloÅ¾kÃ¡ch v topbar/settings
    
- **Endorsement:**
    
    - Promoted â€“ pÅ™ipravenÃ© k sdÃ­lenÃ­/reuse
        
    - Certified â€“ splÅˆuje firemnÃ­ standardy (pouze urÄenÃ¡ skupina mÅ¯Å¾e certifikovat)
        
    - Master data â€“ core datovÃ½ zdroj (jen admin skupina)
        

---

## KlÃ­ÄovÃ© frÃ¡ze pro zkouÅ¡ku DP-700

- **Certified:** SplÅˆuje standardy kvality organizace
    
- **Master data:** HlavnÃ­ zdroj dat organizace
    
- **Promoted:** PÅ™ipraveno k sdÃ­lenÃ­
    

---

> DalÅ¡Ã­ DP-700 studijnÃ­ materiÃ¡ly najdeÅ¡ na fabricdojo nebo playlistu Learn Microsoft Fabric.[youtube](https://www.youtube.com/watch?v=pFvVAZCyYhc)â€‹

---

1. [https://www.youtube.com/watch?v=pFvVAZCyYhc](https://www.youtube.com/watch?v=pFvVAZCyYhc)

---
---

# Orchestration

# Orchestration in Microsoft Fabric | DP-700 (Video 5/11) [youtube](https://www.youtube.com/watch?v=U7t9j95d1WA)â€‹

## TÃ©mata videa podle sylabu DP-700

- Orchestrate processes: kdy pouÅ¾Ã­t pipeline vs. notebook, orchestraÄnÃ­ patterny, parametry, dynamickÃ© vÃ½razy.[youtube](https://www.youtube.com/watch?v=U7t9j95d1WA)â€‹
    
- Triggery: plÃ¡novanÃ© a event-based, vÄetnÄ› Real-Time Hub scÃ©nÃ¡Å™Å¯.[youtube](https://www.youtube.com/watch?v=U7t9j95d1WA)â€‹
    
- Ingest a transform batch data: ingest pomocÃ­ pipelines (Copy Data, metadataâ€‘driven pÅ™Ã­stup).[youtube](https://www.youtube.com/watch?v=U7t9j95d1WA)â€‹
    

---

## 1. Data Pipelines â€“ zÃ¡kladnÃ­ koncepty

## Co je data pipeline ve Fabricu

- Data pipeline je orchestrÃ¡tor, kterÃ½ spouÅ¡tÃ­ rÅ¯znÃ© Fabric assety (notebooky, Spark jobs, KQL DB, data warehouses, semantic model refresh) i externÃ­ sluÅ¾by (Azure Functions, Databricks, REST/HTTP, webhooks).[youtube](https://www.youtube.com/watch?v=U7t9j95d1WA)â€‹
    
- Pipelines jsou postavenÃ© jako DAG (directed acyclic graph) â€“ propojenÃ© aktivity s definovanÃ½mi zÃ¡vislostmi a podmÃ­nkami, jak na sebe navazujÃ­.[youtube](https://www.youtube.com/watch?v=U7t9j95d1WA)â€‹
    

## Aktivity v pipeline

TypickÃ© aktivity, kterÃ© pro DP-700 potÅ™ebujeÅ¡ znÃ¡t:[youtube](https://www.youtube.com/watch?v=U7t9j95d1WA)â€‹

- Copy Data â€“ hlavnÃ­ nÃ¡stroj pro ingest dat.
    
- Notebook activity â€“ spuÅ¡tÄ›nÃ­ Spark notebooku z pipeline.
    
- Dataflow Gen2 activity â€“ spuÅ¡tÄ›nÃ­ dataflow.
    
- Script activity â€“ vykonÃ¡nÃ­ T-SQL nebo jinÃ©ho skriptu.
    
- Semantic model refresh activity â€“ Å™Ã­zenÃ½ refresh modelu.
    
- Invoke pipeline â€“ volÃ¡nÃ­ jinÃ© pipeline (parentâ€“child pattern).
    
- Web/REST/HTTP aktivity â€“ volÃ¡nÃ­ API a web endpointÅ¯.
    
- Azure Activities â€“ volÃ¡nÃ­ Azure Functions, Databricks notebookÅ¯ a dalÅ¡Ã­ch sluÅ¾eb.
    
- NotifikaÄnÃ­ aktivity â€“ Office 365 a Teams pro alerty pÅ™i ÃºspÄ›chu/chybÄ›.
    

---

## 2. Copy Data â€“ ingesce dat

## Zdroje pro Copy Data

Copy Data aktivita obsluhuje vÄ›tÅ¡inu scÃ©nÃ¡Å™Å¯ batch ingesce:[youtube](https://www.youtube.com/watch?v=U7t9j95d1WA)â€‹

- Azure zdroje (Blob, Data Lake, Azure SQL, Synapse, atd.).
    
- SQL Server onâ€‘prem pÅ™es onâ€‘prem data gateway (dÅ¯leÅ¾itÃ© pro hybridnÃ­ scÃ©nÃ¡Å™e).
    
- REST konektor â€“ GET/POST na API, podporuje zÃ¡kladnÃ­ pagination (u sloÅ¾itÄ›jÅ¡Ã­ch API je lepÅ¡Ã­ pÅ™ejÃ­t do notebooku).
    
- HTTP konektor â€“ staÅ¾enÃ­ dat z otevÅ™enÃ½ch webovÃ½ch endpointÅ¯.
    

## CÃ­le pro Copy Data

- Lakehouse (files area) pro souborovÃ¡ data.[youtube](https://www.youtube.com/watch?v=U7t9j95d1WA)â€‹
    
- Lakehouse/warehouse tabulky pro tabulkovÃ¡ data.[youtube](https://www.youtube.com/watch?v=U7t9j95d1WA)â€‹
    
- ExternÃ­ cÃ­le â€“ napÅ™. Azure SQL Database (Fabric orchestruje ETL, ale cÃ­l je mimo Fabric).[youtube](https://www.youtube.com/watch?v=U7t9j95d1WA)â€‹
    

## JednoduchÃ½ scÃ©nÃ¡Å™

- NejzÃ¡kladnÄ›jÅ¡Ã­ pipeline mÅ¯Å¾e obsahovat jedinou Copy Data aktivitu: vezmi data ze zdroje a uloÅ¾ do Fabricu.[youtube](https://www.youtube.com/watch?v=U7t9j95d1WA)â€‹
    
- V UI nastavÃ­Å¡ pÅ™ipojenÃ­, relativnÃ­ URL/soubor, formÃ¡t dat (CSV/JSON/Parquet atd.), mapovÃ¡nÃ­ schÃ©matu a cÃ­lovou destinaci.[youtube](https://www.youtube.com/watch?v=U7t9j95d1WA)â€‹
    

---

## 3. PropojovÃ¡nÃ­ aktivit a Å™Ã­zenÃ­ toku

## PodmÃ­nÄ›nÃ© napojenÃ­ aktivit

KaÅ¾dÃ© propojenÃ­ mezi aktivitami mÃ¡ podmÃ­nku, kdy se dalÅ¡Ã­ krok spustÃ­:[youtube](https://www.youtube.com/watch?v=U7t9j95d1WA)â€‹

- On Success â€“ bÄ›Å¾nÃ½ scÃ©nÃ¡Å™; dalÅ¡Ã­ aktivita se spustÃ­ jen pÅ™i ÃºspÄ›chu.
    
- On Fail â€“ dalÅ¡Ã­ krok se spustÃ­ jen pÅ™i selhÃ¡nÃ­ (napÅ™. notifikace, kompenzaÄnÃ­ job).
    
- On Completion â€“ spustÃ­ se vÅ¾dy (bez ohledu na vÃ½sledek); vhodnÃ© pro cleanup.
    
- On Skip â€“ dalÅ¡Ã­ aktivita se spustÃ­ pouze, pokud pÅ™edchozÃ­ byla pÅ™eskoÄena.
    

PÅ™Ã­klad miniâ€‘DAGu:[youtube](https://www.youtube.com/watch?v=U7t9j95d1WA)â€‹

- Copy Data (ingesce) â†’ On Success â†’ Script/Transform â†’ On Success â†’ Semantic model refresh.
    
- Semantic model refresh â†’ On Fail â†’ Odeslat Teams/O365 notifikaci.
    

## AktivnÃ­ vs. neaktivnÃ­ aktivity

- Aktivitu lze pÅ™epnout na â€Inactiveâ€œ (deaktivovat), takÅ¾e se pÅ™i bÄ›hu pipeline pÅ™eskoÄÃ­.[youtube](https://www.youtube.com/watch?v=U7t9j95d1WA)â€‹
    
- V UI je zaÅ¡edlÃ¡ a ve vÃ½sledcÃ­ch bÄ›hu uvidÃ­Å¡ status â€Inactiveâ€œ, coÅ¾ je ideÃ¡lnÃ­ pÅ™i ladÄ›nÃ­ dlouhÃ½ch pipeline (doÄasnÄ› vypneÅ¡ ÄÃ¡st grafu).[youtube](https://www.youtube.com/watch?v=U7t9j95d1WA)â€‹
    

---

## 4. Metadataâ€‘driven pipelines (bez hardâ€‘codu)

## ProblÃ©m hardâ€‘codu

- Pokud v Copy Data zadÃ¡Å¡ natvrdo relativnÃ­ URL/tabulku, pipeline umÃ­ typicky jen jeden dataset (napÅ™. jednu CSV nebo jednu tabulku).[youtube](https://www.youtube.com/watch?v=U7t9j95d1WA)â€‹
    
- V praxi chceÅ¡ Äasto zpracovat desÃ­tky tabulek/endpointÅ¯ z jednoho zdroje bez psanÃ­ 25 rÅ¯znÃ½ch pipeline.[youtube](https://www.youtube.com/watch?v=U7t9j95d1WA)â€‹
    

## Metadata a jejich umÃ­stÄ›nÃ­

Metadata mÅ¯Å¾eÅ¡ drÅ¾et:[youtube](https://www.youtube.com/watch?v=U7t9j95d1WA)â€‹

- V tabulce ve Fabric Warehouse/Fabric SQL.
    
- V externÃ­ SQL databÃ¡zi.
    
- V souborech (JSON/CSV) v OneLake.
    

TypickÃ© sloupce metadat: nÃ¡zev zdrojovÃ© tabulky, schÃ©ma, cÃ­lovÃ¡ tabulka, relativnÃ­ URL, typ souboru, pÅ™Ã­padnÄ› dalÅ¡Ã­ parametry.[youtube](https://www.youtube.com/watch?v=U7t9j95d1WA)â€‹

## Pattern: Lookup + ForEach + Copy Data

1. Lookup/Script aktivita naÄte metadata (napÅ™. 25 Å™Ã¡dkÅ¯ = 25 tabulek).[youtube](https://www.youtube.com/watch?v=U7t9j95d1WA)â€‹
    
2. VÃ½sledek pÅ™edÃ¡Å¡ do ForEach aktivity.[youtube](https://www.youtube.com/watch?v=U7t9j95d1WA)â€‹
    
3. UvnitÅ™ ForEach je jedna Copy Data aktivita, kde Source/Relative URL a cÃ­lovÃ© nÃ¡zvy bereÅ¡ dynamicky z `item()` (napÅ™. `@item().SourcePath`, `@item().DestinationTable`).[youtube](https://www.youtube.com/watch?v=U7t9j95d1WA)â€‹
    

VÃ½hody:[youtube](https://www.youtube.com/watch?v=U7t9j95d1WA)â€‹

- PÅ™idÃ¡nÃ­ dalÅ¡Ã­ tabulky = pÅ™idÃ¡Å¡ Å™Ã¡dek do metadata tabulky, pipeline se nemÄ›nÃ­.
    
- ÃšdrÅ¾ba je pouze v â€instrukÄnÃ­â€œ metadata tabulce, logika orchestraci zÅ¯stÃ¡vÃ¡ stabilnÃ­.
    

---

## 5. Parentâ€“child architektura pipelines

## Kdy to pouÅ¾Ã­t

- KdyÅ¾ potÅ™ebujeÅ¡ v rÃ¡mci ForEach pro kaÅ¾dou tabulku provÃ©st komplexnÄ›jÅ¡Ã­ workflow: ingest â†’ transform (silver) â†’ gold â†’ semantic model refresh, pÅ™Ã­padnÄ› dalÅ¡Ã­ skripty.[youtube](https://www.youtube.com/watch?v=U7t9j95d1WA)â€‹
    
- NechceÅ¡ mÃ­t obrovskou pipeline se stovkami propojenÃ½ch aktivit.[youtube](https://www.youtube.com/watch?v=U7t9j95d1WA)â€‹
    

## Parent pipeline

- NaÄte metadata (Lookup/Script).[youtube](https://www.youtube.com/watch?v=U7t9j95d1WA)â€‹
    
- ForEach pÅ™es jednotlivÃ© Å™Ã¡dky metadat.[youtube](https://www.youtube.com/watch?v=U7t9j95d1WA)â€‹
    
- UvnitÅ™ ForEach pouÅ¾ije Invoke pipeline aktivitu (doporuÄenÃ¡ â€Legacyâ€œ varianta, pokud potÅ™ebujeÅ¡ vracet hodnoty z child).[youtube](https://www.youtube.com/watch?v=U7t9j95d1WA)â€‹
    
- V Invoke pipeline nastavÃ­Å¡ parametry, kterÃ© pÅ™edÃ¡Å¡ child pipeline (napÅ™. `SourceDirectory`, `SourceFileName`, `DestinationTableName`, `ExpectedColumnNames`).[youtube](https://www.youtube.com/watch?v=U7t9j95d1WA)â€‹
    

## Child pipeline

- MÃ¡ definovanÃ© pipeline parameters, na kterÃ© parent posÃ­lÃ¡ hodnoty.[youtube](https://www.youtube.com/watch?v=U7t9j95d1WA)â€‹
    
- V aktivitÃ¡ch pouÅ¾Ã­vÃ¡Å¡ `@pipeline().parameters.ParamName` (napÅ™. v Copy Data jako zdroj/cÃ­l).[youtube](https://www.youtube.com/watch?v=U7t9j95d1WA)â€‹
    
- Child pipeline mÅ¯Å¾e obsahovat dalÅ¡Ã­ ForEach, Copy Data, skripty, transformace a semantic model refresh â€“ vÅ¡e znovupouÅ¾itelnÃ© pro rÅ¯znÃ© parent pipelines.[youtube](https://www.youtube.com/watch?v=U7t9j95d1WA)â€‹
    

VÃ½hody:[youtube](https://www.youtube.com/watch?v=U7t9j95d1WA)â€‹

- PÅ™ehlednÄ›jÅ¡Ã­ a znovupouÅ¾itelnÃ© orchestrace.
    
- MoÅ¾nost vracet stav/ hodnoty z child do parent (s Legacy Invoke activity).
    

---

## 6. Notebook orchestrace pomocÃ­ notebookutils

## Kdy orchestraci dÄ›lat v notebooku

- KdyÅ¾ chceÅ¡ orchestraci ÄistÄ› v rÃ¡mci Spark prostÅ™edÃ­ a pracovat s jednou sdÃ­lenou Spark session.[youtube](https://www.youtube.com/watch?v=U7t9j95d1WA)â€‹
    
- KdyÅ¾ tvÃ© workflow je â€notebook firstâ€œ a nepotÅ™ebujeÅ¡ zapojit tolik jinÃ½ch asset typÅ¯ jako v pipeline.[youtube](https://www.youtube.com/watch?v=U7t9j95d1WA)â€‹
    

## Notebookutils â€“ modul notebook

- Knihovna `notebookutils` obsahuje Å™adu modulÅ¯ (files, credentials, atd.), pro orchestrace je klÃ­ÄovÃ½ modul `notebook`.[youtube](https://www.youtube.com/watch?v=U7t9j95d1WA)â€‹
    
- Ten umoÅ¾Åˆuje spouÅ¡tÄ›nÃ­ dalÅ¡Ã­ch notebookÅ¯ z jednoho orchestrujÃ­cÃ­ho notebooku.[youtube](https://www.youtube.com/watch?v=U7t9j95d1WA)â€‹
    

ZÃ¡kladnÃ­ pattern:[youtube](https://www.youtube.com/watch?v=U7t9j95d1WA)â€‹

- ImportujeÅ¡ modul (napÅ™. `from notebookutils import notebook as nb`).
    
- PouÅ¾Ã­vÃ¡Å¡ `nb.run_multiple(...)` pro spuÅ¡tÄ›nÃ­ vÃ­ce notebookÅ¯.
    

## ParalelnÃ­ bÄ›h notebookÅ¯

- `run_multiple` mÅ¯Å¾e vzÃ­t jednoduchÃ½ python list nÃ¡zvÅ¯ notebookÅ¯.[youtube](https://www.youtube.com/watch?v=U7t9j95d1WA)â€‹
    
- VÅ¡echny notebooky v seznamu se spustÃ­ paralelnÄ› (poÅ™adÃ­ nehraje roli).[youtube](https://www.youtube.com/watch?v=U7t9j95d1WA)â€‹
    
- VhodnÃ© pro nezÃ¡vislÃ© datovÃ© Ãºlohy (napÅ™. rÅ¯znÃ© domÃ©ny).[youtube](https://www.youtube.com/watch?v=U7t9j95d1WA)â€‹
    

## DAG pro notebook orchestrace

- MÃ­sto listu mÅ¯Å¾eÅ¡ `run_multiple` pÅ™edat DAG (Python dict) se strukturou:[youtube](https://www.youtube.com/watch?v=U7t9j95d1WA)â€‹
    
    - `activities`: list objektÅ¯, kaÅ¾dÃ½ definuje jeden notebook (`name`, `path` + volitelnÃ© `timeoutPerCellInSeconds`, `retry`, `retryIntervalInSeconds`, `dependencies`).
        
- `dependencies` Å™Ã­kÃ¡, po kterÃ½ch aktivitÃ¡ch se konkrÃ©tnÃ­ notebook spustÃ­ (stejnÃ½ princip jako zÃ¡vislosti v pipeline).[youtube](https://www.youtube.com/watch?v=U7t9j95d1WA)â€‹
    
- PouÅ¾ij `validate_dag(...)` pro kontrolu, Å¾e DAG je validnÃ­, a volbu `display_dag_via_graphviz=True` pro vizuÃ¡lnÃ­ zobrazenÃ­ grafu.[youtube](https://www.youtube.com/watch?v=U7t9j95d1WA)â€‹
    

## VÃ½hoda sdÃ­lenÃ© Spark session

- OrchestrujÃ­cÃ­ notebook mÅ¯Å¾e spouÅ¡tÄ›t dalÅ¡Ã­ Spark notebooky ve stejnÃ© Spark session, takÅ¾e sdÃ­lÃ­Å¡ cache, nastavenÃ© promÄ›nnÃ©, knihovny atd.[youtube](https://www.youtube.com/watch?v=U7t9j95d1WA)â€‹
    
- V minulosti to byla velkÃ¡ vÃ½hoda proti pipelines, ale nynÃ­ majÃ­ pipelines â€session tagsâ€œ, takÅ¾e vÃ­c notebookÅ¯ spuÅ¡tÄ›nÃ½ch z pipeline mÅ¯Å¾e bÄ›Å¾et na stejnÃ© session.[youtube](https://www.youtube.com/watch?v=U7t9j95d1WA)â€‹
    

---

## 7. Triggery v Microsoft Fabric

## Schedule triggery

- Lze je nastavit pro: Data Pipelines, notebooky, Dataflow Gen2.[youtube](https://www.youtube.com/watch?v=U7t9j95d1WA)â€‹
    
- DefinujeÅ¡ start time, end time a time zone (defaultnÄ› UTC).[youtube](https://www.youtube.com/watch?v=U7t9j95d1WA)â€‹
    

DoporuÄenÃ½ pÅ™Ã­stup:[youtube](https://www.youtube.com/watch?v=U7t9j95d1WA)â€‹

- Triggery nastavovat primÃ¡rnÄ› na pipelines a z nich spouÅ¡tÄ›t notebooky/dataflows, aby byla orchestrace centralizovanÃ¡ a lÃ©pe monitorovatelnÃ¡.
    

## Eventâ€‘based triggery (Blob Storage)

- Pro Data Pipelines existuje event-based trigger, kterÃ½ reaguje na udÃ¡losti v Azure Blob Storage (napÅ™. novÃ½ soubor, zmÄ›na).[youtube](https://www.youtube.com/watch?v=U7t9j95d1WA)â€‹
    
- V konfiguraci zvolÃ­Å¡ pipeline, storage account a typ udÃ¡losti, na kterou se mÃ¡ reagovat.[youtube](https://www.youtube.com/watch?v=U7t9j95d1WA)â€‹
    

Useâ€‘case:[youtube](https://www.youtube.com/watch?v=U7t9j95d1WA)â€‹

- Fileâ€‘driven ingesce â€“ upload souboru do Blobu automaticky spustÃ­ ingest pipeline ve Fabricu (napÅ™. nÃ¡hrada za ADF/Synapse patterns).
    

## Real-Time Hub triggery (preview)

- Real-Time Hub pÅ™idÃ¡vÃ¡ dalÅ¡Ã­ typy eventÅ¯, kterÃ© mohou spouÅ¡tÄ›t Fabric artefakty:[youtube](https://www.youtube.com/watch?v=U7t9j95d1WA)â€‹
    
    - Job events â€“ zmÄ›ny stavu jobÅ¯ (vytvoÅ™enÃ­, success, fail).
        
    - OneLake events â€“ udÃ¡losti nad soubory/sloÅ¾kami v OneLake.
        
    - Workspace item events â€“ zmÄ›ny nad poloÅ¾kami ve workspace.
        
- Tyto eventy lze pouÅ¾Ã­t pro spuÅ¡tÄ›nÃ­ Data Activator (Activator alerts), Event Streams nebo Data Pipelines.[youtube](https://www.youtube.com/watch?v=U7t9j95d1WA)â€‹
    

---

## 8. Semantic Models â€“ orchestrace refreshÅ¯

## Auto refresh (Direct Lake)

- U Direct Lake semantic modelÅ¯ lze zapnout auto refresh â€“ zmÄ›ny v OneLake datech se automaticky projevÃ­ v modelu pÅ™i dotazovÃ¡nÃ­.[youtube](https://www.youtube.com/watch?v=U7t9j95d1WA)â€‹
    
- JednoduchÃ©, ale z hlediska kontrolovanÃ©ho ETL mÅ¯Å¾e bÃ½t riskantnÃ­ (bÄ›hem bÄ›hu pipeline mohou bÃ½t nÄ›kterÃ© tabulky uÅ¾ po ETL, jinÃ© jeÅ¡tÄ› ne).[youtube](https://www.youtube.com/watch?v=U7t9j95d1WA)â€‹
    

## Å˜Ã­zenÃ½ refresh pÅ™es pipeline

- V semantic model settings mÅ¯Å¾eÅ¡ auto refresh vypnout a nastavit scheduled refresh (hlavnÄ› pro Import).[youtube](https://www.youtube.com/watch?v=U7t9j95d1WA)â€‹
    
- AlternativnÄ› pouÅ¾ijeÅ¡ â€Semantic model refreshâ€œ aktivitu v pipeline:[youtube](https://www.youtube.com/watch?v=U7t9j95d1WA)â€‹
    
    - Nejprve probÄ›hne ingest/transform (bronze/silver/gold) do tabulek.
        
    - Po ÃºspÄ›chu vÅ¡ech krokÅ¯ spustÃ­Å¡ refresh semantic modelu.
        

VÃ½hoda:[youtube](https://www.youtube.com/watch?v=U7t9j95d1WA)â€‹

- UÅ¾ivatelÃ© vÅ¾dy vidÃ­ konzistentnÃ­ pohled â€“ semantic model se aktualizuje aÅ¾ po kompletnÃ­m dokonÄenÃ­ ETL.
    

## Semantic Link â€“ refresh z notebooku

- PomocÃ­ Semantic Link (Python knihovna) mÅ¯Å¾eÅ¡ programovÄ› refreshovat semantic model (napÅ™. metodou `refresh_dataset`) pÅ™Ã­mo z notebooku.[youtube](https://www.youtube.com/watch?v=U7t9j95d1WA)â€‹
    
- UmoÅ¾Åˆuje orchestraci ÄistÄ› z notebook orchestrace, pokud nechceÅ¡ pouÅ¾Ã­t pipeline aktivitu.[youtube](https://www.youtube.com/watch?v=U7t9j95d1WA)â€‹
    

---

## 9. Co si odnÃ©st pro DPâ€‘700 a praxi

- Pro orchestrace ve Fabricu pouÅ¾Ã­vej primÃ¡rnÄ› Data Pipelines, kterÃ© dokÃ¡Å¾ou spouÅ¡tÄ›t rÅ¯znÃ© Fabric a Azure assety, Å™Ã­dit tok, retry a notifikace.[youtube](https://www.youtube.com/watch?v=U7t9j95d1WA)â€‹
    
- Stav pipeline navrhuj metadataâ€‘driven (Lookup â†’ ForEach â†’ Copy Data) a pro komplexnÄ›jÅ¡Ã­ logiku pouÅ¾Ã­vej parentâ€“child pattern s Invoke pipeline.[youtube](https://www.youtube.com/watch?v=U7t9j95d1WA)â€‹
    
- Notebook orchestrace pÅ™es `notebookutils` se hodÃ­, pokud chceÅ¡ orchestraci uvnitÅ™ notebookovÃ©ho/Spark svÄ›ta, zejmÃ©na se sdÃ­lenou session.[youtube](https://www.youtube.com/watch?v=U7t9j95d1WA)â€‹
    
- Triggery (schedule, event-based, Real-Time Hub) pouÅ¾ij pro automatizaci procesÅ¯ â€“ batch plÃ¡novÃ¡nÃ­, fileâ€‘based ingesci a realâ€‘time scÃ©nÃ¡Å™e.[youtube](https://www.youtube.com/watch?v=U7t9j95d1WA)â€‹
    
- Semantic model refresh integruj do orchestracÃ­ (pipeline nebo notebook), abys mÄ›l plnou kontrolu nad tÃ­m, kdy jsou reporty aktualizovanÃ©.[youtube](https://www.youtube.com/watch?v=U7t9j95d1WA)â€‹
    

1. [https://www.youtube.com/watch?v=U7t9j95d1WA](https://www.youtube.com/watch?v=U7t9j95d1WA)
2. [https://www.skool.com/microsoft-fabric/classroom/4a13f708?md=555a311ce05147178dd7bff365229b62](https://www.skool.com/microsoft-fabric/classroom/4a13f708?md=555a311ce05147178dd7bff365229b62)

---
---

# ğŸš€ Decision Guides (plus Shortcuts & Mirroring)
