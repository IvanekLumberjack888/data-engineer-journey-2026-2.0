# ğŸ“… 20dennÃ­ studijnÃ­ plÃ¡n: Microsoft Fabric Data Engineer (DP-700)
[[PLÃN JE JEDNA VÄšC (JAK NA NÄšJ?)]]

## ğŸš€ FÃ¡ze 1: ZÃ¡klady a architektura (Dny 1â€“4)

|   |   |   |
|---|---|---|
|Den|TÃ©ma|KlÃ­ÄovÃ© Fabric Itemy|
|**1**|Ãšvod do Fabric a zkouÅ¡ky|Tenant, Capacity, Workspace|
|**2**|OneLake a ÃºloÅ¾iÅ¡tÄ›|OneLake, Shortcuts, Delta/Parquet|
|**3**|Lakehouse: JÃ¡dro DE|Lakehouse, Files, Tables (Delta)|
|**4**|Data Warehouse (DW)|Warehouse, T-SQL Endpoint, Schemas|

### Den 1: Ãšvod, rozsah zkouÅ¡ky a zÃ¡kladnÃ­ koncepty

|   |   |
|---|---|
|Sekce|Obsah|
|**Theory Focus (English)**|**DP-700 Exam Scope & Fabric Hierarchy**Â The DP-700 exam measures skills across three domains: implementing and managing an analytic solution, ingesting and transforming data, and monitoring and optimizing the solution. Microsoft Fabric is structured hierarchically. The highest level is theÂ **Tenant**Â (your organization), which hostsÂ **Capacities**Â (compute resources) that determine the available processing power and billing (F-SKUs).Â **Workspaces**Â act as collaborative containers for all Fabric items (Lakehouse, Pipelines, etc.) within a Capacity.|
|**ÄŒeskÃ© vysvÄ›tlenÃ­ pojmÅ¯**|ZkouÅ¡ka DP-700 pokrÃ½vÃ¡ tÅ™i hlavnÃ­ oblasti: sprÃ¡vu a implementaci (napÅ™. nastavenÃ­ zabezpeÄenÃ­), naÄÃ­tÃ¡nÃ­ a transformaci dat (napÅ™. Pipelines, PySpark, KQL), a sledovÃ¡nÃ­ vÃ½konu a optimalizace.Â **Tenant**Â je VaÅ¡e organizace.Â **Kapacita**Â (Capacity) je vÃ½poÄetnÃ­ vÃ½kon (CU - Capacity Units), kterÃ½ platÃ­te.Â **Workspace**Â je logickÃ½ kontejner pro vÅ¡echny Fabric objekty (items).|
|**Mini-praktickÃ¡ Ãºloha**|Identifikujte ve VaÅ¡em testovacÃ­m Fabric prostÅ™edÃ­, kde se nastavujÃ­Â **Workspace Settings**Â (napÅ™.Â **Spark settings**Â neboÂ **Domains**) a kdo spravujeÂ **Capacity Settings**Â (Capacity Admin).|
|**Exam-style Question**|_Which Fabric entity primarily determines the available compute power and resource allocation for data processing workloads in your organization?_Â (A) Workspace, (B) Item, (C) Capacity, (D) Tenant.|
|**Mikro lekce angliÄtiny**|**Core Terms:**Â _Compute_,Â _Resource Allocation_,Â _Workload_,Â _Tenant_.Â **FrÃ¡ze:**Â _Implementing Data Solutions_Â (implementace datovÃ½ch Å™eÅ¡enÃ­),Â _Ingest and Transform_Â (naÄÃ­tÃ¡nÃ­ a transformace),Â _Monitoring and Optimization_Â (sledovÃ¡nÃ­ a optimalizace).|
|**Souhrn a DomÃ¡cÃ­ procviÄenÃ­ (10 min)**|Dnes umÃ­te popsat zÃ¡kladnÃ­ hierarchii Fabricu (Tenant â†’ Capacity â†’ Workspace) a znÃ¡te tÅ™i hlavnÃ­ domÃ©ny zkouÅ¡ky DP-700.Â **DomÃ¡cÃ­ Ãºkol:**Â ProjdÄ›te si oficiÃ¡lnÃ­ DP-700 Study Guide (ÃºvodnÃ­ sekce) a zapiÅ¡te si 5 klÃ­ÄovÃ½ch nÃ¡strojÅ¯, kterÃ© patÅ™Ã­ do sekceÂ **Ingest and Transform**Â (napÅ™. Notebooks, Pipelines, Dataflows Gen2).|

--------------------------------------------------------------------------------

### Den 2: OneLake a ÃºloÅ¾iÅ¡tÄ›

|   |   |
|---|---|
|Sekce|Obsah|
|**Theory Focus (English)**|**OneLake and Open Data Format**Â **OneLake**Â acts as the single unified data lake for the entire organization (often called "OneDrive for data"). All structured data in OneLake is stored in the openÂ **Delta Parquet**Â format, which ensures interoperability between different Fabric engines (SQL, Spark, KQL).Â **Shortcuts**Â are virtual links that allow accessing data physically stored in external sources (like ADLS Gen2, AWS S3, or Google Cloud Storage) or in another Fabric item, without replicating the data.|
|**ÄŒeskÃ© vysvÄ›tlenÃ­ pojmÅ¯**|**OneLake**Â je centrÃ¡lnÃ­ datovÃ© jezero, kde je uloÅ¾enaÂ **jedinÃ¡ kopie dat**Â (single copy of data), ÄÃ­mÅ¾ se eliminujÃ­ datovÃ¡ sila a duplicity.Â **Delta Parquet**Â je standardnÃ­ formÃ¡t pro tabulkovÃ¡ data, kterÃ½ je optimalizovanÃ½ pro analÃ½zu a podporuje transakce (ACID) a verzovÃ¡nÃ­ (time travel).Â **Shortcuts**Â slouÅ¾Ã­ kÂ **virtualizaci dat**; odkazujÃ­ na data v jinÃ©m umÃ­stÄ›nÃ­ (internÃ­m i externÃ­m), aniÅ¾ by je kopÃ­rovaly.|
|**Mini-praktickÃ¡ Ãºloha**|NavrhnÄ›te, jak byste zpÅ™Ã­stupnili historickÃ¡ data, kterÃ¡ jsou aktuÃ¡lnÄ› uloÅ¾ena vÂ **Amazon S3**Â (externÃ­ ÃºloÅ¾iÅ¡tÄ›), do VaÅ¡eho Fabric Lakehouse, aniÅ¾ byste je museli pÅ™esouvat (sprÃ¡vnÃ¡ volba jeÂ **Shortcut**).|
|**Exam-style Question**|_A company wants to unify data access without creating multiple copies across departments. Which core Fabric component enables this "single copy of data" principle?_Â (A) Deployment Pipelines, (B) Dataflow Gen2, (C) OneLake, (D) Semantic Model.|
|**Mikro lekce angliÄtiny**|**Core Terms:**Â _Unified Data Lake_,Â _Interoperability_,Â _Data Virtualization_,Â _Replication_.Â **FrÃ¡ze:**Â _Single source of truth_Â (jedinÃ½ zdroj pravdy),Â _Eliminate data silos_Â (odstranit datovÃ¡ sila),Â _Open standard format_Â (otevÅ™enÃ½ standardnÃ­ formÃ¡t).|
|**Souhrn a DomÃ¡cÃ­ procviÄenÃ­ (10 min)**|RozumÃ­te konceptu OneLake a vÃ½hodÃ¡m Delta Parquet formÃ¡tu. VÃ­te, jak pouÅ¾Ã­vatÂ **Shortcuts**Â pro virtualizaci externÃ­ch dat.Â **DomÃ¡cÃ­ Ãºkol:**Â Vyhledejte v dokumentaci, jakÃ© jsou podmÃ­nky proÂ **Shortcut Caching**Â (caching from S3/GCS) a kdy se data z cache automaticky smaÅ¾ou.|

--------------------------------------------------------------------------------

### Den 3: Lakehouse: JÃ¡dro DE

|   |   |
|---|---|
|Sekce|Obsah|
|**Theory Focus (English)**|**Lakehouse Architecture & Usage**Â TheÂ **Lakehouse**Â is a core data store combining the flexibility of a data lake (storing rawÂ **Files**) and the structure of a data warehouse (managedÂ **Tables**Â as Delta Parquet). It is primarily operated using theÂ **Spark engine**Â viaÂ **Notebooks**Â (PySpark, Spark SQL). The Lakehouse provides aÂ **read-only SQL Analytics Endpoint**Â to query data using T-SQL, offering flexibility for BI tools and analysts. The Lakehouse is the most flexible data store, suitable for unstructured, semi-structured, and structured data, making it ideal for the Bronze and Silver layers of a Medallion Architecture.|
|**ÄŒeskÃ© vysvÄ›tlenÃ­ pojmÅ¯**|**Lakehouse**Â je zÃ¡kladnÃ­ ÃºloÅ¾iÅ¡tÄ› pro datovÃ© inÅ¾enÃ½ry, kterÃ© se dÄ›lÃ­ naÂ **Files**Â (pro surovÃ¡, nestrukturovanÃ¡ data) aÂ **Tables**Â (pro Delta tabulky). NativnÃ­ jazyk pro Lakehouse jeÂ **PySpark/Spark SQL**Â (pÅ™es Notebooky). Lze k nÄ›mu pÅ™istupovat i pomocÃ­ T-SQL pÅ™esÂ **SQL Analytics Endpoint**. Pro DP-700 je klÃ­ÄovÃ©, Å¾e Lakehouse nejlÃ©pe podporujeÂ **data science a ML scÃ©nÃ¡Å™e**.|
|**Mini-praktickÃ¡ Ãºloha**|V Notebooku naÄtÄ›te surovÃ½ CSV soubor zÂ **Files**Â oblasti VaÅ¡eho Lakehouse do Spark DataFrame. PÅ™idejte sloupec s aktuÃ¡lnÃ­m Äasem a zapiÅ¡te jej jako novouÂ **Delta tabulku**Â do sekceÂ **Tables**Â (napÅ™. pomocÃ­Â `df.write.format("delta").mode("overwrite").saveAsTable(...)`).|
|**Exam-style Question**|_Which capability is uniquely supported by the Lakehouse data store compared to the Warehouse, making it suitable for landing raw log files?_Â (A) T-SQL Querying, (B) Row-Level Security, (C) Storing unstructured files in the file section, (D) Multi-table transactions.|
|**Mikro lekce angliÄtiny**|**Core Terms:**Â _Raw Data_Â (surovÃ¡ data),Â _Spark Engine_,Â _Delta Table_,Â _SQL Analytics Endpoint_.Â **FrÃ¡ze:**Â _Read-only access_Â (pÅ™Ã­stup pouze pro ÄtenÃ­),Â _Medallion Architecture_Â (medailonovÃ¡ architektura),Â _Unstructured data_Â (nestrukturovanÃ¡ data).|
|**Souhrn a DomÃ¡cÃ­ procviÄenÃ­ (10 min)**|UmÃ­te definovat Lakehouse, znÃ¡te jeho dvÄ› ÄÃ¡sti (Files, Tables) a primÃ¡rnÃ­ jazyk (Spark).Â **DomÃ¡cÃ­ Ãºkol:**Â ZjistÄ›te, jakÃ© Delta Lake operace je nutnÃ© provÃ¡dÄ›t pravidelnÄ› pro optimalizaci Lakehouse tabulek (HINT: Compact files and remove old files).|

--------------------------------------------------------------------------------

### Den 4: Data Warehouse (DW)

|   |   |
|---|---|
|Sekce|Obsah|
|**Theory Focus (English)**|**Warehouse Architecture & T-SQL**Â TheÂ **Data Warehouse**Â is optimized for structured, tabular data and T-SQL transformations. It is backed by the Polaris (SQL) engine and supportsÂ **multi-table transactions**. DW is commonly used for the Gold layer in a Medallion Architecture, providing a robust relational model for BI and advanced security (RLS/CLS). DW supports traditional database objects like tables, views, stored procedures, and schemas. You can ingest data using Data Pipelines, Dataflows, or theÂ **COPY INTO**Â T-SQL command.|
|**ÄŒeskÃ© vysvÄ›tlenÃ­ pojmÅ¯**|**Data Warehouse**Â je postaven na znÃ¡mÃ©m jazyceÂ **T-SQL**Â a je ideÃ¡lnÃ­ pro BI reporting a dimenzionÃ¡lnÃ­ modelovÃ¡nÃ­. KlÃ­ÄovÃ© vÃ½hody:Â **podpora multi-table transakcÃ­**Â (Lakehouse ji nepodporuje) a lepÅ¡Ã­ Git/CI-CD integrace pÅ™esÂ **SQL Database Projects**. Pro DP-700 je dÅ¯leÅ¾itÃ© znÃ¡t pÅ™Ã­kazy jakoÂ **CREATE TABLE AS SELECT (CTAS)**Â aÂ **COPY INTO**Â pro efektivnÃ­ ingest a transformaci.|
|**Mini-praktickÃ¡ Ãºloha**|VytvoÅ™te v DW novÃ½ SQL dotaz. PomocÃ­Â **CTAS**Â naÄtÄ›te data z tabulkyÂ `Sales`Â ve VaÅ¡em Lakehouse (pomocÃ­ cross-database querying, napÅ™.Â `SELECT * FROM LakehouseName.dbo.SalesTable`) do novÃ© tabulkyÂ `DW_Sales_Gold`Â v Data Warehouse.|
|**Exam-style Question**|_Your team requires full T-SQL support, including multi-table transactions and stored procedures, for the final reporting layer. Which Fabric data store should you choose?_Â (A) Eventhouse, (B) Lakehouse, (C) Warehouse, (D) Dataflow Gen2.|
|**Mikro lekce angliÄtiny**|**Core Terms:**Â _T-SQL_,Â _Multi-table Transaction_,Â _Structured Data_,Â _Stored Procedure_,Â _CTAS_.Â **FrÃ¡ze:**Â _Gold Layer_Â (zlatÃ¡ vrstva),Â _Tabular data_Â (tabulkovÃ¡ data),Â _Relational model_Â (relaÄnÃ­ model).|
|**Souhrn a DomÃ¡cÃ­ procviÄenÃ­ (10 min)**|RozumÃ­te rozdÃ­lÅ¯m mezi Lakehouse (Spark, flexibilnÃ­, ML) a Warehouse (T-SQL, transakce, BI).Â **DomÃ¡cÃ­ Ãºkol:**Â Zopakujte si, proÄÂ **Primary Key constraints**Â v Data Warehouse ve Fabricu nejsou vynucovanÃ© (_NOT ENFORCED_) a jakÃ½ je jejich skuteÄnÃ½ ÃºÄel.|

--------------------------------------------------------------------------------

## ğŸš€ FÃ¡ze 2: Pohyb a transformace dat (Dny 5â€“8)

|   |   |   |
|---|---|---|
|Den|TÃ©ma|KlÃ­ÄovÃ© Fabric Itemy|
|**5**|Data Pipelines I|Copy Data, Activities, Triggers (Schedule)|
|**6**|Data Pipelines II|Orchestration, Control Flow, Lookup, ForEach|
|**7**|Dataflows Gen2|Power Query (M), Low-Code Transformation, Staging|
|**8**|RozhodovÃ¡nÃ­ o Ingestu|Pipeline vs. Dataflow vs. Notebook vs. Mirroring|

### Den 5: Data Pipelines I (ZÃ¡klady a Copy Activity)

|   |   |
|---|---|
|Sekce|Obsah|
|**Theory Focus (English)**|**Data Pipelines & Orchestration**Â AÂ **Data Pipeline**Â is the primary tool for orchestration and data movement (ETL/ELT) in Fabric, resembling Azure Data Factory pipelines. Pipelines execute different Fabric items (Notebooks, Dataflows, SQL Scripts) using connectedÂ **Activities**. The key activity for ingestion isÂ **Copy Data**, which supports copying data from over 100 sources (including cloud and on-premises via Data Gateway) to Fabric data stores. Pipelines are commonly triggered by aÂ **Schedule**Â or anÂ **Event**.|
|**ÄŒeskÃ© vysvÄ›tlenÃ­ pojmÅ¯**|**Data Pipeline**Â je orchestrÃ¡tor, kterÃ½ Å™Ã­dÃ­ tok dat a spouÅ¡tÃ­ jinÃ© Fabric aktivity (napÅ™. refresh semantic model). NejdÅ¯leÅ¾itÄ›jÅ¡Ã­ jeÂ **Copy Data**Â aktivita pro hromadnÃ½ pÅ™esun dat, kterÃ¡ podporuje hybridnÃ­ scÃ©nÃ¡Å™e (pÅ™esÂ **on-prem data gateway**). Aktivity se propojujÃ­ pomocÃ­ podmÃ­nek jakoÂ _On Success_,Â _On Fail_,Â _On Completion_. SpuÅ¡tÄ›nÃ­ zajiÅ¡Å¥ujÃ­Â **Triggery**Â (napÅ™. ÄasovÃ½ plÃ¡n).|
|**Mini-praktickÃ¡ Ãºloha**|VytvoÅ™te Data Pipeline. PÅ™idejte aktivituÂ **Copy Data**Â pro naÄtenÃ­ malÃ©ho souboru z externÃ­ho ADLS Gen2 do VaÅ¡eho Lakehouse. PÅ™idejte aktivituÂ **Notebook**Â (Inactive). Propojte Copy Data s Notebookem pomocÃ­ podmÃ­nkyÂ **On Success**.|
|**Exam-style Question**|_A data engineering team needs to move large volumes of batch data from an external Azure SQL Database to a Fabric Lakehouse, followed by a semantic model refresh. Which Fabric item is best suited for this end-to-end orchestration?_Â (A) Dataflow Gen2, (B) Fabric Notebook, (C) Data Pipeline, (D) Eventstream.|
|**Mikro lekce angliÄtiny**|**Core Terms:**Â _Orchestration_,Â _Activity_,Â _Control Flow_,Â _Schedule Trigger_,Â _Copy Data_.Â **FrÃ¡ze:**Â _On Success/On Fail_Â (pÅ™i ÃºspÄ›chu/selhÃ¡nÃ­),Â _Data Movement_Â (pohyb dat),Â _External source_Â (externÃ­ zdroj).|
|**Souhrn a DomÃ¡cÃ­ procviÄenÃ­ (10 min)**|UmÃ­te definovat, co je pipeline, znÃ¡te jejÃ­ klÃ­ÄovÃ© aktivity (Copy Data) a jak propojit kroky.Â **DomÃ¡cÃ­ Ãºkol:**Â ZjistÄ›te, jakÃ½ je rozdÃ­l meziÂ **Pipeline Parameters**Â (pÅ™ijÃ­manÃ© zvenÄÃ­) aÂ **Variables**Â (Å¾ijÃ­ uvnitÅ™ pipeline).|

--------------------------------------------------------------------------------

### Den 6: Data Pipelines II (PokroÄilÃ¡ orchestrace)

|   |   |
|---|---|
|Sekce|Obsah|
|**Theory Focus (English)**|**Advanced Pipeline Control Flow**Â Complex pipelines often utilizeÂ **Control Flow**Â activities likeÂ **Lookup**Â (to retrieve metadata) andÂ **ForEach**Â (to iterate over a list or metadata records) to enableÂ **Metadata-Driven Pipelines**. This pattern allows processing many datasets (e.g., 25 tables) using a single Copy Data activity dynamically. For modularity, theÂ **Parent-Child**Â architecture is used via theÂ **Invoke Pipeline**Â activity, allowing reusable sub-pipelines (Child) to be called from a main pipeline (Parent).|
|**ÄŒeskÃ© vysvÄ›tlenÃ­ pojmÅ¯**|**Metadata-Driven Pipelines**Â umoÅ¾ÅˆujÃ­ automatizovat ingest stovek tabulek bez nutnosti psÃ¡t pro kaÅ¾dou z nich samostatnou logiku. KlÃ­ÄovÃ© aktivity jsouÂ **Lookup**Â (naÄte metadata, napÅ™. seznam tabulek) aÂ **ForEach**Â (iteruje pÅ™es seznam). DynamickÃ© vÃ½razy (Dynamic Expressions) jakoÂ `@item().SourcePath`Â se pouÅ¾Ã­vajÃ­ uvnitÅ™ aktivit pro dynamickÃ© urÄenÃ­ zdroje a cÃ­le.Â **Invoke Pipeline**Â se pouÅ¾Ã­vÃ¡ pro volÃ¡nÃ­ podÅ™azenÃ½ch (Child) pipeline.|
|**Mini-praktickÃ¡ Ãºloha**|NavrhnÄ›te logiku pipeline proÂ **Metadata-Driven Ingest**: 1.Â **Lookup**Â aktivita (simuluje naÄtenÃ­ seznamu tabulek). 2.Â **ForEach**Â aktivita (iteruje seznam). 3. UvnitÅ™ ForEach aktivitaÂ **Copy Data**Â (pouÅ¾Ã­vÃ¡ dynamickÃ© vÃ½razy zÂ `@item()`).|
|**Exam-style Question**|_You need to process 50 database tables using a single Data Pipeline. Which set of activities must you combine to achieve a metadata-driven ingestion pattern?_Â (A) Copy Data and Notebook, (B) Dataflow Gen2 and ForEach, (C) Lookup and ForEach, (D) Invoke Pipeline and Script.|
|**Mikro lekce angliÄtiny**|**Core Terms:**Â _Metadata-Driven_,Â _Lookup Activity_,Â _ForEach Loop_,Â _Invoke Pipeline_,Â _Dynamic Expression_.Â **FrÃ¡ze:**Â _Iterate over a list_Â (iterovat pÅ™es seznam),Â _Reusable component_Â (znovupouÅ¾itelnÃ¡ komponenta),Â _Parent-Child pattern_Â (vzor Parent-Child).|
|**Souhrn a DomÃ¡cÃ­ procviÄenÃ­ (10 min)**|ZvlÃ¡dÃ¡te pokroÄilou orchestraci pomocÃ­ Lookup, ForEach a Invoke Pipeline, a rozumÃ­te, jak navrhnout metadata-driven Å™eÅ¡enÃ­.Â **DomÃ¡cÃ­ Ãºkol:**Â ProjdÄ›te si, jakÃ© aktivity se pouÅ¾Ã­vajÃ­ proÂ **Notifikace/Error Handling**Â v pipeline (napÅ™. Office 365, Teams) a za jakÃ½ch podmÃ­nek se spouÅ¡tÃ­.|

--------------------------------------------------------------------------------

### Den 7: Dataflows Gen2

|   |   |
|---|---|
|Sekce|Obsah|
|**Theory Focus (English)**|**Dataflows Gen2 (DFG2) & Low-Code ETL**Â **Dataflows Gen2**Â is aÂ **low-code/no-code**Â ETL tool powered by theÂ **Power Query (M language)**Â engine. It offers over 300 built-in connectors and is excellent for business analysts or teams preferring a visual transformation interface. DFG2 supports bothÂ **append and replace**Â methods for updates, though support may vary by destination. A key feature isÂ **Fast Copy**, which leverages the high-throughput Copy Data infrastructure for ingestion. DFG2 is often used as an activity within a Data Pipeline for better orchestration and monitoring.|
|**ÄŒeskÃ© vysvÄ›tlenÃ­ pojmÅ¯**|**Dataflows Gen2**Â je nÃ­zko-kÃ³dovÃ½ ETL nÃ¡stroj, kterÃ½ pouÅ¾Ã­vÃ¡ rozhranÃ­ Power Query (M jazyk). Je ideÃ¡lnÃ­ pro rychlÃ©Â **transformace s minimem kÃ³du**. Lze ho pouÅ¾Ã­t proÂ **on-premise zdroje**Â pomocÃ­ Data Gateway.Â **Fast Copy**Â umoÅ¾Åˆuje rychlÃ½ zÃ¡pis do cÃ­lÅ¯ (Lakehouse/Warehouse) s vyuÅ¾itÃ­m Copy Data enginu. Pro DP-700 je klÃ­ÄovÃ©, Å¾e DFG2 se hodÃ­ pro scÃ©nÃ¡Å™e, kterÃ©Â **minimalizujÃ­ vÃ½vojovÃ© ÃºsilÃ­**Â (_minimize development effort_).|
|**Mini-praktickÃ¡ Ãºloha**|VytvoÅ™te Dataflow Gen2. PouÅ¾ijte Power Query k naÄtenÃ­ dat z veÅ™ejnÃ©ho CSV zdroje. PÅ™idejte transformaÄnÃ­ krok, napÅ™.Â **Group By**Â neboÂ **Convert Text to Upper Case**. Nastavte cÃ­lovou destinaci (Destination) do VaÅ¡eho Lakehouse.|
|**Exam-style Question**|_Which Fabric tool allows data ingestion from over 300 connectors, supports on-premises sources via Data Gateway, and minimizes coding effort, making it ideal for business analysts?_Â (A) Fabric Notebook, (B) Data Pipeline, (C) Dataflow Gen2, (D) KQL Queryset.|
|**Mikro lekce angliÄtiny**|**Core Terms:**Â _Low-Code/No-Code_,Â _Power Query_,Â _M language_,Â _On-Premise Sources_,Â _Data Gateway_.Â **FrÃ¡ze:**Â _Visual transformation interface_Â (vizuÃ¡lnÃ­ transformaÄnÃ­ rozhranÃ­),Â _Minimize development effort_Â (minimalizovat vÃ½vojovÃ© ÃºsilÃ­).|
|**Souhrn a DomÃ¡cÃ­ procviÄenÃ­ (10 min)**|RozumÃ­te, kdy pouÅ¾Ã­t DFG2 (low-code, on-prem, rychlÃ© transformace).Â **DomÃ¡cÃ­ Ãºkol:**Â PopiÅ¡te, jak byste orchestraci pro DFG2 Å™eÅ¡ili efektivnÄ›ji neÅ¾ jen manuÃ¡lnÃ­m spouÅ¡tÄ›nÃ­m (HINT: VloÅ¾enÃ­ DFG2 jako aktivity do Data Pipeline).|

--------------------------------------------------------------------------------

### Den 8: RozhodovÃ¡nÃ­ o Ingestu

|   |   |
|---|---|
|Sekce|Obsah|
|**Theory Focus (English)**|**Ingestion Decision Guide**Â Choosing the right tool depends on complexity, volume, required skillset, and latency.Â **Data Pipeline**Â is for orchestration and movement (batch).Â **Dataflow Gen2**Â is for low-code ETL (batch, on-prem).Â **Notebooks (Spark)**Â are for complex, custom transformations, large volume, and custom API integration (full code).Â **Shortcuts**Â andÂ **Database Mirroring**Â are for data virtualization, minimizing ETL effort entirely.Â **Mirroring**Â provides a near real-time replica of supported databases (Azure SQL, Snowflake) in Delta Parquet format.|
|**ÄŒeskÃ© vysvÄ›tlenÃ­ pojmÅ¯**|KlÃ­ÄovÃ© kritÃ©rium:Â **Skillset**Â (SQL, PySpark/Python, M/Low-code) aÂ **Komplexita transformace**. FrÃ¡zeÂ _â€minimalizovat vÃ½vojovÃ© ÃºsilÃ­â€œ_Â obvykle smÄ›Å™uje k low-code nÃ¡strojÅ¯m (Dataflow, Pipeline) nebo virtualizaci (Shortcut, Mirroring).Â **Mirroring**Â se hodÃ­ proÂ _near real-time_Â replikaci podporovanÃ½ch OLTP databÃ¡zÃ­ do Fabricu, ÄÃ­mÅ¾ eliminuje potÅ™ebu psÃ¡t ETL procesy.|
|**Mini-praktickÃ¡ Ãºloha**|**ScÃ©nÃ¡Å™:**Â TÃ½m mÃ¡ silnÃ© Python dovednosti a potÅ™ebuje provÃ©st komplexnÃ­ validaci schÃ©matu pÅ™Ã­chozÃ­ch dat. JakÃ½ nÃ¡stroj zvolÃ­te? (OdpovÄ›Ä:Â **Notebook**Â s Python knihovnami jako Great Expectations).|
|**Exam-style Question**|_A company wants to ingest data from an on-premises SQL Server and needs to apply moderate transformations using a low-code approach. Which tool is the appropriate choice?_Â (A) Fabric Notebook, (B) Database Mirroring, (C) Data Pipeline with Copy Data, (D) Dataflow Gen2.|
|**Mikro lekce angliÄtiny**|**Core Terms:**Â _Mirroring_,Â _Latency_,Â _Skillset_,Â _Custom Transformation_,Â _Near Real-time_.Â **FrÃ¡ze:**Â _Data Replication_Â (replikace dat),Â _Full code solution_Â (Å™eÅ¡enÃ­ zaloÅ¾enÃ© na plnÃ©m kÃ³du),Â _Decision guide_Â (rozhodovacÃ­ prÅ¯vodce).|
|**Souhrn a DomÃ¡cÃ­ procviÄenÃ­ (10 min)**|Jste schopni vybrat sprÃ¡vnÃ½ nÃ¡stroj pro ingest dat do Fabric na zÃ¡kladÄ› kritÃ©riÃ­ (kÃ³d, sloÅ¾itost, zdroj, rychlost).Â **DomÃ¡cÃ­ Ãºkol:**Â Porovnejte, jak se liÅ¡Ã­ pÅ™Ã­stup k datÅ¯m uÂ **Shortcut**Â (virtuÃ¡lnÃ­ odkaz) aÂ **Mirroring**Â (near real-time replika).|

--------------------------------------------------------------------------------

## ğŸš€ FÃ¡ze 3: Spark, PySpark a Modely (Dny 9â€“12)

|   |   |   |
|---|---|---|
|Den|TÃ©ma|KlÃ­ÄovÃ© Fabric Itemy|
|**9**|Notebooks a Spark Basics|Notebooks, Magic Commands, notebookutils|
|**10**|Spark konfigurace a optimalizace|Starter Pool, Custom Pool, High Concurrency|
|**11**|Loading Patterns & PySpark|Full Load, Incremental Load, MERGE|
|**12**|DimenzionÃ¡lnÃ­ modelovÃ¡nÃ­|Star Schema, SCD Type 2, Surrogate Keys|

### Den 9: Notebooks a Spark Basics

|   |   |
|---|---|
|Sekce|Obsah|
|**Theory Focus (English)**|**Notebooks and PySpark**Â **Notebooks**Â are code-based tools, the "Swiss army knife" of Data Engineering, primarily using the Spark engine. They support multiple languages per cell usingÂ **Magic Commands**Â (e.g.,Â `%%sql`Â for Spark SQL,Â `%%pyspark`).Â **Notebooks can be parameterized**Â to accept external values when executed via Pipeline orÂ `notebookutils`. TheÂ **notebookutils**Â library is essential for common tasks like orchestrating other notebooks (`notebook.run_multiple`), reading secrets, and file system operations.|
|**ÄŒeskÃ© vysvÄ›tlenÃ­ pojmÅ¯**|**Notebooky**Â jsou nejuniverzÃ¡lnÄ›jÅ¡Ã­ nÃ¡stroj, kde pÃ­Å¡ete kÃ³d (primÃ¡rnÄ›Â **PySpark**Â pro prÃ¡ci s daty ve Sparku).Â **Magic Commands**Â (napÅ™.Â `%%sql`) VÃ¡m umoÅ¾nÃ­ pÅ™epÃ­nat jazyk v rÃ¡mci jednÃ© buÅˆky.Â **Parametrizace**Â buÅˆky (Parameter Cell) je klÃ­ÄovÃ¡ pro dynamickÃ© spouÅ¡tÄ›nÃ­ kÃ³du. KnihovnaÂ **notebookutils**Â je dÅ¯leÅ¾itÃ¡ pro komunikaci s Fabric (napÅ™. spouÅ¡tÄ›nÃ­ jinÃ½ch notebookÅ¯ nebo sprÃ¡va souborÅ¯).|
|**Mini-praktickÃ¡ Ãºloha**|V Notebooku naÄtÄ›te DataFrame s daty. V jednÃ© buÅˆce pouÅ¾ijteÂ **%%sql**Â **magic command**Â pro provedenÃ­ jednoduchÃ©hoÂ `SELECT COUNT(*)`. V dalÅ¡Ã­ buÅˆce pouÅ¾ijteÂ **PySpark**Â k vykonÃ¡nÃ­ tÃ©Å¾e operace a porovnejte vÃ½sledky.|
|**Exam-style Question**|_Which built-in Python library in Fabric Notebooks is designed to streamline common tasks such as accessing secrets from Azure Key Vault and orchestrating other notebooks?_Â (A) Pandas, (B) MLFlow, (C) notebookutils, (D) Semantic Link.|
|**Mikro lekce angliÄtiny**|**Core Terms:**Â _Parameter Cell_,Â _Magic Command_,Â _PySpark_,Â _DataFrame_,Â _Exit Value_.Â **FrÃ¡ze:**Â _Streamline common tasks_Â (zefektivnit bÄ›Å¾nÃ© Ãºkoly),Â _Execution context_Â (kontext spuÅ¡tÄ›nÃ­),Â _Run multiple_Â (spustit vÃ­ce).|
|**Souhrn a DomÃ¡cÃ­ procviÄenÃ­ (10 min)**|RozumÃ­te NotebookÅ¯m, jak parametrizovat kÃ³d a znÃ¡te zÃ¡kladnÃ­ funkceÂ `notebookutils`.Â **DomÃ¡cÃ­ Ãºkol:**Â Nastudujte si rozdÃ­l mezi spuÅ¡tÄ›nÃ­m notebooku pomocÃ­Â **%run**Â (stejnÃ½ kontext) aÂ **notebookutils.notebook.run(...)**Â (jinÃ½ kontext).|

--------------------------------------------------------------------------------

### Den 10: Spark konfigurace a optimalizace

|   |   |
|---|---|
|Sekce|Obsah|
|**Theory Focus (English)**|**Spark Configuration and Pools**Â Fabric provides a defaultÂ **Starter Pool**Â for fast startup (under 10 seconds), which is sufficient for most smaller/interactive workloads. For demanding jobs, you must configure aÂ **Custom Spark Pool**Â (allowing changes in node size, autoscale, executor range), although this increases startup time. The settingÂ **High Concurrency**Â allows multiple notebooks or jobs running concurrently for one user/pipeline to share the same underlying Spark session, optimizing resource usage.Â **Environments**Â are Fabric assets used to manage Spark configurations and install custom Python libraries.|
|**ÄŒeskÃ© vysvÄ›tlenÃ­ pojmÅ¯**|**Starter Pool**Â (vÃ½chozÃ­) je rychlÃ½, ale omezenÃ½ pro velkÃ© objemy dat a konkurenci. Pro nÃ¡roÄnÃ© Ãºlohy se pouÅ¾Ã­vÃ¡Â **Custom Spark Pool**, kde mÅ¯Å¾ete definovat velikost uzlÅ¯ (nodes) a alokaci jader.Â **High Concurrency**Â je klÃ­ÄovÃ© nastavenÃ­, kterÃ© umoÅ¾Åˆuje sdÃ­lenÃ­ jednÃ© Spark Session mezi vÃ­ce Notebooky (napÅ™. spuÅ¡tÄ›nÃ½mi z jednÃ© Pipeline).Â **Environments**Â se pouÅ¾Ã­vajÃ­ k verzovÃ¡nÃ­ Spark nastavenÃ­ a Python knihoven.|
|**Mini-praktickÃ¡ Ãºloha**|Identifikujte, kde veÂ **Workspace Settings**Â se spravujÃ­Â **Custom Spark Pool**Â aÂ **Environments**. Nastudujte, jakÃ© parametry mÅ¯Å¾ete u custom poolu mÄ›nit (napÅ™.Â _node size_,Â _autoscale_).|
|**Exam-style Question**|_A production data solution experiences performance bottlenecks due to long Spark cluster startup times for batch jobs. The jobs are large and complex. What feature should the engineer configure to optimize resources while accommodating the high load?_Â (A) Enable High Concurrency on the Starter Pool, (B) Create a Custom Spark Pool with optimized node settings, (C) Disable the Autoscale feature, (D) Use notebookutils for parallel execution.|
|**Mikro lekce angliÄtiny**|**Core Terms:**Â _Spark Pool_,Â _Node Size_,Â _Autoscale_,Â _Executor_,Â _High Concurrency_.Â **FrÃ¡ze:**Â _Customization of Spark settings_Â (pÅ™izpÅ¯sobenÃ­ nastavenÃ­ Sparku),Â _Dynamic allocation_Â (dynamickÃ¡ alokace),Â _Share the same session_Â (sdÃ­let stejnou relaci).|
|**Souhrn a DomÃ¡cÃ­ procviÄenÃ­ (10 min)**|ZnÃ¡te rozdÃ­ly mezi Spark Pooly a vÃ­te, co jeÂ **High Concurrency**Â a k Äemu slouÅ¾Ã­Â **Environments**.Â **DomÃ¡cÃ­ Ãºkol:**Â ZjistÄ›te, jakÃ½ je hlavnÃ­ dopadÂ **Custom Spark Pool**Â na Äas spouÅ¡tÄ›nÃ­ (HINT: Je delÅ¡Ã­ neÅ¾ u Starter Poolu).|

--------------------------------------------------------------------------------

### Den 11: Loading Patterns & PySpark

|   |   |
|---|---|
|Sekce|Obsah|
|**Theory Focus (English)**|**Loading Patterns: Full vs Incremental**Â Synchronization between source and Fabric can be achieved viaÂ **Full Load**Â (overwrite all data, simple but costly for large tables) orÂ **Incremental Load**Â (only load changed data, faster but complex implementation).Â **Incremental Loading**Â in Spark/PySpark is typically implemented using theÂ **Delta Lake MERGE**Â command, which handles inserts, updates, and deletes (UPSERTs) based on key matching. This requires tracking aÂ **watermark**Â (e.g., a timestamp or sequence number) from the source.|
|**ÄŒeskÃ© vysvÄ›tlenÃ­ pojmÅ¯**|**Full Load**Â (plnÃ© naÄtenÃ­) pÅ™epÃ­Å¡e cÃ­lovou tabulku pokaÅ¾dÃ© (napÅ™.Â `mode("overwrite")`Â v PySpark).Â **Incremental Load**Â (pÅ™Ã­rÅ¯stkovÃ© naÄtenÃ­) je efektivnÄ›jÅ¡Ã­, vyÅ¾aduje ale sledovÃ¡nÃ­Â _watermarku_Â (poslednÃ­ zpracovanÃ½ Äas/ID). Pro implementaci pÅ™Ã­rÅ¯stkovÃ©ho naÄÃ­tÃ¡nÃ­ v Lakehouse je klÃ­ÄovÃ½ pÅ™Ã­kazÂ **Delta MERGE INTO**Â (obdoba UPSERT).|
|**Mini-praktickÃ¡ Ãºloha**|NapiÅ¡te kÃ³d pro jednoduchÃ½Â **Full Load**Â (overwrite) v PySpark: naÄtÄ›te data do DataFrame a zapiÅ¡te je do Lakehouse s reÅ¾imemÂ **mode("overwrite")**.|
|**Exam-style Question**|_A Data Engineer needs to efficiently synchronize a large source table with a Delta table in Lakehouse, ensuring that new, updated, and deleted rows are handled in a single operation. Which Delta Lake command provides this functionality?_Â (A) COPY INTO, (B) UNION ALL, (C) MERGE INTO, (D) VACUUM.|
|**Mikro lekce angliÄtiny**|**Core Terms:**Â _Incremental Load_,Â _Full Load_,Â _Watermark_,Â _Merge Into_,Â _UPSERT_.Â **FrÃ¡ze:**Â _Data synchronization_Â (synchronizace dat),Â _Handle inserts, updates, and deletes_Â (zpracovat vloÅ¾enÃ­, aktualizace a mazÃ¡nÃ­).|
|**Souhrn a DomÃ¡cÃ­ procviÄenÃ­ (10 min)**|RozumÃ­te rozdÃ­lu mezi full a incremental loading a vÃ­te, Å¾e MERGE INTO je klÃ­ÄovÃ½ pro UPSERT operace v Lakehouse.Â **DomÃ¡cÃ­ Ãºkol:**Â ZjistÄ›te, jak se v T-SQL ve Warehouse provÃ¡dÃ­ operace, kterÃ¡ odpovÃ­dÃ¡ PySparkÂ `MERGE INTO`.|

--------------------------------------------------------------------------------

### Den 12: DimenzionÃ¡lnÃ­ modelovÃ¡nÃ­

|   |   |
|---|---|
|Sekce|Obsah|
|**Theory Focus (English)**|**Dimensional Modeling and SCD Type 2**Â DP-700 requires knowledge of dimensional modeling concepts (Star Schema, Fact and Dimension tables).Â **Surrogate Keys**Â (system-generated integers) are used in dimension tables instead ofÂ **Natural Keys**Â (source IDs) to support historical tracking and protect against source system changes.Â **Slowly Changing Dimensions (SCD)**Â describe how to handle changes in dimension attributes.Â **SCD Type 2**Â tracks full history by creating a new row for every change, usingÂ `ValidFrom`Â andÂ `ValidTo`Â timestamps or anÂ `IsCurrent`Â flag.|
|**ÄŒeskÃ© vysvÄ›tlenÃ­ pojmÅ¯**|**DimenzionÃ¡lnÃ­ modelovÃ¡nÃ­**Â (Star Schema) je klÃ­ÄovÃ© pro analytiku.Â **Surrogate Keys**Â (nÃ¡hradnÃ­ klÃ­Äe) se pouÅ¾Ã­vajÃ­ pro zajiÅ¡tÄ›nÃ­ unikÃ¡tnosti Å™Ã¡dkÅ¯ a sledovÃ¡nÃ­ historie.Â **SCD Type 2**Â (pomalu se mÄ›nÃ­cÃ­ dimenze typu 2) se pouÅ¾Ã­vÃ¡, kdyÅ¾ potÅ™ebujemeÂ **zachovat celou historii**Â zmÄ›n atributu (napÅ™. adresa zÃ¡kaznÃ­ka). NovÃ½ Å™Ã¡dek se vytvoÅ™Ã­, starÃ½ se "uzavÅ™e" (ValidTo).Â **SCD Type 3**Â sleduje jenÂ **poslednÃ­ zmÄ›nu**Â (napÅ™.Â `PreviousCountry`).|
|**Mini-praktickÃ¡ Ãºloha**|NavrhnÄ›te schÃ©ma pro dimenziÂ `DimCustomer`Â (SCD Type 2). KterÃ© sloupce musÃ­ obsahovat pro sledovÃ¡nÃ­ historie? (HINT:Â `SurrogateKey`,Â `NaturalKey`,Â `ValidFrom`,Â `ValidTo`/`IsCurrent`).|
|**Exam-style Question**|_A dimension table tracks customer location changes. The requirement is to maintain a full history of all past and current addresses for accurate historical reporting. Which type of Slowly Changing Dimension (SCD) should be implemented?_Â (A) SCD Type 0, (B) SCD Type 1, (C) SCD Type 2, (D) SCD Type 3.|
|**Mikro lekce angliÄtiny**|**Core Terms:**Â _Dimensional Model_,Â _Star Schema_,Â _Fact Table_,Â _Dimension Table_,Â _Surrogate Key_,Â _Natural Key_,Â _SCD Type 2_.Â **FrÃ¡ze:**Â _Maintain full history_Â (zachovat celou historii),Â _Temporal analysis_Â (ÄasovÃ¡ analÃ½za),Â _Data Integrity_Â (datovÃ¡ integrita).|
|**Souhrn a DomÃ¡cÃ­ procviÄenÃ­ (10 min)**|RozumÃ­te dimenzionÃ¡lnÃ­m zÃ¡kladÅ¯m a vÃ­te, jak implementovat (konceptuÃ¡lnÄ›) SCD Type 2 pro sledovÃ¡nÃ­ historie.Â **DomÃ¡cÃ­ Ãºkol:**Â ZjistÄ›te, jakÃ½m zpÅ¯sobem se ve Fabric DW generujÃ­ Surrogate Keys, kdyÅ¾ DW nepodporujeÂ _Identity_Â sloupce.|

--------------------------------------------------------------------------------

## ğŸš€ FÃ¡ze 4: Real-Time a Security (Dny 13â€“16)

|   |   |   |
|---|---|---|
|Den|TÃ©ma|KlÃ­ÄovÃ© Fabric Itemy|
|**13**|Real-Time Analytics I|Eventstreams, Sources, Destinations|
|**14**|Real-Time Analytics II|KQL Database, KQL Basics, Materialized Views|
|**15**|Security I|Workspace/Item Roles, Dynamic Data Masking (DDM)|
|**16**|Security II|RLS, OLS, CLS (T-SQL vs OneSecurity)|

### Den 13: Real-Time Analytics I (Eventstreams)

|   |   |
|---|---|
|Sekce|Obsah|
|**Theory Focus (English)**|**Eventstreams and Streaming Data**Â **Real-time systems**Â process data continuously with low latency, suitable for telemetry, logs, and IoT.Â **Eventstreams**Â is the no-code/low-code tool for processing and routing streaming data in Fabric. It supports sources like Azure Event Hubs/IoT Hub and custom HTTP endpoints. Key operations includeÂ **Filtering**,Â **Aggregating**Â overÂ **Time Windows**Â (Tumbling, Hopping, Sliding), and directing data to destinations likeÂ **Eventhouse (KQL DB)**,Â **Lakehouse**, orÂ **Data Activator**.Â **Spark Structured Streaming**Â is the code-based alternative for complex streaming transformations.|
|**ÄŒeskÃ© vysvÄ›tlenÃ­ pojmÅ¯**|**Eventstreams**Â je vizuÃ¡lnÃ­ nÃ¡stroj, kterÃ½ pomÃ¡hÃ¡ s ingestem, lehkou transformacÃ­ a pÅ™esmÄ›rovÃ¡nÃ­m streamovanÃ½ch dat. MÅ¯Å¾ete definovatÂ **ÄasovÃ¡ okna**Â (Time Windows) pro agregace:Â **Tumbling**Â (nepÅ™ekrÃ½vajÃ­cÃ­ se),Â **Hopping**Â (pÅ™ekrÃ½vajÃ­cÃ­ se). Data se typicky uklÃ¡dajÃ­ doÂ **Eventhouse (KQL DB)**Â pro real-time analÃ½zu. Pro komplexnÃ­, kÃ³dovÃ© transformace se pouÅ¾Ã­vÃ¡Â **Spark Structured Streaming**.|
|**Mini-praktickÃ¡ Ãºloha**|**ScÃ©nÃ¡Å™:**Â PotÅ™ebujete vypoÄÃ­tat prÅ¯mÄ›rnou teplotu senzoru kaÅ¾dou celou minutu. JakÃ½ typÂ **Time Window**Â pouÅ¾ijete pro tuto pevnou, nepÅ™ekrÃ½vajÃ­cÃ­ se agregaci? (OdpovÄ›Ä:Â **Tumbling Window**).|
|**Exam-style Question**|_A solution requires low-code ingestion and transformation of telemetry data coming from Azure Event Hubs, with the goal of writing aggregated results to a KQL Database. Which Fabric item should be used?_Â (A) Data Pipeline with Notebook Activity, (B) Dataflow Gen2, (C) Eventstreams, (D) Spark Structured Streaming.|
|**Mikro lekce angliÄtiny**|**Core Terms:**Â _Streaming Data_,Â _Latency_,Â _Tumbling Window_,Â _Hopping Window_,Â _Event Hubs_,Â _Time-Series_.Â **FrÃ¡ze:**Â _Real-time analytics_Â (analÃ½za v reÃ¡lnÃ©m Äase),Â _Source/Destination_Â (zdroj/cÃ­l),Â _Unbounded table_Â (neomezenÃ¡ tabulka - koncept Spark Streaming).|
|**Souhrn a DomÃ¡cÃ­ procviÄenÃ­ (10 min)**|RozumÃ­te real-time scÃ©nÃ¡Å™Å¯m, roli Eventstreams a typÅ¯m ÄasovÃ½ch oken.Â **DomÃ¡cÃ­ Ãºkol:**Â ZjistÄ›te, kdy je vhodnÃ© zvolitÂ **Spark Structured Streaming**Â namÃ­sto Eventstreams (HINT: KdyÅ¾ je vyÅ¾adovÃ¡na velkÃ¡ komplexita transformacÃ­ a kÃ³d).|

--------------------------------------------------------------------------------

### Den 14: Real-Time Analytics II (KQL Database a KQL)

|   |   |
|---|---|
|Sekce|Obsah|
|**Theory Focus (English)**|**KQL Database and KQL Basics**Â **Eventhouse**Â is the container forÂ **KQL Databases**, which are optimized for log and time-series data using the Kusto Query Language (KQL). The KQL database manages data throughÂ **Retention**Â (how long data is kept) andÂ **Caching**Â policies (hot vs. cold data).Â **KQL**Â is the read-only query language used for fetching, filtering, and analyzing this data. Key KQL operators includeÂ `take`,Â `project`Â (select columns),Â `where`Â (filter), andÂ `summarize`Â (aggregation, similar to SQLÂ `GROUP BY`).Â **Materialized Views**Â store pre-computed aggregation results for faster reporting.|
|**ÄŒeskÃ© vysvÄ›tlenÃ­ pojmÅ¯**|**Eventhouse**Â je kontejner proÂ **KQL DatabÃ¡ze**. KQL DB je ideÃ¡lnÃ­ pro data, kde zÃ¡leÅ¾Ã­ na Äase (time-series) a rychle se mÄ›nÃ­.Â **KQL (Kusto Query Language)**Â je ÄtecÃ­ jazyk pro dotazovÃ¡nÃ­ tÄ›chto dat; dotazy se Å™etÄ›zÃ­ pomocÃ­ operÃ¡toru `|
|**Mini-praktickÃ¡ Ãºloha**|NapiÅ¡te jednoduchÃ½ KQL dotaz pro fiktivnÃ­ tabulkuÂ `SensorLogs`: Chcete zobrazit prvnÃ­ch 10 Å™Ã¡dkÅ¯ a pouze sloupceÂ `SensorID`Â aÂ `Temperature`. (HINT: `SensorLogs|
|**Exam-style Question**|_A KQL database table contains millions of log entries. You need to create a pre-computed aggregation to speed up a real-time dashboard query that frequently calculates the daily average temperature. Which KQL feature should you use?_Â (A) KQL Function, (B) Materialized View, (C) Update Policy, (D) Retention Policy.|
|**Mikro lekce angliÄtiny**|**Core Terms:**Â _Kusto Query Language (KQL)_,Â _Eventhouse_,Â _Retention Policy_,Â _Caching Policy_,Â _Materialized View_.Â **FrÃ¡ze:**Â _Read-only request language_Â (jazyk pro dotazy pouze pro ÄtenÃ­),Â _Time-series data_Â (data ÄasovÃ½ch Å™ad),Â _Pre-computed results_Â (pÅ™edpoÄÃ­tanÃ© vÃ½sledky).|
|**Souhrn a DomÃ¡cÃ­ procviÄenÃ­ (10 min)**|ChÃ¡pete strukturu Eventhouse, znÃ¡te zÃ¡kladnÃ­ KQL operÃ¡tory (`project`,Â `summarize`,Â `take`) a vÃ­te, proÄ se pouÅ¾Ã­vajÃ­ Materialized Views.Â **DomÃ¡cÃ­ Ãºkol:**Â ZjistÄ›te, jakÃ© jsouÂ **hlavnÃ­ limity**Â T-SQL pÅ™i dotazovÃ¡nÃ­ KQL databÃ¡ze ve srovnÃ¡nÃ­ s KQL.|

--------------------------------------------------------------------------------

### Den 15: Security I (ZÃ¡kladnÃ­ pÅ™Ã­stupy a maskovÃ¡nÃ­)

|   |   |
|---|---|
|Sekce|Obsah|
|**Theory Focus (English)**|**Workspace and Item-Level Access**Â Access control uses theÂ **Principle of Least Privilege**.Â **Workspace roles**Â (Admin, Member, Contributor, Viewer) control broad permissions over all items in the workspace.Â **Viewer**Â role can read data via SQL endpoints but cannot run pipelines.Â **Item-level access**Â grants permissions only to a specific item (e.g., one Data Warehouse) without granting access to the whole workspace.Â **Dynamic Data Masking (DDM)**Â hides sensitive information (e.g., salaries, emails) at the query result level for non-privileged users, without changing the underlying data.|
|**ÄŒeskÃ© vysvÄ›tlenÃ­ pojmÅ¯**|CÃ­lem je vÅ¾dy pouÅ¾Ã­tÂ **nejmenÅ¡Ã­ nutnÃ¡ prÃ¡va**Â (Principle of Least Privilege). PrÃ¡va se definujÃ­ buÄ na ÃºrovniÂ **Workspace**Â (platÃ­ pro vÅ¡e uvnitÅ™), nebo na ÃºrovniÂ **Item**Â (pouze pro konkrÃ©tnÃ­ objekt).Â **Viewer**Â role umoÅ¾Åˆuje pouze ÄtenÃ­ dat.Â **Dynamic Data Masking (DDM)**Â maskuje citlivÃ¡ data (napÅ™. email) pÅ™i zobrazenÃ­ (napÅ™.Â `MASKED WITH (FUNCTION = 'email()')`), ale nejednÃ¡ se o plnohodnotnÃ© bezpeÄnostnÃ­ opatÅ™enÃ­, protoÅ¾e jej lze obejÃ­t.|
|**Mini-praktickÃ¡ Ãºloha**|**ScÃ©nÃ¡Å™:**Â UÅ¾ivatel potÅ™ebuje vidÄ›t necitlivÃ¡ data z tabulkyÂ `Employee`Â ve Warehouse, ale nechcete, aby vidÄ›l e-mailovÃ© adresy. NavrhnÄ›te DDM masku pro sloupecÂ `Email`.|
|**Exam-style Question**|_You need to prevent non-administrative users from viewing full email addresses in a financial table, but the original data must remain unencrypted. Which security feature should you implement in the Data Warehouse?_Â (A) Row-Level Security, (B) Column-Level Security, (C) Dynamic Data Masking, (D) Object-Level Security.|
|**Mikro lekce angliÄtiny**|**Core Terms:**Â _Least Privilege_,Â _Access Control_,Â _Viewer Role_,Â _Dynamic Data Masking_,Â _Sensitive Information_.Â **FrÃ¡ze:**Â _Hide sensitive data_Â (skrÃ½t citlivÃ¡ data),Â _Query result level_Â (na Ãºrovni vÃ½sledku dotazu),Â _Unencrypted data_Â (neÅ¡ifrovanÃ¡ data).|
|**Souhrn a DomÃ¡cÃ­ procviÄenÃ­ (10 min)**|ZnÃ¡te role ve Workspace a umÃ­te definovat DDM.Â **DomÃ¡cÃ­ Ãºkol:**Â ZjistÄ›te, jakÃ© jsou ÄtyÅ™i podporovanÃ© maskovacÃ­ funkce pro DDM v T-SQL (HINT: default, email, random, partial).|

--------------------------------------------------------------------------------

### Den 16: Security II (RLS, OLS, CLS)

|   |   |
|---|---|
|Sekce|Obsah|
|**Theory Focus (English)**|**Granular Access Control**Â **Row-Level Security (RLS)**Â restricts data access based on user identity or role (e.g., a sales rep only sees their own sales records). In T-SQL (Warehouse/SQL Endpoint), RLS is implemented using aÂ **Table-Valued Function (TVF)**Â and aÂ **Security Policy**.Â **Object-Level Security (OLS)**Â controls access to entire tables or objects viaÂ `GRANT SELECT`.Â **Column-Level Security (CLS)**Â controls access to specific columns viaÂ `GRANT SELECT (Column1, Column2)`. Fabric is moving towards a unifiedÂ **OneSecurity**Â model to apply RLS/CLS across all engines (Spark, SQL).|
|**ÄŒeskÃ© vysvÄ›tlenÃ­ pojmÅ¯**|**RLS (ZabezpeÄenÃ­ na Ãºrovni Å™Ã¡dkÅ¯)**Â filtruje Å™Ã¡dky podle toho, kdo se dotazuje (napÅ™. pomocÃ­Â `USER_NAME()`).Â **OLS/CLS (ObjektovÃ©/SloupcovÃ© zabezpeÄenÃ­)**Â se aplikuje pomocÃ­ pÅ™Ã­kazuÂ **GRANT SELECT**Â na konkrÃ©tnÃ­ tabulky nebo sloupce. U Warehouse/SQL Endpointu se RLS implementuje pomocÃ­ T-SQL (TVF + Security Policy). DP-700 klade dÅ¯raz na implementaci RLS/CLS, kterÃ© jsou nejÄastÄ›ji nutnÃ© v Gold vrstvÄ›.|
|**Mini-praktickÃ¡ Ãºloha**|**ScÃ©nÃ¡Å™:**Â PotÅ™ebujete, aby se uÅ¾ivatelÅ¯m skupinyÂ `Finance`Â zobrazily pouze sloupceÂ `InvoiceID`Â aÂ `Amount`Â z tabulkyÂ `FactOrders`. NapiÅ¡te zjednoduÅ¡enÃ½ pÅ™Ã­kaz CLS. (OdpovÄ›Ä:Â `GRANT SELECT (InvoiceID, Amount) ON FactOrders TO Finance;`).|
|**Exam-style Question**|_A team wants to ensure that users in the Sales group can only view the sales orders where the 'SalesRep' column matches their own username. Which feature should be implemented?_Â (A) Dynamic Data Masking, (B) Object-Level Security, (C) Column-Level Security, (D) Row-Level Security.|
|**Mikro lekce angliÄtiny**|**Core Terms:**Â _Row-Level Security (RLS)_,Â _Column-Level Security (CLS)_,Â _Object-Level Security (OLS)_,Â _Table-Valued Function (TVF)_,Â _Security Policy_.Â **FrÃ¡ze:**Â _Granular access control_Â (detailnÃ­ Å™Ã­zenÃ­ pÅ™Ã­stupu),Â _Restrict data access_Â (omezit pÅ™Ã­stup k datÅ¯m),Â _Apply the policy_Â (aplikovat politiku).|
|**Souhrn a DomÃ¡cÃ­ procviÄenÃ­ (10 min)**|ZnÃ¡te vÅ¡echny ÃºrovnÄ› zabezpeÄenÃ­ DW (DDM, RLS, CLS/OLS) a vÃ­te, jak se RLS implementuje pomocÃ­ TVF a Security Policy.Â **DomÃ¡cÃ­ Ãºkol:**Â ZjistÄ›te, co jeÂ **OneSecurity**Â a jakÃ½ je jeho budoucÃ­ cÃ­l (HINT: SjednocenÃ­ zabezpeÄenÃ­ RLS/CLS napÅ™Ã­Ä Spark a SQL enginy).|

--------------------------------------------------------------------------------

## ğŸš€ FÃ¡ze 5: Governance, Monitoring a FinÃ¡lnÃ­ PÅ™Ã­prava (Dny 17â€“20)

|   |   |   |
|---|---|---|
|Den|TÃ©ma|KlÃ­ÄovÃ© Fabric Itemy|
|**17**|Governance a Data Quality|Endorsement, Sensitivity Labels, Lineage, Domains|
|**18**|Deployment a CI/CD|Git Integration, Deployment Pipelines, SQL Database Projects|
|**19**|Monitoring a Optimalizace|Monitoring Hub, Capacity Metrics, V-Order, Query Optimization|
|**20**|Semantic Models a Exam Prep|Semantic Models, Direct Lake, DAX Basics, Exam Strategy|

### Den 17: Governance a Data Quality

|   |   |
|---|---|
|Sekce|Obsah|
|**Theory Focus (English)**|**Data Governance Features**Â **Endorsements**Â help users find trusted data items. There are three types:Â **Promoted**Â (recommended by item creator),Â **Certified**Â (meets organizational quality standards, requires specific permissions), andÂ **Master Data**Â (core enterprise data source, highest trust level).Â **Sensitivity Labels**Â (managed via Purview) protect data based on its confidentiality (e.g., Confidential) and can automatically apply to downstream items.Â **Lineage View**Â provides a visual map of data flow and dependencies within a workspace.Â **Domains**Â are used to logically group Workspaces based on organizational structure.|
|**ÄŒeskÃ© vysvÄ›tlenÃ­ pojmÅ¯**|**Endorsements**Â (doporuÄenÃ­) zvyÅ¡ujÃ­ dÅ¯vÄ›ru v data:Â **Promoted**Â (propagovÃ¡no) je nejniÅ¾Å¡Ã­ ÃºroveÅˆ;Â **Certified**Â (certifikovÃ¡no) znamenÃ¡ splnÄ›nÃ­ firemnÃ­ch standardÅ¯;Â **Master Data**Â je nejvyÅ¡Å¡Ã­ ÃºroveÅˆ dÅ¯vÄ›ryhodnosti.Â **Sensitivity Labels**Â (Å¡tÃ­tek citlivosti) se pouÅ¾Ã­vajÃ­ k ochranÄ› informacÃ­ (napÅ™.Â _Confidential_) a synchronizujÃ­ se s Microsoft Purview.Â **Lineage View**Â je klÃ­ÄovÃ½ pro sledovÃ¡nÃ­Â **zÃ¡vislostÃ­ dat**Â a dopadovÃ© analÃ½zy (Impact Analysis).|
|**Mini-praktickÃ¡ Ãºloha**|**ScÃ©nÃ¡Å™:**Â VÃ¡Å¡ kolega vytvoÅ™il Dataflow, kterÃ½ by mÄ›l bÃ½t Å¡iroce sdÃ­len, ale jeÅ¡tÄ› neproÅ¡el formÃ¡lnÃ­m auditem kvality. JakÃ½ typ Endorsement mu doporuÄÃ­te? (OdpovÄ›Ä:Â **Promoted**).|
|**Exam-style Question**|_Which endorsement type in Microsoft Fabric is typically reserved for data items that meet organizational quality standards and require approval from a dedicated team?_Â (A) Master Data, (B) Promoted, (C) Certified, (D) Verified.|
|**Mikro lekce angliÄtiny**|**Core Terms:**Â _Endorsement_,Â _Sensitivity Label_,Â _Master Data_,Â _Certified_,Â _Lineage View_,Â _Domain_.Â **FrÃ¡ze:**Â _Organizational standards_Â (organizaÄnÃ­ standardy),Â _Downstream item_Â (nÃ¡slednÃ½/zÃ¡vislÃ½ objekt),Â _Impact analysis_Â (dopadovÃ¡ analÃ½za).|
|**Souhrn a DomÃ¡cÃ­ procviÄenÃ­ (10 min)**|ZnÃ¡te role EndorsementÅ¯ a Lineage View pro governance.Â **DomÃ¡cÃ­ Ãºkol:**Â ZjistÄ›te, jakÃ© nÃ¡stroje se v Pythonu pouÅ¾Ã­vajÃ­ pro pokroÄilouÂ **Validaci schÃ©matu dat**Â na vstupu (HINT: Great Expectations).|

--------------------------------------------------------------------------------

### Den 18: Deployment a CI/CD

|   |   |
|---|---|
|Sekce|Obsah|
|**Theory Focus (English)**|**CI/CD and Deployment**Â **Version Control**Â in Fabric is achieved viaÂ **Git Integration**Â (Azure DevOps or GitHub), which stores item metadata (Notebook code, Pipeline structure).Â **Deployment Pipelines**Â allow manual or automated promotion of Fabric items across stages (Dev â†’ Test â†’ Prod) by comparing content between workspaces. Git integration isÂ _not mandatory_Â for Deployment Pipelines.Â **SQL Database Projects**Â offer an alternative method to deploy Data Warehouse schema changes. Changes are made locally (e.g., in VS Code), compiled into aÂ **DACPAC**Â artifact, and deployed using a connection string.|
|**ÄŒeskÃ© vysvÄ›tlenÃ­ pojmÅ¯**|**Git Integrace**Â (Version Control) se pouÅ¾Ã­vÃ¡ k verzovÃ¡nÃ­ kÃ³du a struktury Fabric objektÅ¯ (kromÄ› dat).Â **Deployment Pipelines**Â (KanÃ¡ly nasazenÃ­) automaticky porovnÃ¡vajÃ­ a posouvajÃ­ objekty mezi prostÅ™edÃ­mi (stages - Dev, Test, Prod).Â **Deployment Pipelines a Git integrace jsou oddÄ›lenÃ© funkce**.Â **SQL Database Projects**Â jsou dÅ¯leÅ¾itÃ© pro automatizovanÃ© nasazenÃ­ schÃ©matu Data Warehouse. LokÃ¡lnÃ­ zmÄ›ny se kompilujÃ­ do souboruÂ **DACPAC**, kterÃ½ reprezentuje databÃ¡zovÃ© schÃ©ma.|
|**Mini-praktickÃ¡ Ãºloha**|**ScÃ©nÃ¡Å™:**Â PrÃ¡vÄ› jste dokonÄili Data Pipeline (Dev) a chcete ji posunout do Test prostÅ™edÃ­. JakÃ½ Fabric nÃ¡stroj pouÅ¾ijete pro tento automatizovanÃ½ pÅ™esun (promotion)? (OdpovÄ›Ä:Â **Deployment Pipeline**).|
|**Exam-style Question**|_A data engineer has modified a Data Warehouse schema locally using VS Code. To deploy the schema changes to the Production environment, the engineer must first compile the project into which specific artifact type?_Â (A) JSON, (B) SQL Script, (C) DACPAC, (D) .FABRIC file.|
|**Mikro lekce angliÄtiny**|**Core Terms:**Â _Deployment Pipeline_,Â _Git Integration_,Â _Version Control_,Â _Stage_,Â _DACPAC_,Â _SQL Database Project_.Â **FrÃ¡ze:**Â _Promotion of content_Â (posun obsahu),Â _Compare changes_Â (porovnat zmÄ›ny),Â _Schema deployment_Â (nasazenÃ­ schÃ©matu).|
|**Souhrn a DomÃ¡cÃ­ procviÄenÃ­ (10 min)**|VÃ­te, jak se nasazujÃ­ zmÄ›ny pomocÃ­ Deployment Pipelines a SQL Database Projects.Â **DomÃ¡cÃ­ Ãºkol:**Â ZjistÄ›te, co seÂ **NEverzionuje**Â v Gitu (HINT: Data, Credentials, Refresh Schedules).|

--------------------------------------------------------------------------------

### Den 19: Monitoring a Optimalizace

|   |   |
|---|---|
|Sekce|Obsah|
|**Theory Focus (English)**|**Monitoring and Optimization**Â **Monitoring Hub**Â is the central dashboard for tracking all runs (Pipelines, Notebooks, Dataflows, Semantic Model Refreshes).Â **Capacity Metrics App**Â is a Power BI report used by Capacity Admins to monitorÂ **CU consumption**, identify overages, and detectÂ **throttling**. For Data Warehouse optimization,Â **Query Activity tab**Â andÂ **Query Insights**Â views track long-running or frequently executed T-SQL queries.Â **V-Order**Â is a proprietary optimization applied to Delta files in Lakehouse/DW, improving read performance (especially for Direct Lake). Delta table maintenance includesÂ **OPTIMIZE**Â (compacts small files) andÂ **VACUUM**Â (removes old files).|
|**ÄŒeskÃ© vysvÄ›tlenÃ­ pojmÅ¯**|**Monitoring Hub**Â je primÃ¡rnÃ­ mÃ­sto pro kontrolu stavu vÅ¡ech spuÅ¡tÄ›nÃ­.Â **Capacity Metrics App**Â slouÅ¾Ã­ ke sledovÃ¡nÃ­ nÃ¡kladÅ¯ a spotÅ™eby (CU usage), a k identifikaciÂ **throttlingu**Â (snÃ­Å¾enÃ­ vÃ½konu pÅ™i pÅ™etÃ­Å¾enÃ­). Pro DW optimalizaci je klÃ­ÄovÃ© sledovÃ¡nÃ­Â **Long-running queries**.Â **V-Order**Â je optimalizace souborÅ¯ pro rychlejÅ¡Ã­ ÄtenÃ­ (zvlÃ¡Å¡tÄ› pro Direct Lake). V Lakehouse se pro ÃºdrÅ¾bu pouÅ¾Ã­vÃ¡Â **OPTIMIZE**Â (kompakce) aÂ **VACUUM**Â (mazÃ¡nÃ­ starÃ½ch souborÅ¯, uvolnÄ›nÃ­ mÃ­sta).|
|**Mini-praktickÃ¡ Ãºloha**|**ScÃ©nÃ¡Å™:**Â Reporty Power BI nad VaÅ¡Ã­m Lakehouse jsou pomalÃ©. KterÃ½ pÅ™Ã­kaz byste spustili v Notebooku, abyste zlepÅ¡ili vÃ½kon ÄtenÃ­ Delta tabulky slouÄenÃ­m malÃ½ch souborÅ¯? (OdpovÄ›Ä:Â **OPTIMIZE**).|
|**Exam-style Question**|_A data engineer is debugging a recurring performance issue caused by heavy CU usage and resource contention. Which tool provides detailed insight into capacity consumption across all workloads and helps detect throttling?_Â (A) Monitoring Hub, (B) Lineage View, (C) Capacity Metrics App, (D) Query Activity Tab.|
|**Mikro lekce angliÄtiny**|**Core Terms:**Â _Monitoring Hub_,Â _Capacity Unit (CU)_,Â _Throttling_,Â _V-Order_,Â _Optimize_,Â _Vacuum_,Â _Query Insights_.Â **FrÃ¡ze:**Â _Resource contention_Â (konkurence zdrojÅ¯),Â _Long-running query_Â (dlouho bÄ›Å¾Ã­cÃ­ dotaz),Â _Compacting files_Â (kompakce souborÅ¯).|
|**Souhrn a DomÃ¡cÃ­ procviÄenÃ­ (10 min)**|ZnÃ¡te monitorovacÃ­ nÃ¡stroje na Ãºrovni Item (Monitoring Hub) a Capacity (Capacity Metrics App). UmÃ­te navrhnout ÃºdrÅ¾bu DW (V-Order) a Lakehouse (`OPTIMIZE`,Â `VACUUM`).Â **DomÃ¡cÃ­ Ãºkol:**Â ZjistÄ›te, kterÃ© typy chyb (napÅ™. pÅ™ipojenÃ­, dynamickÃ© vÃ½razy) se nejlÃ©pe diagnostikujÃ­ v paneluÂ **Inputs/Outputs**Â v detailu Pipeline Run.|

--------------------------------------------------------------------------------

### Den 20: Semantic Models a FinÃ¡lnÃ­ PÅ™Ã­prava

|   |   |
|---|---|
|Sekce|Obsah|
|**Theory Focus (English)**|**Semantic Models and Direct Lake**Â **Semantic Models**Â (formerly Datasets) are the backend for Power BI Reports, defining relationships, metrics, and DAX calculations.Â **Direct Lake**Â is the preferred connection mode in Fabric, combining the speed of Import mode with the currency of Direct Query. Direct Lake reads data directly from Delta Parquet files in OneLake. If Direct Lake cannot process a query (e.g., due to unsupported DAX or Views), it automaticallyÂ **falls back to Direct Query**.Â **DAX**Â (Data Analysis Expressions) is the language used for defining measures and calculated columns in the model.|
|**ÄŒeskÃ© vysvÄ›tlenÃ­ pojmÅ¯**|**Semantic Models**Â (SÃ©mantickÃ© modely) obsahujÃ­ logiku modelu (relace, DAX).Â **Direct Lake**Â je klÃ­ÄovÃ¡ inovace, kterÃ¡ umoÅ¾Åˆuje BI reportÅ¯m ÄÃ­st data pÅ™Ã­mo z Delta tabulek (OneLake) s extrÃ©mnÃ­ rychlostÃ­ aÂ **bez nutnosti refreshÅ¯**Â (data jsou vÅ¾dy ÄerstvÃ¡). Pokud Direct Lake narazÃ­ na limit, provede seÂ **Fallback**Â (pÅ™epnutÃ­) na Direct Query.Â **DAX**Â je jazyk pro definovÃ¡nÃ­ metrik.|
|**Mini-praktickÃ¡ Ãºloha**|**ScÃ©nÃ¡Å™:**Â VÃ¡Å¡ report pouÅ¾Ã­vÃ¡ Direct Lake, ale data z Lakehouse se prÃ¡vÄ› aktualizovala. MusÃ­te reportu zajistit nejaktuÃ¡lnÄ›jÅ¡Ã­ data. MusÃ­te ruÄnÄ› refreshovat sÃ©mantickÃ½ model? (OdpovÄ›Ä: Ne, Direct Lake Äte automaticky ÄerstvÃ¡ data).|
|**Exam-style Question**|_Which Power BI storage mode in Fabric combines the performance of Import mode with the ability to query the latest data directly from Delta Parquet files in OneLake?_Â (A) Direct Query, (B) Live Connection, (C) Import, (D) Direct Lake.|
|**Mikro lekce angliÄtiny**|**Core Terms:**Â _Semantic Model_,Â _Direct Lake_,Â _Direct Query_,Â _Import Mode_,Â _Fallback_,Â _DAX_.Â **FrÃ¡ze:**Â _Currency of data_Â (aktuÃ¡lnost dat),Â _Read directly from Delta_Â (ÄÃ­st pÅ™Ã­mo z Delta),Â _Calculated measures_Â (vypoÄÃ­tanÃ© mÃ­ry).|
|**Souhrn a DomÃ¡cÃ­ procviÄenÃ­ (10 min)**|RozumÃ­te roli Semantic ModelÅ¯ a vÃ½hodÃ¡m Direct Lake (rychlost, aktuÃ¡lnost).Â **FinÃ¡lnÃ­ Ãºkol:**Â ProjdÄ›te siÂ **Strategii zkouÅ¡ky DP-700**: ÄasovÃ½ management (1 minuta na otÃ¡zku), kdy pouÅ¾Ã­vat Learn dokumentaci a jak pÅ™istupovat k Case Study.Â **GRATULACE! Jste pÅ™ipraven/a na zÃ¡vÄ›reÄnÃ½ test.**|

--------------------------------------------------------------------------------

## ğŸ§® PrÅ¯bÄ›Å¾nÃ© mikro-hodnocenÃ­ (Micro-assessment)

Po dnech 4, 8, 12 a 16 provedeme krÃ¡tkÃ© hodnocenÃ­. V praxi to znamenÃ¡, Å¾e mi na konci tÄ›chto dnÅ¯ poloÅ¾Ã­te otÃ¡zky typu:Â _â€Pochopil/a jsem sprÃ¡vnÄ›, Å¾e pro inkrementÃ¡lnÃ­ loading je v Lakehouse klÃ­ÄovÃ½ MERGE INTO, zatÃ­mco DW pouÅ¾Ã­vÃ¡ STORED PROCEEDURES a T-SQL?â€œ_

Na zÃ¡kladÄ› VaÅ¡ich odpovÄ›dÃ­ a pÅ™Ã­padnÃ½ch nejasnostÃ­Â **dynamicky pÅ™izpÅ¯sobÃ­m**Â obsah nÃ¡sledujÃ­cÃ­ fÃ¡ze, abychom se zamÄ›Å™ili na VaÅ¡e slabÅ¡Ã­ mÃ­sta.

**Motivace:**Â Pamatujte, Å¾e Data Engineering je zÃ¡kladem modernÃ­ho BI a AI, a poptÃ¡vka po tÄ›chto dovednostech neustÃ¡le stoupÃ¡. VÃ¡Å¡ strukturovanÃ½ pÅ™Ã­stup k DP-700 VÃ¡m dÃ¡vÃ¡ nejen certifikaci, ale hlavnÄ› ucelenouÂ _strukturu pro studium a uÄenÃ­_. VydrÅ¾te, jde VÃ¡m to skvÄ›le!

# [[PLÃN JE JEDNA VÄšC (JAK NA NÄšJ?)]]
## ğŸ¯ SrovnÃ¡nÃ­ studijnÃ­ch zdrojÅ¯ pro DP-700

|   |   |   |
|---|---|---|
|Zdroj|SilnÃ© strÃ¡nky (DP-700 Focus)|SlabÃ© strÃ¡nky (DP-700 Focus)|
|**Microsoft Learn** (OficiÃ¡lnÃ­ moduly, dokumentace)|**PokrytÃ­ teorie (VysokÃ©)**, OficiÃ¡lnÃ­ **anglickÃ¡ terminologie**. NutnÃ© pro zÃ­skÃ¡nÃ­ pÅ™Ã­padnÃ©ho slevovÃ©ho voucheru.|**Praxe/Realismus (NÃ­zkÃ©)**. Moduly nejsou _dostateÄnÃ©_ ke sloÅ¾enÃ­ zkouÅ¡ky. ChybÃ­ hloubka pro rozhodovÃ¡nÃ­ ("kdy co pouÅ¾Ã­t").|
|**Fabric Dojo** (PlacenÃ© moduly 3/13)|**ReÃ¡lnÃ© scÃ©nÃ¡Å™e (VysokÃ©)**, **HyperkondenzovanÃ½** obsah, cÃ­lenÃ¡ pÅ™Ã­prava na orchestraci, CI/CD, Eventhouse. Obsahuje **hands-on tutoriÃ¡ly** a Q&A.|PokrytÃ­ je omezenÃ© na zakoupenÃ© moduly.|
|**Hands-on labs / Sandbox**|**PraktickÃ© procviÄenÃ­ (KlÃ­ÄovÃ©)**. ZkouÅ¡ka vyÅ¾aduje **hands-on zkuÅ¡enost** s Pipelines, Lakehouse, Spark, KQL a security.|Neposkytuje strukturu ani teoretickÃ© vysvÄ›tlenÃ­.|
|**Certas.com** (Practice Questions)|**Exam-style otÃ¡zky (VysokÃ©)**. Poskytuje custom-made otÃ¡zky s vysvÄ›tlenÃ­m, kterÃ© jsou podobnÃ© otÃ¡zkÃ¡m v reÃ¡lnÃ©m testu.|NepokrÃ½vÃ¡ teorii.|
|**YouTube KanÃ¡ly** (NapÅ™. Aleksi Partanen, Ansh Lamba, Will Needham)|**Hloubka teorie i praxe (VysokÃ¡)**. NabÃ­zÃ­ kompletnÃ­ sÃ©rie s _teoriÃ­_, _hands-on demy_ a _exam-like otÃ¡zkami_ v kaÅ¾dÃ©m dÃ­le.|MÅ¯Å¾e trvat dlouho (napÅ™. 11hodinovÃ½ kurz), nutnÃ¡ peÄlivÃ¡ filtrace relevantnÃ­ch tÃ©mat.|

--------------------------------------------------------------------------------

## ğŸ¥‡ Kombinace a PrioritnÃ­ Strategie (20 dnÃ­)

Vzhledem k ÄasovÃ©mu limitu a potÅ™ebÄ› posÃ­lit praktickÃ© dovednosti i angliÄtinu musÃ­m 70 % Äasu vÄ›novat praxi a 30 % cÃ­lenÃ© teorii a terminologii.

### I. Priorita 1: PraktickÃ© procviÄenÃ­ a rozhodovÃ¡nÃ­ (70 % Äasu)

ZÃ¡kladem pro ÃºspÄ›ch u DP-700 je schopnost **rozhodnout se, kterÃ½ nÃ¡stroj kdy pouÅ¾Ã­t**.

|   |   |   |
|---|---|---|
|Zdroj|DoporuÄenÃ¡ aktivita (DennÄ›)|ProÄ|
|**VÃ¡Å¡ 20dennÃ­ PlÃ¡n**|PlÅˆte **Mini-praktickÃ© Ãºlohy** a **DomÃ¡cÃ­ procviÄenÃ­** v testovacÃ­m Fabric Workspace.|OvÄ›Å™enÃ­ znalostÃ­ v praxi. NapÅ™. implementace **MERGE INTO** pro inkrementÃ¡lnÃ­ loading.|
|**YouTube SÃ©rie / PlacenÃ© Moduly**|Sledujte **cÃ­lenÃ© hands-on tutoriÃ¡ly/dema** k aktuÃ¡lnÃ­mu tÃ©matu (napÅ™. Den 13: Eventstreams dema, Den 16: RLS implementace v DW).|UmoÅ¾Åˆuje vidÄ›t reÃ¡lnÃ© implementaÄnÃ­ detaily a slyÅ¡et klÃ­Äovou anglickou terminologii.|
|**Certas.com**|DennÄ› **10â€“15 practice questions** k modulÅ¯m, kterÃ© jste ten den studoval/a (napÅ™. DP-700 Modul 1: Ingest, Modul 2: Lakehouse).|Posiluje **rozhodovacÃ­ mindset** a pÅ™Ã­pravu na _exam-style_ otÃ¡zky.|
|**GitHub / Sandboxy**|StÃ¡hnÄ›te si **kÃ³dovÃ© ukÃ¡zky** (napÅ™. Python notebooky pro **Great Expectations** validaci nebo CI/CD scÃ©nÃ¡Å™e s **SQL Database Project**).|ZÃ­skejte praktickÃ© zkuÅ¡enosti s Å™eÅ¡enÃ­m "full code" scÃ©nÃ¡Å™Å¯.|

### II. Priorita 2: CÃ­lenÃ¡ Teorie a Terminologie (30 % Äasu)

ZamÄ›Å™te se na detaily, kterÃ© dÄ›lajÃ­ rozdÃ­l mezi Lakehouse a Warehouse a na anglickÃ© frÃ¡ze.

|   |   |   |
|---|---|---|
|Zdroj|DoporuÄenÃ¡ aktivita (DennÄ›)|ProÄ|
|**VÃ¡Å¡ 20dennÃ­ PlÃ¡n**|PeÄlivÃ© prostudovÃ¡nÃ­ **Theory Focus (English)** a **Mikro lekce angliÄtiny**.|PÅ™Ã­mÃ¡ pÅ™Ã­prava na anglickou terminologii a porozumÄ›nÃ­ kontextu zkouÅ¡ky.|
|**Microsoft Learn Dokumentace**|Vyhledejte a studujte jen **rozhodovacÃ­ prÅ¯vodce** (_Decision Guides_) â€“ napÅ™. srovnÃ¡nÃ­ Lakehouse vs. Warehouse vs. KQL Database nebo volbu nÃ¡strojÅ¯ Pipeline vs. Dataflow Gen2 vs. Spark.|Poskytuje formÃ¡lnÃ­, oficiÃ¡lnÃ­ a detailnÃ­ informace nutnÃ© pro zkouÅ¡ku.|
|**Fabric Dojo** (3 moduly)|ZamÄ›Å™te se na moduly pokrÃ½vajÃ­cÃ­ **Deployment Pipelines, Version Control (Git)** a **GranulÃ¡rnÃ­ Security (RLS/CLS)**.|Tyto pokroÄilÃ© organizaÄnÃ­/sprÃ¡vnÃ­ tÃ©mata jsou Äasto v testu a vyÅ¾adujÃ­ pÅ™esnou znalost terminologie.|

--------------------------------------------------------------------------------

## â±ï¸ DennÃ­ Rozvrh (PomÄ›r Teorie vs. Praxe)

VyuÅ¾ijte strukturu z 20dennÃ­ho plÃ¡nu a alokujte Äas nÃ¡sledujÃ­cÃ­m zpÅ¯sobem:

|   |   |   |
|---|---|---|
|ÄŒasovÃ¡ Alokace|Aktivita|ZamÄ›Å™enÃ­|
|**30 min**|**Teorie + Terminologie**|Prostudujte **Theory Focus (English)** a **ÄŒeskÃ© vysvÄ›tlenÃ­** k danÃ©mu dni. ZamÄ›Å™te se na **Core Terms** a **FrÃ¡ze** (napÅ™. _High Concurrency, Multi-table transactions, Low-Code_).|
|**70 min**|**PraktickÃ© Demos & Hands-on**|Sledujte tutoriÃ¡ly nebo provÃ¡dÄ›jte **Mini-praktickÃ© Ãºlohy** (kÃ³dovÃ¡nÃ­ v Notebooku, nastavenÃ­ Copy Data, KQL dotaz). DÅ¯raz na reÃ¡lnÃ© nastavenÃ­ a **chybovÃ© stavy**.|
|**20 min**|**TestovÃ¡nÃ­ ZnalostÃ­**|10â€“15 practice questions na Certas.com a Å™eÅ¡te **Exam-style questions** z dennÃ­ho plÃ¡nu.|

--------------------------------------------------------------------------------

## ğŸš« Co PÅ™eskoÄit (Optimalizace Äasu)

ProtoÅ¾e mÃ¡me pouze 20 dnÃ­, mÄ›li byste se vyhnout aktivitÃ¡m s nÃ­zkou nÃ¡vratnostÃ­ Äasu:

1. **Aplikace znalostÃ­, kterÃ© nejsou primÃ¡rnÄ› v DP-700:** PÅ™eskoÄte hloubkovÃ© kurzy Power BI Reportingu a DAX (DAX nenÃ­ v DP-700, zamÄ›Å™te se jen na **Semantic Model** a **Direct Lake** koncepty).

2. **ÄŒtenÃ­ celÃ½ch Learn modulÅ¯:** Nestravujte celÃ½ obsah modulÅ¯ od Microsoft Learn (protoÅ¾e "nejsou dostateÄnÃ© na sloÅ¾enÃ­ zkouÅ¡ky"). PouÅ¾Ã­vejte je jen jako **rychlÃ½ referenÄnÃ­ zdroj** a pro potvrzenÃ­ terminologie.

3. **HlubokÃ© ladÄ›nÃ­ Python/PySpark syntaxe:** NepiÅ¡te sloÅ¾itÃ© Python/Pandas transformace od nuly. MÃ­sto toho se zamÄ›Å™te na **integraci Sparku do Fabricu** (Notebooks, `notebookutils`, `%%sql`) a **Delta operace (****MERGE INTO****,** **OPTIMIZE****)**.

4. **NepotÅ™ebnÃ© T-SQL detaily:** Zopakujte si klÃ­ÄovÃ© T-SQL pÅ™Ã­kazy pro DW (napÅ™. **CTAS**, **COPY INTO**), ale vyhnÄ›te se cviÄenÃ­, kterÃ¡ se silnÄ› zamÄ›Å™ujÃ­ na detaily klasickÃ½ch SQL serverÅ¯, kterÃ© Fabric DW nepodporuje (napÅ™. vynucenÃ© Primary Keys).

VÃ¡Å¡ nejvÄ›tÅ¡Ã­ nepÅ™Ã­tel je Äas. MusÃ­te **kombinovat teorii s okamÅ¾itou praktickou aplikacÃ­**, protoÅ¾e zkouÅ¡ka testuje, zda chÃ¡pete _dopady_ VaÅ¡ich architektonickÃ½ch rozhodnutÃ­ v reÃ¡lnÃ©m Fabric prostÅ™edÃ­. DrÅ¾te se striktnÄ› pomÄ›ru 70:30 (Praxe:Teorie).