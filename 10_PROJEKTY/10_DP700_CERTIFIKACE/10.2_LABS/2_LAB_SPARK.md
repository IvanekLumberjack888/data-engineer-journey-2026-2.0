# üî¨ Lab 2: Spark Notebook Transformations

**C√≠l:** Napsat PySpark transformace a pochopit lazy evaluation

---

## ‚è±Ô∏è ƒåas
60 minut

## üìã Prerequisites
- Dokonƒçen√Ω Lab 1
- Table `bronze_sales` existuje

---

## üî® Kroky

### 1Ô∏è‚É£ Create Notebook

1. V lakehouse klikni **Open notebook** ‚Üí **New notebook**
2. N√°zev: `Lab02_Transformations`

---

### 2Ô∏è‚É£ Load Data

```python
# Load bronze table
df = spark.read.table("bronze_sales")

# Show schema
df.printSchema()

# Count rows
print(f"Total rows: {df.count()}")
```

---

### 3Ô∏è‚É£ Filter Data

```python
# Filter: Only sales > $100
df_filtered = df.filter(df["SalesAmount"] > 100)

# POZOR: Zat√≠m se NIC neprovedlo! (lazy evaluation)

# Action - TEƒé se provede
print(f"Rows with sales > 100: {df_filtered.count()}")
```

---

### 4Ô∏è‚É£ Select Columns

```python
# Select only certain columns
df_selected = df_filtered.select(
    "OrderDate",
    "ProductName",
    "SalesAmount"
)

display(df_selected.limit(10))
```

---

### 5Ô∏è‚É£ Aggregate Data

```python
from pyspark.sql.functions import sum, count, avg

# Group by ProductName
df_agg = df.groupBy("ProductName").agg(
    sum("SalesAmount").alias("TotalSales"),
    count("*").alias("OrderCount"),
    avg("SalesAmount").alias("AvgSale")
)

display(df_agg.orderBy("TotalSales", ascending=False))
```

---

### 6Ô∏è‚É£ Write to Silver Layer

```python
# Write aggregated data to silver table
df_agg.write.format("delta")\
    .mode("overwrite")\
    .saveAsTable("silver_product_summary")

print("‚úÖ Silver table created!")
```

---

### 7Ô∏è‚É£ Verify Silver Table

```sql
-- Change to SQL cell
SELECT * FROM silver_product_summary
ORDER BY TotalSales DESC
```

---

### 8Ô∏è‚É£ Test Lazy Evaluation

```python
# Chain multiple transformations
df_lazy = df\
    .filter(df["SalesAmount"] > 50)\
    .select("ProductName", "SalesAmount")\
    .groupBy("ProductName")\
    .sum("SalesAmount")

print("Transformations defined!")
print("Nothing executed yet!")

# NOW execute
df_lazy.show(10)  # Action triggers execution
```

---

## ‚úÖ Success Criteria

- [ ] Notebook `Lab02_Transformations` vytvo≈ôen√Ω
- [ ] PySpark filter, select, groupBy bƒõ≈æ√≠
- [ ] Silver table `silver_product_summary` vytvo≈ôen√°
- [ ] Rozum√≠≈° lazy evaluation (transformations vs actions)
- [ ] Um√≠≈° chain multiple transformations

---

## üí° Tips

- **Transformations** (lazy): filter, select, join, groupBy
- **Actions** (execute): show, count, write, collect
- Chain transformations p≈ôed action pro lep≈°√≠ optimization
- `display()` je Fabric-specific (use `show()` in production)

---

## üîó Linky

- **Teorie:** [[10.1_NOTES/2_LAKEHOUSE_SPARK|Note 2: Lakehouse & Spark]]
- **Dal≈°√≠ Lab:** [[3_LAB_DATAFLOW|Lab 3: Dataflow Gen2]]
- **P≈ôedchoz√≠:** [[1_LAB_LAKEHOUSE|Lab 1: Lakehouse]]
- **Index:** [[10_INDEX|Zpƒõt na index]]

---

## NEXT ‚Üí [[3_LAB_DATAFLOW|Lab 3: Dataflow Gen2]]
