# Repeat testing (MY OWN QUIZ)

Advance

Q1/55

1/55

ðŸ“š CASE STUDY

Case Study 1: Data Pipeline Failure Investigation

Your organization uses Microsoft Fabric for data engineering tasks. Recently, there have been issues with data pipeline runs failing intermittently, causing delays in data processing. The failures occur randomly, and the team needs to identify the root cause quickly.

**Question:**Â You need to monitor data pipeline runs to identify the root cause of failures. What should you do?

Extend timeout settings for pipelines.

Retry failed runs without investigation.

Use a third-party tool to monitor pipeline runs.

âœ… Use the Monitoring hub to analyze pipeline run details.

#### âœ… CORRECT!

**Explanation:**Â The Monitoring Hub in Fabric provides comprehensive visibility into pipeline execution, including detailed logs, error messages, and activity-level diagnostics. This is the native tool designed for investigating pipeline failures.

Q2/55

2/55

âœ“ MULTIPLE CHOICE

Your company is using Microsoft Fabric to manage data pipelines for a lakehouse solution. Recently, a pipeline failed due to a script activity error. You need to ensure that the pipeline execution fails with a customized error message and code. Which two actions should you perform?

âœ… Add a Fail activity.

âœ… Configure custom error settings in the Fail activity.

Enable logging for pipeline activities.

Use a Try-Catch activity.

#### âœ… CORRECT!

**Explanation:**Â To ensure that a pipeline reports a custom error message and code when a script activity fails, you must add a Fail activity and configure its settings with the desired error message and code. The Fail activity is specifically designed for this purpose.

Q3/55

3/55

ðŸ“š CASE STUDY

Case Study 2: Real-Time IoT Data Ingestion

Your company processes IoT sensor data in real-time. Currently, data arrives at 10,000 events per second from IoT devices. You need to ingest this data into Fabric with minimal latency (<1 second end-to-end) and ensure data persistence for historical analysis.

**Question:**Â Which two components should you use to achieve both real-time ingestion and persistence?

âœ… Eventstream to ingest events, then Spark Structured Streaming for transformation

âœ… Eventstream configured with a Lakehouse destination

Spark Structured Streaming directly from Event Hubs with checkpointing

Dataflow Gen2 for ingestion and Warehouse for storage

#### âœ… CORRECT!

**Explanation:**Â For real-time IoT ingestion at 10K events/sec, Eventstream is ideal for initial ingestion and routing. To achieve sub-1-second latency with persistence, you can either use Eventstream directly to Lakehouse or pair Eventstream with Spark Structured Streaming.

Q4/55

4/55

â—¯ SINGLE CHOICE

You have implemented a Spark Structured Streaming job that processes customer transaction data. The job has crashed after processing 10 million events. The checkpoint location is configured. When you restart the job, from which offset should it resume?

Offset 0 - restart from the beginning

âœ… The last checkpointed offset - resume without reprocessing

Offset at 10 million - start from where it crashed

Cannot determine - manual recovery required

#### âœ… CORRECT!

**Explanation:**Â Spark Structured Streaming uses checkpoints to track the exact offset/position of processed data. When a job restarts, it automatically resumes from the last checkpointed offset, eliminating duplicate processing.

Q5/55

5/55

ðŸ“š CASE STUDY

Case Study 3: Customer Dimension Management

Your organization maintains customer master data in a Data Warehouse. Customer attributes like address, email, and phone number change frequently. You need to maintain a complete history of all customer changes for audit purposes using SCD Type 2.

**Question:**Â When implementing SCD Type 2 for the customer dimension, after a customer's address changes and you execute a MERGE statement, how many rows should exist for that customer in the dimension table?

1 - the new address replaces the old

âœ… 2 - the old record is marked inactive, a new record is added

0 - old records are deleted

Depends on the MERGE logic

#### âœ… CORRECT!

**Explanation:**Â SCD Type 2 maintains complete history by adding new rows and marking old rows as inactive (is_active = 0). You'll find both the old inactive record and the new active record.

Q7/55

6/55

ðŸ“š CASE STUDY

Case Study 4: Medallion Architecture for E-Commerce

Your e-commerce company is building a data lake using Fabric Lakehouse with a medallion architecture. Raw order data arrives daily (~50GB) with duplicates and quality issues. The silver layer should contain cleaned, deduplicated data.

**Question:**Â What is the primary responsibility of the Silver layer in your medallion architecture?

Store raw, unprocessed data

âœ… Perform data quality checks, deduplication, and business rule application

Serve aggregated data for BI dashboards

Archive historical data

#### âœ… CORRECT!

**Explanation:**Â The Silver layer is the 'cleansed' layer where data quality rules are applied, duplicates are removed, schema is enforced, and business transformations begin. Bronze is raw, Gold is aggregated.

Q8/55

7/55

â—¯ SINGLE CHOICE

Your Lakehouse bronze layer contains 50GB of daily raw data. Silver layer performs deduplication and type casting. How frequently should Silver layer tables refresh?

After every bronze file lands (near real-time, every 15 minutes)

âœ… Once daily after all bronze loads complete

On-demand by analysts using notebooks

Every 6 hours regardless of updates

#### âœ… CORRECT!

**Explanation:**Â Once daily after all bronze loads complete is optimal because all source data is present before transformation, enabling efficient batch processing and predictable SLAs.

Q9/55

8/55

ðŸ“š CASE STUDY

Case Study 5: Large-Scale Sales Data Optimization

You manage a 100-million row sales fact table in a Lakehouse. Common queries filter by product category (1000 unique values), region (50 values), and date (2-year lookback). Query performance is slow (30+ seconds). The table is currently not partitioned.

**Question:**Â What partitioning strategy would BEST optimize queries on this 100M-row table?

Partition on category (each category gets its own partition folder)

âœ… Partition on date (year/month level) with Z-order clustering on category+region

Partition on region, then Z-order on date+category

No partitioning - instead create 3 separate tables per category

#### âœ… CORRECT!

**Explanation:**Â Partition on date (month level = ~24 partitions) with Z-order on high-cardinality columns balances reasonable partition count, partition pruning for date-range queries, and multi-column performance.

Q10/55

9/55

â—¯ SINGLE CHOICE

You're optimizing a slow Lakehouse query on a 500GB fact table. Query profiling shows: 30% I/O, 40% shuffle, 30% compute. Which optimization provides the BIGGEST win?

Reduce I/O by reading fewer columns

âœ… Reduce shuffle by optimizing join order and broadcast joins

Increase compute by scaling to larger Spark pool

Cache frequently-accessed DataFrames

#### âœ… CORRECT!

**Explanation:**Â Shuffle is 40% of runtime - optimizing joins can cut this by 50%+, providing the highest ROI. Target the largest bottleneck first.

Q11/55

10/55

ðŸ“š CASE STUDY

Case Study 6: Multi-Role Data Governance

Your organization has sensitive financial data: Finance team needs ALL data (all rows, all columns), Auditors need all columns but only audit scope records, Accountants need only specific columns and their assigned accounts.

**Question:**Â Which combination of security features MUST you implement to meet these requirements?

âœ… Row-Level Security (RLS) to filter rows by role/identity

âœ… Column-Level Security (CLS) to hide sensitive columns

Item-level permission controls

Dynamic Data Masking (DDM) only

#### âœ… CORRECT!

**Explanation:**Â This scenario requires BOTH RLS (to filter rows per role) and CLS (to hide columns per role). Finance: no RLS, no CLS. Auditors: RLS filters to audit scope. Accountants: RLS filters accounts, CLS hides salary columns.

Q12/55

11/55

â—¯ SINGLE CHOICE

You have implemented Row-Level Security (RLS) in your Data Warehouse. The CEO must bypass RLS to see ALL data. How do you implement this WITHOUT breaking RLS for other users?

Create a separate 'ceo_view' with no RLS; grant CEO access

âœ… Modify the RLS filter function to check user identity and exempt CEO role

Disable RLS entirely for the CEO user

Add CEO to all roles (this still applies RLS)

#### âœ… CORRECT!

**Explanation:**Â The RLS filter function can include a conditional check: IF USER() = 'ceo@company.com' THEN 1=1 (return all) ELSE apply_normal_filter. This exempts CEO at the function level while maintaining RLS for all other users.

Q13/55

12/55

ðŸ“š CASE STUDY

Case Study 7: Delta Lake Performance Tuning

Your Lakehouse table contains 5 years of transaction history (1.5 billion rows). Recent queries on the last 90 days are slow (45 seconds), even though they filter by date column early. The table has not been optimized since initial load.

**Question:**Â What is the FIRST optimization action you should take to improve query performance?

âœ… Run OPTIMIZE tablename to compact small files

Run VACUUM tablename to remove old versions

Partition the table by date (requires rewrite)

Add Z-ORDER by date column

#### âœ… CORRECT!

**Explanation:**Â OPTIMIZE compacts small files into larger ones, improving scan performance immediately without rewriting the entire table. This is often the quickest win.

Q14/55

12/55

â—¯ SINGLE CHOICE

You run 'OPTIMIZE tablename ZORDER BY date, product' on a Delta Lake table. What does Z-ORDER accomplish?

Sorts data alphabetically

==Physically clusters rows by specified columns for faster multi-column range queries==

âŒ Compresses the data

Removes duplicate rows

#### âŒ INCORRECT

**Explanation:**Â Z-ORDER physically rearranges data so rows with similar column values are stored together. This dramatically speeds up multi-column range queries.

**Correct Answer:**Â Physically clusters rows by specified columns for faster multi-column range queries

Q15/55

13/55

ðŸ“š CASE STUDY

Case Study 8: Complex Pipeline Orchestration

Your data platform has 12 independent Copy Activities that load different source systems (total 2TB). Each Copy Activity has a 2% independent failure rate. Without retry logic, combined success rate is only 78%. You need 95%+ reliability.

**Question:**Â What is the most efficient approach to achieve 95%+ pipeline success rate?

Add individual retry logic to each of the 12 Copy Activities

âœ… Add a single retry policy at the Pipeline level

Implement a For-Each loop that retries the entire pipeline if any activity fails

Increase the timeout settings for all Copy Activities

#### âœ… CORRECT!

**Explanation:**Â A Pipeline-level retry (3 retries) is clean and scalable. If any Copy Activity fails, the entire parallel batch retries. Math: with 3 retries, success â‰ˆ 97%+.

Q16/55

14/55

â—¯ SINGLE CHOICE

In a Data Pipeline, you have 10 independent Spark Notebook activities that run sequentially, each taking 10 minutes. Total runtime: 100 minutes. You need <30 minutes. What's best?

Optimize each notebook individually (save 2-3 min per notebook)

âœ… Run notebooks in parallel using For Each or parallel activities

Switch from Spark to T-SQL (not applicable)

Accept 100-minute SLA

#### âœ… CORRECT!

**Explanation:**Â If notebooks are independent, run them in parallel. 10 notebooks Ã— 10 min parallel â‰ˆ 10-15 min total (depending on cluster capacity). This is the highest ROI.

Q17/55

14/55

âœ“ MULTIPLE CHOICE

Which actions are required to enable Git integration for a Fabric Workspace?

==Configure Azure DevOps or GitHub credentials at the workspace level==

==Connect the workspace to a Git repository==

âŒ Create a branch for each Fabric item

Enable CI/CD pipelines in Azure Pipelines

#### âŒ INCORRECT

**Explanation:**Â Configuring credentials and connecting to a Git repo are the core requirements. Creating branches for each item is optional (you can have multiple items in one branch).

**Correct Answers:**Â Configure Azure DevOps or GitHub credentials at the workspace level, Connect the workspace to a Git repository

Q18/55

14/55

â—¯ SINGLE CHOICE

You're implementing Deployment Pipelines with stages: Dev â†’ Test â†’ Prod. Which statement is TRUE about semantic model deployments?

Data in tables is automatically copied from Dev to Prod

If a report is renamed in Dev, it keeps its original name after deployment to Test

==Deployment Rules can override lakehouse bindings between stages==

âŒ A semantic model cannot be deployed without its source Data Warehouse

#### âŒ INCORRECT

**Explanation:**Â Deployment Rules allow binding changes between stages (e.g., dev_lakehouse â†’ test_lakehouse â†’ prod_lakehouse) automatically.

**Correct Answer:**Â Deployment Rules can override lakehouse bindings between stages

Q19/55

15/55

â—¯ SINGLE CHOICE

You have a Lakehouse table with 100M rows. Analysts frequently query the last 30 days (10M rows) in <2 seconds, but querying all 100M rows takes 30 seconds. How do you optimize?

Create a separate table for last 30 days (maintenance burden)

âœ… Partition by date with partition pruning in queries

Cache all 100M rows in memory

Create a materialized view for last 30 days

#### âœ… CORRECT!

**Explanation:**Â Partition by date + partition pruning is the standard approach. Queries on recent data scan only relevant partitions (dramatically faster).

Q20/55

15/55

â—¯ SINGLE CHOICE

Which tool would you use to identify if a Fabric workspace is being throttled due to capacity shortage?

âŒ Monitoring Hub (shows run status)

==Capacity Metrics App (shows resource consumption)==

Workspace Settings (shows access controls)

Git Integration (shows version history)

#### âŒ INCORRECT

**Explanation:**Â The Capacity Metrics App provides visibility into CU (Capacity Unit) consumption and throttling events.

**Correct Answer:**Â Capacity Metrics App (shows resource consumption)

Q21/55

16/55

â—¯ SINGLE CHOICE

A Dataflow Gen2 is running slowly on a 500GB dataset. Which optimization is NOT effective for Dataflow Gen2?

Reduce column count in source query

Filter data early (push predicate to source)

Use custom functions to parallelize operations

âœ… Run multiple dataflows in sequence

#### âœ… CORRECT!

**Explanation:**Â Running dataflows sequentially increases total time. Reducing columns, early filtering, and parallelization all help.

Q22/55

16/55

â—¯ SINGLE CHOICE

You need to load 2TB of data from Azure SQL Database into a Lakehouse table daily. Which method is BEST?

Dataflow Gen2 incremental load

Data Pipeline Copy Activity (full load daily)

==Pipeline Copy Activity with incremental load (timestamp-based)==

âŒ Spark notebook with direct JDBC connection

#### âŒ INCORRECT

**Explanation:**Â Pipeline Copy Activity with incremental load (using a timestamp column to load only changed rows) is best for daily 2TB loads.

**Correct Answer:**Â Pipeline Copy Activity with incremental load (timestamp-based)

Q23/55

16/55

â—¯ SINGLE CHOICE

You have implemented RLS in a Warehouse with a security policy. A query by an unprivileged user returns 0 rows (they have no rows matching their RLS filter). Is this expected?

No, RLS should return at least 1 row

âŒ No, RLS is not working correctly

==Yes, RLS correctly filters to 0 rows if no data matches the user's role==

Yes, but you should get a warning message

#### âŒ INCORRECT

**Explanation:**Â RLS correctly filters rows based on the user's identity. If no rows match the filter, returning 0 rows is expected and correct behavior.

**Correct Answer:**Â Yes, RLS correctly filters to 0 rows if no data matches the user's role

Q24/55

16/55

â—¯ SINGLE CHOICE

What is the primary purpose of a Fabric Domain?

âŒ To isolate workspaces by department

To manage capacity allocation

==To organize workspaces and enforce governance policies centrally==

To control row-level access to data

#### âŒ INCORRECT

**Explanation:**Â Domains are governance containers that organize workspaces and allow central management of policies (security labels, endorsement requirements, etc.).

**Correct Answer:**Â To organize workspaces and enforce governance policies centrally

Q25/55

17/55

â—¯ SINGLE CHOICE

You're building a real-time dashboard. Latency requirement: <500ms end-to-end. Which architecture is viable?

Eventstream â†’ Lakehouse â†’ Power BI Direct Lake

âœ… Eventstream â†’ KQL Database â†’ Power BI Real-Time Dashboard

Eventstream â†’ Spark Structured Streaming â†’ Warehouse â†’ Power BI

Eventstream â†’ Dataflow Gen2 â†’ Lakehouse â†’ Power BI

#### âœ… CORRECT!

**Explanation:**Â Only KQL Database + Power BI Real-Time Dashboard achieves <500ms latency at scale. Lakehouse/Spark/Warehouse introduce 1-5+ second latencies.

Q26/55

18/55

â—¯ SINGLE CHOICE

Which statement about Delta Lake shortcuts is TRUE?

Shortcuts copy data into Lakehouse (expensive)

âœ… Shortcuts create logical references to external data without copying

Shortcuts only work with Azure Blob Storage

Shortcuts require external data to be in Delta format only

#### âœ… CORRECT!

**Explanation:**Â Shortcuts are logical pointers that don't copy data. They support Delta, Iceberg, CSV, JSON, and other formats from external storage.

Q27/55

18/55

â—¯ SINGLE CHOICE

You're implementing incremental load for a large fact table. What is the MANDATORY column requirement?

A business key

==A timestamp or version column to track changes==

âŒ A surrogate key

A hash of all columns

#### âŒ INCORRECT

**Explanation:**Â An incremental load MUST track which rows are new/changed using a timestamp (LoadDate, ModifiedDate) or version number.

**Correct Answer:**Â A timestamp or version column to track changes

Q28/55

19/55

â—¯ SINGLE CHOICE

A semantic model refresh takes 3 hours. Users need fresh data within 1 hour. What's the BEST solution?

Increase Premium capacity by 100%

âœ… Implement incremental refresh + schedule every 1 hour

Switch to DirectQuery (may be slower)

Reduce the number of rows in the semantic model

#### âœ… CORRECT!

**Explanation:**Â Incremental refresh loads only changed rows, reducing refresh time dramatically. Schedule it every 1 hour to meet the requirement.

Q29/55

20/55

â—¯ SINGLE CHOICE

In Spark Structured Streaming, what does a watermark do?

Compresses data

Partitions data into windows

âŒ Marks acceptable lateness for late-arriving data

Prevents data duplication

#### âœ… CORRECT!

**Explanation:**Â A watermark sets a threshold for how late data can arrive before being dropped. This is crucial for windowed aggregations.

Q30/55

21/55

â—¯ SINGLE CHOICE

What mode should be used when writing a Spark Structured Streaming DataFrame to a Delta table?

Overwrite

âœ… Append

Ignore

Error

#### âœ… CORRECT!

**Explanation:**Â Streaming writes should use Append mode. Overwrite would delete existing data on every micro-batch.

Q31/55

22/55

âœ“ MULTIPLE CHOICE

Which two components are required for Spark Structured Streaming fault tolerance?

âœ… Checkpoint location

âœ… WAL (Write-Ahead Logging)

Source rate limiting

Explicit try-catch blocks

#### âœ… CORRECT!

**Explanation:**Â Checkpoint location and WAL together enable Spark Structured Streaming to recover from failures without data loss or duplication.

Q32/55

23/55

â—¯ SINGLE CHOICE

A Power BI report using DirectQuery on a Warehouse is slow (8 sec per query). What's the BEST optimization?

Switch to Import mode

âœ… Add aggregation tables in Warehouse + enable aggregations in Power BI

Increase Warehouse size

Disable query folding

#### âœ… CORRECT!

**Explanation:**Â Aggregation tables pre-calculate common aggregations. When a query matches an aggregation structure, Power BI routes to the aggregation (fast) instead of full scan.

Q33/55

24/55

â—¯ SINGLE CHOICE

You need to mask SSNs in a customer table from unprivileged users. What's the STRONGEST security approach?

Dynamic Data Masking (DDM) on SSN column

âœ… Hash SSNs at data load time (irreversible)

Use Row-Level Security to hide SSN rows

Apply Sensitivity Label to table

#### âœ… CORRECT!

**Explanation:**Â Hashing SSNs at load time is strongest because SSNs are never stored in raw form. DDM can be bypassed by DBAs.

Q34/55

24/55

â—¯ SINGLE CHOICE

Your Lakehouse table is partitioned by date. A query filters by date early: WHERE date >= '2024-01-01'. Does partition pruning apply?

No, partition pruning only works with equality filters (WHERE date = '...')

==Yes, partition pruning applies to range queries on partition column==

Only if you manually specify which partitions to read

âŒ Partition pruning requires a clustered index

#### âŒ INCORRECT

**Explanation:**Â Partition pruning applies to range queries (>=, <=, BETWEEN) on the partition column. The query engine automatically skips partitions outside the range.

**Correct Answer:**Â Yes, partition pruning applies to range queries on partition column

Q35/55

25/55

â—¯ SINGLE CHOICE

You're implementing cost optimization for a 500GB Lakehouse. 50GB is frequently queried (hot), 450GB is archival. What's best?

Keep all 500GB in Lakehouse

âœ… Archive 450GB to Blob Storage; keep 50GB in Lakehouse

Compress all data; no archival needed

Create two separate Lakehouses

#### âœ… CORRECT!

**Explanation:**Â Archiving cold 450GB to Blob Storage (~$20/month vs ~$2K/month in Lakehouse) saves ~99% on storage with minimal performance impact.

Q36/55

26/55

â—¯ SINGLE CHOICE

Which of the following CANNOT be stored in a Lakehouse?

Structured tables (parquet)

Semi-structured data (JSON)

Unstructured files (images, PDFs)

âœ… Relational schema with constraints and triggers

#### âœ… CORRECT!

**Explanation:**Â Lakehouse stores files and tables in Delta format but doesn't enforce relational constraints (PKs, FKs) or triggers at the storage layer.

Q37/55

27/55

â—¯ SINGLE CHOICE

You create a shortcut from Lakehouse A to Lakehouse B. Files in B are updated daily. When do changes appear in A?

âœ… Immediately (shortcuts reference live data)

After A's next refresh cycle

Never (shortcuts are snapshots)

Only after manually updating the shortcut

#### âœ… CORRECT!

**Explanation:**Â Shortcuts are logical references to live data. Changes in B are immediately visible through A's shortcut.

Q38/55

27/55

â—¯ SINGLE CHOICE

A Notebook activity in a Pipeline fails with an error. The pipeline continues and runs a downstream Refresh Semantic Model activity. Why did the downstream activity run?

Bug in Fabric

The downstream activity is configured to run on completion (success or failure)

Automatic retry restarted the notebook

The downstream activity is independent

#### âŒ INCORRECT

**Explanation:**Â Pipeline activities can be configured with different dependency rules: On Success, On Failure, On Skip, or On Completion.

**Correct Answer:**Â The downstream activity is configured to run on completion (success or failure)