Testování znalostí:

You're using Deployment Pipeline to move semantic models from Dev to Prod. What's deployed and what's NOT?
	  
	  The semantic model definition IS deployed; the refresh schedule is NOT deployed

You have a Lakehouse with Delta tables. A data quality check discovered 500 duplicate records in a 2 million row Orders table. What is the best approach to remove them?
	
	Use MERGE to INSERT from deduplicated CTE, then DELETE non-matching rows

Your Lakehouse is experiencing slow queries on a 500 GB fact table. You've analyzed the query plan and see that scanning is inefficient. Which optimization should you try FIRST?
	
	Implement partition pruning with WHERE clauses on partition columns

MULTIPLE CHOICE

Your company processes real-time stock price data: 100K events/second, <1 sec latency requirement. Which architecture balances latency, cost, complexity?
	
	Eventstream + Eventhouse: sub-100ms latency, highest cost

	Spark Structured Streaming → Lakehouse: 1-5 sec latency, medium cost

	Eventstream → KQL → Real-Time Dashboard: <1 sec latency, moderate cost

	Batch pipeline every 1 minute: unacceptable for stock prices

Your company uses a Fabric Workspace with a Lakehouse containing 500 million customer records. Sales team needs to see only their region's data, Finance needs all data but without customer names, and Warehouse needs access for schema updates. What is the PRIMARY security mechanism you should implement for Sales team?
	
	Row-Level Security (RLS) with SECURITY POLICY filtering by region

Your organization needs GDPR compliance: EU users' data must stay in EU. You have Fabric workspaces in both EU and US regions. What governance control ensures compliance?
	
	  Use Fabric capacity in EU region; restrict workspace assignments via Domain rules

You have a Data Warehouse with millions of transactions. A simple aggregation query takes 30 seconds. Query plan shows full table scan. What's the ROOT CAUSE you should investigate first?
	
	Missing index on the GROUP BY column

You have a real-time dashboard showing fraud alerts that must display within 5 seconds of a suspicious transaction. Which Fabric component should you use?
	
	Eventstream → KQL database → Real-Time Dashboard with KQL queries

You're loading 100 GB of customer data from on-premise SQL Server to Fabric. What's the BEST approach to minimize source system impact?
	
	Use Fabric Mirroring for automatic real-time replication

MULTIPLE CHOICE

You're implementing error handling in a data pipeline. What's the BEST layered approach?
	
	Try-Catch block with Application Insights logging
	
	Schema validation at ingest (reject invalid JSON); bad records → quarantine table
	
	Data quality checks (NULL %, type mismatches); alert if thresholds exceeded
	
	Slack/Teams notifications on pipeline failure with error details
	
	Store raw API responses as JSON files; parse in separate notebook

Your Lakehouse medallion has Bronze (raw 100 GB), Silver (cleaned 20 GB), Gold (aggregated 1 GB). How often should Silver refresh?
	
	Once daily after Bronze completes = all source data present + efficient batch processing + predictable SLA. If Bronze loads continuously throughout day, waiting until completion prevents partial refreshes.

You have Eventstream outputting at-least-once to Lakehouse table. Events arrive at 100K/second. How do you handle duplicates?
	
	  Implement MERGE in your ETL to deduplicate by event ID

To stačí - 33. den v řadě. A nezastavuji.