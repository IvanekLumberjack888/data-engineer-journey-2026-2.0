Repeating

# EXAMPLE CASE STUDY PREP:

# ðŸš€ CASE STUDY #4: "LogisticsPro" â€“ Real-Time Streaming & Monitoring

Now switching toÂ **real-time architecture and monitoring**Â â€” critical for DP-700.

---

## ðŸ“– Scenario

**LogisticsPro**Â is a logistics company tracking shipments in real-time:

|Data Source|Type|Volume|Latency Requirement|
|---|---|---|---|
|**GPS Trackers**|Event Hub stream|50K events/sec|<5 seconds|
|**Delivery Status Updates**|Event Hub messages|100K/day|<1 minute|
|**Vehicle Diagnostics**|IoT Hub streaming|Continuous|Real-time alerts|
|**Customer Orders**|SQL Database|Batch daily|Hourly refresh|
|**Historical Shipments**|Parquet files in ADLS|1 year archive|Batch queries|

**Challenges:**

- Need real-time dashboards for dispatchers (vehicle locations, alerts)
    
- Detect anomalies (vehicle stops, delays, theft risk) instantly
    
- Archive historical data for compliance
    
- Monitor pipeline health and alert on failures
    

---

## ðŸ§  Question 1

GPS trackers and vehicle diagnostics stream continuously to Event Hub.  
WhichÂ **combination**Â ingests this into Fabric withÂ **lowest latency**?

**A.**Â Dataflow Gen2 â†’ Lakehouse  
**B.**Â Eventstream â†’ Eventhouse (KQL DB)  
**C.**Â Spark Structured Streaming â†’ Warehouse  
**D.**Â Data Pipeline (Copy activity) â†’ Lakehouse

---

## ðŸ§  Question 2

Real-time dashboards need to showÂ **vehicle locations and alerts**Â updated every 5-10 seconds.  
Which Fabric visualization tool is best?

**A.**Â Power BI Report (refresh every 5 sec)  
**B.**Â Semantic model with direct query  
**C.**Â Real-time Dashboard (KQL-backed)  
**D.**Â Power BI Paginated Report

---

## ðŸ§  Question 3

GPS stream shows sudden vehicleÂ **stop events**Â that might indicate theft or breakdown.  
WhichÂ **Eventstream transformation**Â detects stops and routes them to an alert queue?

**A.**Â Filter (condition: speed = 0)  
**B.**Â Aggregate (count events by vehicle)  
**C.**Â Group By (window aggregation)  
**D.**Â Union (combine multiple streams)

---

## ðŸ§  Question 4

The GPS stream is ingestingÂ **50K events/sec**Â into Eventhouse.  
Pipeline suddenly starts dropping events (OutgoingMessages < IncomingMessages).  
What is theÂ **most likely cause**?

**A.**Â Eventstream transformation too complex  
**B.**Â Eventhouse write throttling (capacity limit)  
**C.**Â Event Hub producer sending malformed messages  
**D.**Â Networking latency between Event Hub and Fabric

---

## ðŸ§  Question 5

Where do youÂ **monitor Eventstream health**Â â€” check Data Insights metrics (IncomingMessages, OutgoingMessages, latency)?

**A.**Â Monitoring Hub  
**B.**Â Fabric Admin Portal  
**C.**Â Eventstream's Data Insights tab  
**D.**Â Capacity Metrics App

---

## ðŸ§  Question 6

Daily customer orders from SQL Database must beÂ **merged with historical shipments**Â (Parquet in ADLS) to create a unified analytics model.  
Which approach avoidsÂ **data duplication**?

**A.**Â Copy orders + historical to Lakehouse, then MERGE  
**B.**Â Use shortcuts: historical shortcut + copy orders into Lakehouse  
**C.**Â Create Warehouse with views over external data  
**D.**Â Replicate everything into Eventhouse

---

## ðŸ§  Question 7

Data Pipeline ingesting orders sometimes fails due toÂ **transient network errors**.  
How do you configureÂ **automatic retry**Â without manual intervention?

**A.**Â Add retry logic in the Copy activity  
**B.**Â Schedule trigger with daily run (manual retry next day)  
**C.**Â Deployment pipeline with approval step  
**D.**Â Use Eventstream with buffer mode

---

## ðŸ§  Question 8

When a pipeline fails, ops team must beÂ **notified immediately**Â (not check dashboard manually).  
How?

**A.**Â Configure alert rule in Monitor Hub with email notification  
**B.**Â Add "Send Email" activity on the On-Fail branch  
**C.**Â Set up Fabric Activator trigger on semantic model  
**D.**Â All of the above work

---

## ðŸ§  Question 9 (Bonus)

LogisticsPro wants toÂ **archive cold data**Â (shipments older than 6 months) to reduce storage costs while keeping active data fast.  
Which strategy in Lakehouse?

**A.**Â VACUUM to delete old Delta versions  
**B.**Â Partition by ship_date, move old partitions to external storage via shortcuts  
**C.**Â Disable VOrder on old tables  
**D.**Â Move to Warehouse and delete from Lakehouse

---

