Repeating

# EXAMPLE CASE STUDY PREP:

# ðŸš€ CASE STUDY #4: "LogisticsPro" â€“ Real-Time Streaming & Monitoring

Now switching toÂ **real-time architecture and monitoring**Â â€” critical for DP-700.

---

## ðŸ“– Scenario

**LogisticsPro**Â is a logistics company tracking shipments in real-time:

|Data Source|Type|Volume|Latency Requirement|
|---|---|---|---|
|**GPS Trackers**|Event Hub stream|50K events/sec|<5 seconds|
|**Delivery Status Updates**|Event Hub messages|100K/day|<1 minute|
|**Vehicle Diagnostics**|IoT Hub streaming|Continuous|Real-time alerts|
|**Customer Orders**|SQL Database|Batch daily|Hourly refresh|
|**Historical Shipments**|Parquet files in ADLS|1 year archive|Batch queries|

**Challenges:**

- Need real-time dashboards for dispatchers (vehicle locations, alerts)
    
- Detect anomalies (vehicle stops, delays, theft risk) instantly
    
- Archive historical data for compliance
    
- Monitor pipeline health and alert on failures
    

---

## ðŸ§  Question 1

GPS trackers and vehicle diagnostics stream continuously to Event Hub.  
WhichÂ **combination**Â ingests this into Fabric withÂ **lowest latency**?

**A.**Â Dataflow Gen2 â†’ Lakehouse  
**B.**Â Eventstream â†’ Eventhouse (KQL DB)  
**C.**Â Spark Structured Streaming â†’ Warehouse  
**D.**Â Data Pipeline (Copy activity) â†’ Lakehouse

---

## ðŸ§  Question 2

Real-time dashboards need to showÂ **vehicle locations and alerts**Â updated every 5-10 seconds.  
Which Fabric visualization tool is best?

**A.**Â Power BI Report (refresh every 5 sec)  
**B.**Â Semantic model with direct query  
**C.**Â Real-time Dashboard (KQL-backed)  
**D.**Â Power BI Paginated Report

---

## ðŸ§  Question 3

GPS stream shows sudden vehicleÂ **stop events**Â that might indicate theft or breakdown.  
WhichÂ **Eventstream transformation**Â detects stops and routes them to an alert queue?

**A.**Â Filter (condition: speed = 0)  
**B.**Â Aggregate (count events by vehicle)  
**C.**Â Group By (window aggregation)  
**D.**Â Union (combine multiple streams)

---

## ðŸ§  Question 4

The GPS stream is ingestingÂ **50K events/sec**Â into Eventhouse.  
Pipeline suddenly starts dropping events (OutgoingMessages < IncomingMessages).  
What is theÂ **most likely cause**?

**A.**Â Eventstream transformation too complex  
**B.**Â Eventhouse write throttling (capacity limit)  
**C.**Â Event Hub producer sending malformed messages  
**D.**Â Networking latency between Event Hub and Fabric

---

## ðŸ§  Question 5

Where do youÂ **monitor Eventstream health**Â â€” check Data Insights metrics (IncomingMessages, OutgoingMessages, latency)?

**A.**Â Monitoring Hub  
**B.**Â Fabric Admin Portal  
**C.**Â Eventstream's Data Insights tab  
**D.**Â Capacity Metrics App

---

## ðŸ§  Question 6

Daily customer orders from SQL Database must beÂ **merged with historical shipments**Â (Parquet in ADLS) to create a unified analytics model.  
Which approach avoidsÂ **data duplication**?

**A.**Â Copy orders + historical to Lakehouse, then MERGE  
**B.**Â Use shortcuts: historical shortcut + copy orders into Lakehouse  
**C.**Â Create Warehouse with views over external data  
**D.**Â Replicate everything into Eventhouse

---

## ðŸ§  Question 7

Data Pipeline ingesting orders sometimes fails due toÂ **transient network errors**.  
How do you configureÂ **automatic retry**Â without manual intervention?

**A.**Â Add retry logic in the Copy activity  
**B.**Â Schedule trigger with daily run (manual retry next day)  
**C.**Â Deployment pipeline with approval step  
**D.**Â Use Eventstream with buffer mode

---

## ðŸ§  Question 8

When a pipeline fails, ops team must beÂ **notified immediately**Â (not check dashboard manually).  
How?

**A.**Â Configure alert rule in Monitor Hub with email notification  
**B.**Â Add "Send Email" activity on the On-Fail branch  
**C.**Â Set up Fabric Activator trigger on semantic model  
**D.**Â All of the above work

---

## ðŸ§  Question 9 (Bonus)

LogisticsPro wants toÂ **archive cold data**Â (shipments older than 6 months) to reduce storage costs while keeping active data fast.  
Which strategy in Lakehouse?

**A.**Â VACUUM to delete old Delta versions  
**B.**Â Partition by ship_date, move old partitions to external storage via shortcuts  
**C.**Â Disable VOrder on old tables  
**D.**Â Move to Warehouse and delete from Lakehouse

---

## ðŸ§  Q1 â€“ GPS trackers + vehicle diagnostics streaming to Event Hub

**Your answer: B: Eventstream â†’ Eventhouse (KQL DB)**  
â†’ âœ…Â **Correct!**

Eventstream is designed for real-time ingestion from Event Hub, and Eventhouse (KQL DB) is optimized for high-rate time-series data with <5-second latency. Perfect combo for GPS tracking.

---

## ðŸ§  Q2 â€“ Real-time dashboards (vehicle locations, alerts, 5-10 sec refresh)

**Your answer: C: Real-time Dashboard (KQL-backed)**  
â†’ âœ…Â **Correct!**

Real-time Dashboards in Fabric are built on KQL Databases (Eventhouse) and automatically refresh without the 5-second refresh limits of Power BI. Best for operational monitoring.

---

## ðŸ§  Q3 â€“ Detect vehicle stop (speed = 0) and route to alert queue

**Your answer: B: Aggregate (count events by vehicle)**  
â†’ âŒÂ **Not quite. Correct answer is A: Filter (condition: speed = 0)**

**Why:**

- **Filter**Â =Â **condition-based routing**Â â€” keep only events where speed = 0 (stops detected)
    
- **Aggregate**Â =Â **windowed counting**Â â€” useful for "count events per vehicle per time window", but doesn't detect individual stops
    
- You'd use Filter first to identify stops, then optionally Aggregate to count them
    

**Exam tip:**Â When you see "detect/identify events matching a condition" â†’Â **Filter**. When you see "count/sum over time" â†’Â **Aggregate**.

---

## ðŸ§  Q4 â€“ GPS stream drops events (OutgoingMessages < IncomingMessages)

**Your answer: B: Eventhouse write throttling (capacity limit)**  
â†’ âœ…Â **Correct!**

When Eventhouse can't keep up with ingest rate, it throttles writes. You'd see this in Data Insights: messages arrive but not all get written. Solution: scale capacity or optimize transformation complexity.

---

## ðŸ§  Q5 â€“ Monitor Eventstream Data Insights

**Your answer: C: Eventstream's Data Insights tab**  
â†’ âœ…Â **Correct!**

TheÂ **Data Insights**Â tab in the Eventstream UI shows IncomingMessages, OutgoingMessages, latency, and runtime logs. This is the native monitoring place for Eventstreams (not Monitoring Hub, which is for pipelines/notebooks).

---

## ðŸ§  Q6 â€“ Merge daily orders with historical shipments without duplication

**Your answer: B: Use shortcuts: historical shortcut + copy orders into Lakehouse**  
â†’ âœ…Â **Correct!**

Shortcuts avoid data duplication by creatingÂ **logical pointers**Â to external Parquet files. You copy fresh orders, then join with historical data via shortcut â€” no duplicate copy of historical data.

---

## ðŸ§  Q7 â€“ Auto-retry on transient network failures

**Your answer: C: Deployment pipeline with approval step**  
â†’ âŒÂ **Not right. Correct answer is A: Add retry logic in the Copy activity**

**Why:**

- **Copy activity**Â hasÂ **Retry Policy**Â (number of retries, backoff interval) â€” designed for transient failures
    
- **Deployment pipeline**Â is for Dev/Prod promotion, not failure handling
    
- **Schedule trigger**Â (option B) would just re-run the whole pipeline next day â€” not automatic retry
    

**Exam tip:**Â For transient failures in Copy/activities â†’ configureÂ **Retry Policy**Â directly on the activity.

---

## ðŸ§  Q8 â€“ Notify ops team immediately on pipeline failure

**Your answer: B: Add "Send Email" activity on the On-Fail branch**  
â†’ âœ…Â **Correct!**

This is the most direct approach â€” when pipeline fails, the On-Fail branch triggers Send Email. Monitor Hub alerts (option A) also work but require separate setup. Option C (Activator) is more complex.

**Note:**Â Options A and B both work in practice, butÂ **B is the cleanest pipeline-level solution**.

---

## ðŸ§  Q9 (Bonus) â€“ Archive cold data without deleting active data

**Your answer: A: VACUUM to delete old Delta versions**  
â†’ âŒÂ **Not quite. Correct answer is B: Partition by ship_date, move old partitions to external storage via shortcuts**

**Why:**

- **VACUUM**Â deletesÂ **old versions**Â of tables (Delta time-travel cleanup), not the actual old rows
    
- **Partitioning by ship_date**Â lets youÂ **selectively move**Â old date ranges to cold storage
    
- Historical data stays accessible viaÂ **shortcuts**Â without paying for hot Lakehouse storage
    
- This is the classicÂ **data tiering**Â pattern for cost optimization
    

**Exam tip:**Â VACUUM = cleanup versions. Partitioning + shortcuts = data tiering. These are different concepts!

---

## âœ… Final Score: 6.5 / 9 â‰ˆÂ **72%**

Good progress! ðŸ’ª You understandÂ **real-time architecture well**Â (Eventstream â†’ Eventhouse, Data Insights monitoring). The gaps are on:

- **Filter vs. Aggregate**Â in Eventstreams (very common confusion)
    
- **Retry policy**Â vs. deployment pipeline (different concerns)
    
- **VACUUM vs. partitioning**Â (data versioning vs. data tiering)
    

These are subtle but important distinctions for DP-700.