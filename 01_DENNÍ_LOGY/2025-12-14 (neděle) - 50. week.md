Další den učení.

# Důležitý poznatky:

## Spark pool customization

- **Starter Pool** https://learn.microsoft.com/en-us/fabric/data-engineering/spark-compute 
	- Benefits (Starter Pool)
		- Example: It takes less than 10 seconds normally to start a Spark Session.
		- Essentially, it is 'good-enough' for simple workloads - it can Autoscale to larger workloads too. Node Size is always Medium.

- **Custom Pool**
			**Data size**: Large datasets => Node (UZEL) much more increased sizing.
			**Concurency**: limited set of resources based on CU for multiple users => mnoho uživatelů spapá kapacitu Spraku
			**Variance in job resource requirements**: Protože mohou být vysoké rozptyly na joby je potřeba ty Variance v poždavcích customizovat
			**Job Type**: Kapacita pro ML uživatele Respektive Scientisty je potřeba větší.

### Options for _where_ to perform
	
- **Capacity level**: 
	- Capacity Admins can set limitations of the types of Pools that Workspaces in the Capacity can use.
	- Important impact has size of the Capacity
- **Workspace-level**:
	- can define a Custom Pool in customization everything. by the way settings for concurency, allocation...
- **Environment-level**
	- configuration
- **Session-level -** some of the Spark settings are 'mutable'

##  CU/SKU
- There is some maths behind it, essentially, if you buy an F64 capacity (for example), you will get 2 Spark VCores per capacity unit (so 128), PLUS Fabric Capacities allow you to burst 3x that amount, for short periods of time. So during intensive workloads, on an F64, the maximum Spark VCores available to you will be 384.
- Capacity Administrator can also limit what Workspace Administrators
- Admins might:
	- Disable starter pools for workspaces
	- Allow/ disallow Workspace Admins to create customized workspace pools
	- 
