# Repeat testing (MY OWN QUIZ)

Q1/55

1/55

ðŸ“š CASE STUDY

Case Study 1: Data Pipeline Failure Investigation

Your organization uses Microsoft Fabric for data engineering tasks. Recently, there have been issues with data pipeline runs failing intermittently, causing delays in data processing. The failures occur randomly, and the team needs to identify the root cause quickly.

**Question:**Â You need to monitor data pipeline runs to identify the root cause of failures. What should you do?

Extend timeout settings for pipelines.

Retry failed runs without investigation.

Use a third-party tool to monitor pipeline runs.

**Use the Monitoring hub to analyze pipeline run details.**

#### âœ… CORRECT!

**Explanation:**Â The Monitoring Hub in Fabric provides comprehensive visibility into pipeline execution, including detailed logs, error messages, and activity-level diagnostics. This is the native tool designed for investigating pipeline failures.

Q2/55

2/55

âœ“ MULTIPLE CHOICE

Your company is using Microsoft Fabric to manage data pipelines for a lakehouse solution. Recently, a pipeline failed due to a script activity error. You need to ensure that the pipeline execution fails with a customized error message and code. Which two actions should you perform?

==Add a Fail activity.==

==Configure custom error settings in the Fail activity.==

Enable logging for pipeline activities.

Use a Try-Catch activity.

#### âœ… CORRECT!

**Explanation:**Â To ensure that a pipeline reports a custom error message and code when a script activity fails, you must add a Fail activity and configure its settings with the desired error message and code. The Fail activity is specifically designed for this purpose.

ðŸ“š CASE STUDY

Case Study 2: Real-Time IoT Data Ingestion

Your company processes IoT sensor data in real-time. Currently, data arrives at 10,000 events per second from IoT devices. You need to ingest this data into Fabric with minimal latency (<1 second end-to-end) and ensure data persistence for historical analysis.

**Question:**Â Which two components should you use to achieve both real-time ingestion and persistence?

==Eventstream to ingest events, then Spark Structured Streaming for transformation==

==Eventstream configured with a Lakehouse destination==

Spark Structured Streaming directly from Event Hubs with checkpointing

Dataflow Gen2 for ingestion and Warehouse for storage

#### âœ… CORRECT!

**Explanation:**Â For real-time IoT ingestion at 10K events/sec, Eventstream is ideal for initial ingestion and routing. To achieve sub-1-second latency with persistence, you can either use Eventstream directly to Lakehouse or pair Eventstream with Spark Structured Streaming.

Q4/55

4/55

â—¯ SINGLE CHOICE

You have implemented a Spark Structured Streaming job that processes customer transaction data. The job has crashed after processing 10 million events. The checkpoint location is configured. When you restart the job, from which offset should it resume?

Offset 0 - restart from the beginning

==The last checkpointed offset - resume without reprocessing==

Offset at 10 million - start from where it crashed

Cannot determine - manual recovery required

#### âœ… CORRECT!

**Explanation:**Â Spark Structured Streaming uses checkpoints to track the exact offset/position of processed data. When a job restarts, it automatically resumes from the last checkpointed offset, eliminating duplicate processing.

Q5/55

5/55

ðŸ“š CASE STUDY

Case Study 3: Customer Dimension Management

Your organization maintains customer master data in a Data Warehouse. Customer attributes like address, email, and phone number change frequently. You need to maintain a complete history of all customer changes for audit purposes using SCD Type 2.

**Question:**Â When implementing SCD Type 2 for the customer dimension, after a customer's address changes and you execute a MERGE statement, how many rows should exist for that customer in the dimension table?

1 - the new address replaces the old

==2 - the old record is marked inactive, a new record is added==

0 - old records are deleted

Depends on the MERGE logic

#### âœ… CORRECT!

**Explanation:**Â SCD Type 2 maintains complete history by adding new rows and marking old rows as inactive (is_active = 0). You'll find both the old inactive record and the new active record.

Q6/55

5/55

âœ“ MULTIPLE CHOICE

You're loading customer changes into a dimension table using SCD Type 2. Which columns are MANDATORY for this implementation?

==Surrogate key (customer_sk) to uniquely identify each version==

==Business key (customer_id) to group versions of the same customer==

==Effective_date to mark when the change became active==

==End_date to mark when the version became inactive==

#### âœ… CORRECT!

**Explanation:**Â SCD Type 2 requires all four elements: surrogate key (version ID), business key (customer ID), effective_date (when change active), and end_date (when version expires).

Q7/55

6/55

ðŸ“š CASE STUDY

Case Study 4: Medallion Architecture for E-Commerce

Your e-commerce company is building a data lake using Fabric Lakehouse with a medallion architecture. Raw order data arrives daily (~50GB) with duplicates and quality issues. The silver layer should contain cleaned, deduplicated data.

**Question:**Â What is the primary responsibility of the Silver layer in your medallion architecture?

Store raw, unprocessed data

==Perform data quality checks, deduplication, and business rule application==

Serve aggregated data for BI dashboards

Archive historical data

#### âœ… CORRECT!

**Explanation:**Â The Silver layer is the 'cleansed' layer where data quality rules are applied, duplicates are removed, schema is enforced, and business transformations begin. Bronze is raw, Gold is aggregated.

Q8/55

7/55

â—¯ SINGLE CHOICE

Your Lakehouse bronze layer contains 50GB of daily raw data. Silver layer performs deduplication and type casting. How frequently should Silver layer tables refresh?

After every bronze file lands (near real-time, every 15 minutes)

==Once daily after all bronze loads complete==

On-demand by analysts using notebooks

Every 6 hours regardless of updates

#### âœ… CORRECT!

**Explanation:**Â Once daily after all bronze loads complete is optimal because all source data is present before transformation, enabling efficient batch processing and predictable SLAs.

Q9/55

8/55

ðŸ“š CASE STUDY

Case Study 5: Large-Scale Sales Data Optimization

You manage a 100-million row sales fact table in a Lakehouse. Common queries filter by product category (1000 unique values), region (50 values), and date (2-year lookback). Query performance is slow (30+ seconds). The table is currently not partitioned.

**Question:**Â What partitioning strategy would BEST optimize queries on this 100M-row table?

Partition on category (each category gets its own partition folder)

==Partition on date (year/month level) with Z-order clustering on category+region==

Partition on region, then Z-order on date+category

No partitioning - instead create 3 separate tables per category

#### âœ… CORRECT!

**Explanation:**Â Partition on date (month level = ~24 partitions) with Z-order on high-cardinality columns balances reasonable partition count, partition pruning for date-range queries, and multi-column performance.

Q10/55

8/55

â—¯ SINGLE CHOICE

You're optimizing a slow Lakehouse query on a 500GB fact table. Query profiling shows: 30% I/O, 40% shuffle, 30% compute. Which optimization provides the BIGGEST win?

Reduce I/O by reading fewer columns

==Reduce shuffle by optimizing join order and broadcast joins==

âŒ Increase compute by scaling to larger Spark pool

Cache frequently-accessed DataFrames

#### âŒ INCORRECT

**Explanation:**Â Shuffle is 40% of runtime - optimizing joins can cut this by 50%+, providing the highest ROI. Target the largest bottleneck first.

**Correct Answer:**Â Reduce shuffle by optimizing join order and broadcast joins

Q11/55

9/55

ðŸ“š CASE STUDY

Case Study 6: Multi-Role Data Governance

Your organization has sensitive financial data: Finance team needs ALL data (all rows, all columns), Auditors need all columns but only audit scope records, Accountants need only specific columns and their assigned accounts.

**Question:**Â Which combination of security features MUST you implement to meet these requirements?

==Row-Level Security (RLS) to filter rows by role/identity==

==Column-Level Security (CLS) to hide sensitive columns==

Item-level permission controls

Dynamic Data Masking (DDM) only

#### âœ… CORRECT!

**Explanation:**Â This scenario requires BOTH RLS (to filter rows per role) and CLS (to hide columns per role). Finance: no RLS, no CLS. Auditors: RLS filters to audit scope. Accountants: RLS filters accounts, CLS hides salary columns.

Q12/55

10/55

â—¯ SINGLE CHOICE

You have implemented Row-Level Security (RLS) in your Data Warehouse. The CEO must bypass RLS to see ALL data. How do you implement this WITHOUT breaking RLS for other users?

Create a separate 'ceo_view' with no RLS; grant CEO access

==Modify the RLS filter function to check user identity and exempt CEO role==

Disable RLS entirely for the CEO user

Add CEO to all roles (this still applies RLS)

#### âœ… CORRECT!

**Explanation:**Â The RLS filter function can include a conditional check: IF USER() = 'ceo@company.com' THEN 1=1 (return all) ELSE apply_normal_filter. This exempts CEO at the function level while maintaining RLS for all other users.

Q13/55

10/55

ðŸ“š CASE STUDY

Case Study 7: Delta Lake Performance Tuning

Your Lakehouse table contains 5 years of transaction history (1.5 billion rows). Recent queries on the last 90 days are slow (45 seconds), even though they filter by date column early. The table has not been optimized since initial load.

**Question:**Â What is the FIRST optimization action you should take to improve query performance?

==Run OPTIMIZE tablename to compact small files==

âŒ Run VACUUM tablename to remove old versions

Partition the table by date (requires rewrite)

Add Z-ORDER by date column

#### âŒ INCORRECT

**Explanation:**Â OPTIMIZE compacts small files into larger ones, improving scan performance immediately without rewriting the entire table. This is often the quickest win.

**Correct Answer:**Â Run OPTIMIZE tablename to compact small files

