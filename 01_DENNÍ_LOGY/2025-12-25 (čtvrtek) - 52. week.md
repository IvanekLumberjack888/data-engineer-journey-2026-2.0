# Repeat testing (MY OWN QUIZ)

Q1/55

1/55

ðŸ“š CASE STUDY

Case Study 1: Data Pipeline Failure Investigation

Your organization uses Microsoft Fabric for data engineering tasks. Recently, there have been issues with data pipeline runs failing intermittently, causing delays in data processing. The failures occur randomly, and the team needs to identify the root cause quickly.

**Question:**Â You need to monitor data pipeline runs to identify the root cause of failures. What should you do?

Extend timeout settings for pipelines.

Retry failed runs without investigation.

Use a third-party tool to monitor pipeline runs.

**Use the Monitoring hub to analyze pipeline run details.**

#### âœ… CORRECT!

**Explanation:**Â The Monitoring Hub in Fabric provides comprehensive visibility into pipeline execution, including detailed logs, error messages, and activity-level diagnostics. This is the native tool designed for investigating pipeline failures.

Q2/55

2/55

âœ“ MULTIPLE CHOICE

Your company is using Microsoft Fabric to manage data pipelines for a lakehouse solution. Recently, a pipeline failed due to a script activity error. You need to ensure that the pipeline execution fails with a customized error message and code. Which two actions should you perform?

==Add a Fail activity.==

==Configure custom error settings in the Fail activity.==

Enable logging for pipeline activities.

Use a Try-Catch activity.

#### âœ… CORRECT!

**Explanation:**Â To ensure that a pipeline reports a custom error message and code when a script activity fails, you must add a Fail activity and configure its settings with the desired error message and code. The Fail activity is specifically designed for this purpose.

ðŸ“š CASE STUDY

Case Study 2: Real-Time IoT Data Ingestion

Your company processes IoT sensor data in real-time. Currently, data arrives at 10,000 events per second from IoT devices. You need to ingest this data into Fabric with minimal latency (<1 second end-to-end) and ensure data persistence for historical analysis.

**Question:**Â Which two components should you use to achieve both real-time ingestion and persistence?

==Eventstream to ingest events, then Spark Structured Streaming for transformation==

==Eventstream configured with a Lakehouse destination==

Spark Structured Streaming directly from Event Hubs with checkpointing

Dataflow Gen2 for ingestion and Warehouse for storage

#### âœ… CORRECT!

**Explanation:**Â For real-time IoT ingestion at 10K events/sec, Eventstream is ideal for initial ingestion and routing. To achieve sub-1-second latency with persistence, you can either use Eventstream directly to Lakehouse or pair Eventstream with Spark Structured Streaming.

