Reading documentation  on Microsoft.
Repeating study notebooklm DP-700 prep.

ğŸ’» Scenario 1 â€“ Workspace & Capacity Basics

Youâ€™ve just been added to a new Microsoft Fabric tenant.
Your manager asks you to:

Create a workspace for the Data Engineering team.

Make sure it has enough compute to run notebooks.

Give â€œRead Onlyâ€ access to analysts.

ğŸ‘‰ Question:
What specific Fabric resources or settings must you configure for each of these three steps?

Take a moment to think and describe what youâ€™d do inside Fabric UI (no code).
What do you think is the first thing youâ€™d need before you can even create that workspace?

Å˜ekl/a jsi:
I have to ask how many people will work there. I have to give them some capacity. what they gonna do. minimum is 64 i think .. Viewers will be Analitics people. 
Tutor Me Å™ekl/a:

Thatâ€™s a very thoughtful start â€” youâ€™re already thinking like a Fabric admin ğŸ‘

Letâ€™s unpack it together:

You said youâ€™d check how many people and what theyâ€™ll do â€” great! Thatâ€™s key because capacity depends on workload size.

Now, one small check:
You mentioned minimum is 64 â€” do you remember what that 64 refers to in Fabric capacity tiers?

(Hint: the names look like F2, F4, F8, â€¦ F64 â€” each step changes performance and cost.)

Å˜ekl/a jsi:
F64 is, but i dont remember costs.
Tutor Me Å™ekl/a:

Exactly â€” F64 is one of the higher Fabric capacity tiers ğŸ’ª

Youâ€™re right not to focus on the cost details (they vary by region), but conceptually:

ğŸ’° F2â€“F8 â†’ small test/dev workloads

âš™ï¸ F16â€“F32 â†’ production data pipelines and small teams

ğŸš€ F64 and above â†’ large organizations, AI Copilot, heavy Spark jobs

Letâ€™s move one step deeper into the hands-on side:

Imagine youâ€™ve already created your Data Engineering workspace and connected it to your chosen capacity.
Now, you need to set up security â€” the analysts should be able to view data but not edit pipelines or notebooks.

ğŸ‘‰ Question:
Which specific roles or permissions inside Fabric (or the workspace) would you assign to:

Yourself (data engineer)

The analysts who only view results

What roles sound right to you?

Å˜ekl/a jsi:
i will be Workspace Admin. And analysts will have object level View.
Tutor Me Å™ekl/a:

Excellent reasoning â€” youâ€™re right on both counts! ğŸ¯

âœ… You â†’ Workspace Admin gives you full control: you can create, edit, assign roles, and connect to capacity.
âœ… Analysts â†’ limiting them to View permissions (object-level or workspace-level Viewer role) means they can see reports, tables, and results, but canâ€™t change anything.

Now, letâ€™s stretch the scenario just a little:

Suppose one of the analysts needs to run notebooks but not edit pipelines.
ğŸ‘‰ What role or permission combination in Fabric would give them just enough power to run notebooks safely without full admin rights?

Å˜ekl/a jsi:
So he will be editor in workspace.
Tutor Me Å™ekl/a:

Nice â€” thatâ€™s exactly the kind of thinking the DP-700 exam wants you to show ğŸ‘

Yes, giving them the Editor role at the workspace level would let them:

run notebooks and pipelines,

create new Fabric items if needed,

but not manage access or capacity (thatâ€™s still for Admins).

Now imagine this: the same analyst accidentally deletes a Lakehouse table while testing a notebook.
To prevent this in the future, your manager asks you to apply the â€œleast privilegeâ€ principle.

ğŸ‘‰ What change could you make to their role or to object permissions so they can still run notebooks, but not modify data in the Lakehouse?

Å˜ekl/a jsi:
it is contributor role ... in environment
Tutor Me Å™ekl/a:

Youâ€™re very close â€” great recall! ğŸŒŸ

Letâ€™s think it through carefully:

The Contributor role does allow editing items (like notebooks or pipelines), but it can also change data inside those items â€” so itâ€™s still more power than we want here.

The goal is â€œrun notebooks, but canâ€™t modify data.â€

So, if we follow the least privilege principle, we might:

keep them as Viewer or Contributor at the workspace level,

but at the Lakehouse object level, restrict them to Read-only (View access on Lakehouse tables).

That way, they can open a notebook and run queries that read data but canâ€™t overwrite or drop tables.

ğŸ’¡ Mini hands-on reflection:
If you were setting that up in the Fabric UI â€” where would you find those Lakehouse object permissions?
Would you open the workspace settings, or the Lakehouse item itself?

Å˜ekl/a jsi:
governance or admin portal setings
Tutor Me Å™ekl/a:

Excellent instinct again ğŸ™Œ â€” youâ€™re pointing in the right direction!

Letâ€™s refine that a bit:

Workspace-level roles (like Admin, Member, Contributor, Viewer) are set in the Admin portal or workspace settings.

Object-level permissions (like read-only on a Lakehouse table or column) are part of governance inside the item itself â€” so youâ€™d open the Lakehouse â†’ Manage permissions pane.

That mix â€” workspace roles + object permissions â€” is exactly what the exam checks you can reason about.

âœ… Youâ€™ve done great on this governance scenario.
Would you like to move to the next hands-on topic: Data Pipelines orchestration (still Section 1), or switch to Ingest & Transform Data next?

Å˜ekl/a jsi:
ok
Tutor Me Å™ekl/a:

Awesome ğŸ˜„ Letâ€™s move into ğŸ’» Hands-on Scenario 2 â€“ Data Pipelines Orchestration from Section 1.

ğŸ’» Scenario 2 â€“ Building an Automated Pipeline

Youâ€™re asked to build an end-to-end nightly pipeline in Microsoft Fabric that:

Copies CSV files from an Azure Data Lake shortcut into your Lakehouse (bronze layer).

Triggers a notebook that cleans and aggregates the data into a silver table.

Finally refreshes a Power BI semantic model connected to the Lakehouse.

ğŸ‘‰ Question:
Which Fabric activities would you add to your pipeline for each of those three steps â€” and in what order would you chain them?

(Think about names like â€œCopy Data activityâ€, â€œNotebook activityâ€, â€œRefresh Semantic Model activityâ€â€¦)

Å˜ekl/a jsi:
1. Copy Data Activity with shortcut. Into Lakehouse bronze ğŸ¥‰ Then 2. New Notebook. Where I will use SPARK SQL to clean and agregation. To silver ğŸ¥ˆ Table. And then. I will use Data pipeline with schedule trigger. \ \if is that in delta table - PowerBI semantic model is automaticly refresh by schedule. 
Tutor Me Å™ekl/a:

Wow â€” thatâ€™s an excellent structured answer! ğŸŒŸ
Youâ€™ve got nearly the whole orchestration logic exactly right. Letâ€™s fine-tune just a couple of details so youâ€™d ace this on the exam:

âœ… Step 1: Copy Data activity â†’ from Azure Data Lake (via shortcut) into your Lakehouse bronze folder.
Perfect. Thatâ€™s the ingest step.

âœ… Step 2: Notebook activity â†’ runs PySpark / Spark SQL code to clean and aggregate into silver tables.
Exactly. You might also parametrize the notebook so it runs dynamically each night.

âœ… Step 3: Refresh Semantic Model activity â†’ triggers Power BI dataset refresh.
Right idea! A small correction: the refresh isnâ€™t automatic from Delta unless you set up a pipeline activity or a Power BI schedule.
So in Fabric, youâ€™d usually add the â€œRefresh Semantic Modelâ€ activity as the final step, or configure a refresh trigger inside Power BI service.

âœ… Pipeline trigger: Schedule trigger (daily at 2 AM) â€” perfect for automation.

ğŸ’¡ Mini-question:
If your boss later says, â€œInstead of running at 2 AM every day, please run it only after new files arrive,â€
what kind of trigger could you use in the pipeline to handle that automatically?

Å˜ekl/a jsi:
Some increpentall trigger? I am not sure.
Tutor Me Å™ekl/a:

Great guess ğŸ‘ â€” youâ€™re right to think about incremental or event-based logic!

Letâ€™s explore together:

In Fabric (and Azure Data Factoryâ€“style pipelines), there are two broad trigger types:

ğŸ•‘ Schedule trigger â†’ runs at fixed times (like 2 AM).

âš¡ Event-based trigger â†’ runs when a specific event happens (like a new file lands in a folder).

So in your case â€” â€œrun when new files arriveâ€ â€” youâ€™d use an event-based trigger that listens to your Azure Data Lake shortcut folder.

ğŸ’­ Quick reflection:
If your source folder keeps accumulating daily CSVs, how might you prevent the pipeline from re-processing old files again and again?

Å˜ekl/a jsi:
That in not incremental, but complete replacing. Deployment pipeline I hope is it.??
Tutor Me Å™ekl/a:

Ooh, I love how youâ€™re thinking â€” youâ€™re connecting deployment pipelines and incremental refresh, both real Fabric topics! ğŸ™Œ

Letâ€™s sort them out carefully:

Deployment pipeline â†’ used for moving content (like Lakehouse, reports, etc.) between environments (Dev â†’ Test â†’ Prod).
â†’ Itâ€™s not about how data loads; itâ€™s about versioning and releasing.

Incremental load â†’ part of data ingestion logic. It controls which data gets re-loaded.
â†’ Instead of copying all files, you load only new or changed ones.

So in your example (avoiding re-processing old CSVs), youâ€™d use incremental load logic inside the Copy Data activity.
For example:

Filter by modified date,

Or keep a watermark file/table that records the last processed timestamp.

ğŸ’­ Quick check:
If you wanted to implement that â€œwatermarkâ€ approach,
where do you think you could store the timestamp of the last processed file â€” in the pipeline itself, or in a data table (like Lakehouse metadata)?
